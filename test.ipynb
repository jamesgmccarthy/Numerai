{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0f579924bbded7ff69891cddd8dbc43d362b318d04c1573b30ec0f21b7672eee6",
   "display_name": "Python 3.8.8 64-bit ('Kaggle': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f579924bbded7ff69891cddd8dbc43d362b318d04c1573b30ec0f21b7672eee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import copy\n",
    "from data_loading import utils, purged_group_time_series\n",
    "from models import resnet, lightning_nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import joblib\n",
    "from models.SupervisedAutoEncoder import SupAE, create_hidden_rep\n",
    "import os\n",
    "import catboost as cat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model_res' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-89b5b3ed063d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m#model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mmodel_ae\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'./saved_models/trained/trained_ae.pth'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mp_ae\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpl_lightning\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSupAE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mmodels_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m'ResNet'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmodel_res\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mp_res\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'ae'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmodel_ae\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mp_ae\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'model_res' is not defined"
     ]
    }
   ],
   "source": [
    "p_ae = {'dim_1': 675, 'dim_2': 400, 'dim_3': 224, 'hidden': 162,\n",
    "         'activation': nn.ReLU, 'dropout': 0.2916447561918717, 'lr': 0.030272591341587315,\n",
    "         'recon_loss_factor': 0.4447516076774931, 'batch_size': 1252, 'loss_sup_ae': nn.MSELoss,\n",
    "         'loss_recon': nn.MSELoss,\n",
    "         'embedding': True,\n",
    "         'input_size':310,'output_size':1,'batch_size':4000}\n",
    "#p_res = {'dim_1': 843, 'dim_2': 2637, 'dim_3': 1618, 'dim_4': 880, 'dim_5': 220, 'activation': nn.LeakyReLU, 'dropout': 0.453246718032545, 'lr': 0.04565788979670108, 'batch_size': 10836,'loss':nn.MSELoss,'embedding':True,'input_size':310,'output_size':1,'hidden_len':p_ae['hidden']}\n",
    "#model_res = resnet.ResNet(input_size=p_res['input_size'],output_size=p_res['output_size'],params=p_res)\n",
    "#model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\n",
    "model_ae = utils.load_model(path='./saved_models/trained/trained_ae.pth',p=p_ae,pl_lightning=False,model=SupAE)\n",
    "models_dict = {'ResNet':[model_res,p_res],'ae':[model_ae,p_ae]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dict = {'lgb':[clf_lgb,p_lgb]}\n",
    "df_preds = utils.create_predictions(models=models_dict,ae=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = utils.create_prediction_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = f\"{utils.get_data_path(root_dir='./data')}/predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eras = df[df['data_type']=='validation']['era'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = os.listdir(pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [f\"era{era.replace('.csv','')}\" for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [era for era in eras if era in val_eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras =[era.replace('era','') for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [f'{era}.csv' for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.read_csv('./data/numerai_dataset_259/predictions/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_preds, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = df_preds[['id','prediction']]\n",
    "df_preds.to_csv('./data/numerai_dataset_259/predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       id  prediction\n",
       "0        n0003aa52cab36c2    0.495438\n",
       "1        n000920ed083903f    0.494747\n",
       "2        n0038e640522c4a6    0.512597\n",
       "3        n004ac94a87dc54b    0.501078\n",
       "4        n0052fe97ea0c05f    0.500134\n",
       "...                   ...         ...\n",
       "1693041  nffe7b93a3b669a5    0.509144\n",
       "1693042  nffeaf15f7ecd10b    0.496386\n",
       "1693043  nfff26fc05c7d497    0.503823\n",
       "1693044  nfff4cabd97debf2    0.507167\n",
       "1693045  nfff5bf75c1ccb60    0.498593\n",
       "\n",
       "[1693046 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n0003aa52cab36c2</td>\n      <td>0.495438</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n000920ed083903f</td>\n      <td>0.494747</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n0038e640522c4a6</td>\n      <td>0.512597</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n004ac94a87dc54b</td>\n      <td>0.501078</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n0052fe97ea0c05f</td>\n      <td>0.500134</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1693041</th>\n      <td>nffe7b93a3b669a5</td>\n      <td>0.509144</td>\n    </tr>\n    <tr>\n      <th>1693042</th>\n      <td>nffeaf15f7ecd10b</td>\n      <td>0.496386</td>\n    </tr>\n    <tr>\n      <th>1693043</th>\n      <td>nfff26fc05c7d497</td>\n      <td>0.503823</td>\n    </tr>\n    <tr>\n      <th>1693044</th>\n      <td>nfff4cabd97debf2</td>\n      <td>0.507167</td>\n    </tr>\n    <tr>\n      <th>1693045</th>\n      <td>nfff5bf75c1ccb60</td>\n      <td>0.498593</td>\n    </tr>\n  </tbody>\n</table>\n<p>1693046 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'era'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-63c279518e97>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_preds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'era'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscoring\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36mgroupby\u001B[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001B[0m\n\u001B[1;32m   6715\u001B[0m         \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_axis_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6716\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6717\u001B[0;31m         return DataFrameGroupBy(\n\u001B[0m\u001B[1;32m   6718\u001B[0m             \u001B[0mobj\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6719\u001B[0m             \u001B[0mkeys\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mby\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001B[0m\n\u001B[1;32m    558\u001B[0m             \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrouper\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_grouper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m             grouper, exclusions, obj = get_grouper(\n\u001B[0m\u001B[1;32m    561\u001B[0m                 \u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m                 \u001B[0mkeys\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/pandas/core/groupby/grouper.py\u001B[0m in \u001B[0;36mget_grouper\u001B[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001B[0m\n\u001B[1;32m    809\u001B[0m                 \u001B[0min_axis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgpr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgpr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgpr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgpr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mGrouper\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mgpr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkey\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m             \u001B[0;31m# Add key to exclusions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'era'"
     ]
    }
   ],
   "source": [
    "scores = df_preds.groupby('era').apply(scoring)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/numerai_dataset_259/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       id  prediction\n",
       "0        n0003aa52cab36c2    0.524231\n",
       "1        n000920ed083903f    0.513588\n",
       "2        n0038e640522c4a6    0.535152\n",
       "3        n004ac94a87dc54b    0.512500\n",
       "4        n0052fe97ea0c05f    0.513177\n",
       "...                   ...         ...\n",
       "1693041  nffe7b93a3b669a5    0.503227\n",
       "1693042  nffeaf15f7ecd10b    0.509167\n",
       "1693043  nfff26fc05c7d497    0.524319\n",
       "1693044  nfff4cabd97debf2    0.526400\n",
       "1693045  nfff5bf75c1ccb60    0.529765\n",
       "\n",
       "[1693046 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n0003aa52cab36c2</td>\n      <td>0.524231</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n000920ed083903f</td>\n      <td>0.513588</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n0038e640522c4a6</td>\n      <td>0.535152</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n004ac94a87dc54b</td>\n      <td>0.512500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n0052fe97ea0c05f</td>\n      <td>0.513177</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1693041</th>\n      <td>nffe7b93a3b669a5</td>\n      <td>0.503227</td>\n    </tr>\n    <tr>\n      <th>1693042</th>\n      <td>nffeaf15f7ecd10b</td>\n      <td>0.509167</td>\n    </tr>\n    <tr>\n      <th>1693043</th>\n      <td>nfff26fc05c7d497</td>\n      <td>0.524319</td>\n    </tr>\n    <tr>\n      <th>1693044</th>\n      <td>nfff4cabd97debf2</td>\n      <td>0.526400</td>\n    </tr>\n    <tr>\n      <th>1693045</th>\n      <td>nfff5bf75c1ccb60</td>\n      <td>0.529765</td>\n    </tr>\n  </tbody>\n</table>\n<p>1693046 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')\n",
    "df = df[df['data_type'] == 'validation']\n",
    "df_test = df\n",
    "df = utils.load_data(root_dir='./data/',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-d35d0c86b098>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['era'][df['era'] == 'eraX'] = 'era999'\n"
     ]
    }
   ],
   "source": [
    "df['era'][df['era'] == 'eraX'] = 'era999'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndata_ = copy.deepcopy(data)\\ntarget_ = copy.deepcopy(target)\\noe = OrdinalEncoder()\\ndata = oe.fit_transform(data)\\ntarget=oe.fit_transform(target_.values.reshape(-1,1)).reshape(-1)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data,target,features,era = utils.preprocess_data(df,ordinal=True)\n",
    "data_test,target_test,features_test,era_test = utils.preprocess_data(df_test,ordinal=True)\n",
    "\"\"\"\n",
    "data_ = copy.deepcopy(data)\n",
    "target_ = copy.deepcopy(target)\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "target=oe.fit_transform(target_.values.reshape(-1,1)).reshape(-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df ,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autoencodr training\n",
    "data = np.concatenate([data,data_test],0)\n",
    "target = np.concatenate([target,target_test],0)\n",
    "era = np.concatenate([era,era_test],0)\n",
    "t_idx =np.where(era <121)[0].tolist()\n",
    "v_idx =np.where(era >=121)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ae = joblib.load('./hpo/params/ae_sup_params.pkl')\n",
    "p_ae = p_ae.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dim_1': 957,\n",
       " 'dim_2': 973,\n",
       " 'dim_3': 418,\n",
       " 'hidden': 211,\n",
       " 'activation': 'gelu',\n",
       " 'dropout': 0.4359906639099919,\n",
       " 'lr': 0.04371817065406649,\n",
       " 'recon_loss_factor': 0.3347465392013046,\n",
       " 'batch_size': 4098}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "p_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_ae = {'dim_1': 1014, 'dim_2': 682, 'dim_3': 247, 'hidden': 119, 'activation': 'silu', 'dropout': 0.1216503434767066, 'lr': 0.017815077087898312, 'recon_loss_factor': 0.4519573434211407, 'batch_size': 941}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ae['activation'] = nn.GELU\n",
    "p_ae['loss_sup_ae']= nn.MSELoss\n",
    "p_ae['loss_recon']= nn.MSELoss\n",
    "p_ae['embedding']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "dataset = utils.FinData(data = data,target=target,era=era)\n",
    "dataloaders = utils.create_dataloaders(dataset,indexes={'train':t_idx,'val':v_idx},batch_size=p_ae['batch_size'])\n",
    "p_ae['input_size'] = len(features)\n",
    "p_ae['output_size'] = 1\n",
    "model = SupAE(params=p_ae)\n",
    "es = EarlyStopping(monitor='val_sup_loss',patience=10,min_delta=0.0005,mode='min')\n",
    "trainer = pl.Trainer(max_epochs=100,gpus=1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | loss_recon       | MSELoss    | 0     \n",
      "1 | loss_sup_ae      | MSELoss    | 0     \n",
      "2 | embedding_layers | ModuleList | 4.7 K \n",
      "3 | encoder          | Sequential | 2.3 M \n",
      "4 | regressor        | Sequential | 634   \n",
      "5 | decoder          | Sequential | 2.3 M \n",
      "------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.611    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba8fa4dfc9894b16b3ab385cffb172e1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4051c4be8f04094aed12f8886818af2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48baba92603845328b9a0a466f219225"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5272de78002747b8a7e4da6d0b482c60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce17daad62504e9f8e888e301e334cd2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2f9a379b25541fb8c9af58a99535dab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e01838b78d844c1aa9307617bab83eb0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "353052c358ac40e1afc62d7647be9fa7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b42cd05477554667a1a30f7bb6930680"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fff32f8f9dbd4d1f8cc79fe6a0be927d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c16a726cb7a64497a00b6a5bad6067f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2774e1f2f7bd47749d2b6822d73ad193"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a626268ee1d4f8b8af19a97d2254271"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "170f28d8186b4cf7a8e8ca3fcab1ac43"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af0a275ec8344df48e2c4dda4e59b081"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "trainer.fit(model,train_dataloader = dataloaders['train'],val_dataloaders=dataloaders['val'])\n",
    "torch.save(model.state_dict(),f'./saved_models/trained/ae_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict = {'data': data, 'target': target,\n",
    "                                 'features': features, 'era': era}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 501808, number of used features: 310\n",
      "[LightGBM] [Info] Start training from score 0.499997\n",
      "[1]\tvalid_0's l2: 0.0499474\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalid_0's l2: 0.049947\n",
      "[3]\tvalid_0's l2: 0.0499466\n",
      "[4]\tvalid_0's l2: 0.0499462\n",
      "[5]\tvalid_0's l2: 0.0499458\n",
      "[6]\tvalid_0's l2: 0.0499456\n",
      "[7]\tvalid_0's l2: 0.0499454\n",
      "[8]\tvalid_0's l2: 0.0499451\n",
      "[9]\tvalid_0's l2: 0.0499445\n",
      "[10]\tvalid_0's l2: 0.049944\n",
      "[11]\tvalid_0's l2: 0.0499438\n",
      "[12]\tvalid_0's l2: 0.0499434\n",
      "[13]\tvalid_0's l2: 0.0499432\n",
      "[14]\tvalid_0's l2: 0.049943\n",
      "[15]\tvalid_0's l2: 0.0499427\n",
      "[16]\tvalid_0's l2: 0.0499423\n",
      "[17]\tvalid_0's l2: 0.0499421\n",
      "[18]\tvalid_0's l2: 0.0499417\n",
      "[19]\tvalid_0's l2: 0.0499412\n",
      "[20]\tvalid_0's l2: 0.0499409\n",
      "[21]\tvalid_0's l2: 0.0499404\n",
      "[22]\tvalid_0's l2: 0.0499403\n",
      "[23]\tvalid_0's l2: 0.0499398\n",
      "[24]\tvalid_0's l2: 0.0499395\n",
      "[25]\tvalid_0's l2: 0.0499397\n",
      "[26]\tvalid_0's l2: 0.0499393\n",
      "[27]\tvalid_0's l2: 0.0499392\n",
      "[28]\tvalid_0's l2: 0.0499389\n",
      "[29]\tvalid_0's l2: 0.0499387\n",
      "[30]\tvalid_0's l2: 0.0499386\n",
      "[31]\tvalid_0's l2: 0.0499383\n",
      "[32]\tvalid_0's l2: 0.0499378\n",
      "[33]\tvalid_0's l2: 0.0499376\n",
      "[34]\tvalid_0's l2: 0.0499376\n",
      "[35]\tvalid_0's l2: 0.0499373\n",
      "[36]\tvalid_0's l2: 0.0499369\n",
      "[37]\tvalid_0's l2: 0.0499367\n",
      "[38]\tvalid_0's l2: 0.0499361\n",
      "[39]\tvalid_0's l2: 0.0499357\n",
      "[40]\tvalid_0's l2: 0.0499354\n",
      "[41]\tvalid_0's l2: 0.0499352\n",
      "[42]\tvalid_0's l2: 0.0499348\n",
      "[43]\tvalid_0's l2: 0.0499345\n",
      "[44]\tvalid_0's l2: 0.0499343\n",
      "[45]\tvalid_0's l2: 0.0499339\n",
      "[46]\tvalid_0's l2: 0.0499337\n",
      "[47]\tvalid_0's l2: 0.0499335\n",
      "[48]\tvalid_0's l2: 0.0499332\n",
      "[49]\tvalid_0's l2: 0.049933\n",
      "[50]\tvalid_0's l2: 0.0499329\n",
      "[51]\tvalid_0's l2: 0.0499327\n",
      "[52]\tvalid_0's l2: 0.0499323\n",
      "[53]\tvalid_0's l2: 0.049932\n",
      "[54]\tvalid_0's l2: 0.0499317\n",
      "[55]\tvalid_0's l2: 0.0499315\n",
      "[56]\tvalid_0's l2: 0.0499313\n",
      "[57]\tvalid_0's l2: 0.0499311\n",
      "[58]\tvalid_0's l2: 0.049931\n",
      "[59]\tvalid_0's l2: 0.0499311\n",
      "[60]\tvalid_0's l2: 0.0499307\n",
      "[61]\tvalid_0's l2: 0.0499305\n",
      "[62]\tvalid_0's l2: 0.0499303\n",
      "[63]\tvalid_0's l2: 0.0499302\n",
      "[64]\tvalid_0's l2: 0.04993\n",
      "[65]\tvalid_0's l2: 0.0499297\n",
      "[66]\tvalid_0's l2: 0.0499294\n",
      "[67]\tvalid_0's l2: 0.0499293\n",
      "[68]\tvalid_0's l2: 0.0499291\n",
      "[69]\tvalid_0's l2: 0.049929\n",
      "[70]\tvalid_0's l2: 0.049929\n",
      "[71]\tvalid_0's l2: 0.0499288\n",
      "[72]\tvalid_0's l2: 0.0499284\n",
      "[73]\tvalid_0's l2: 0.0499283\n",
      "[74]\tvalid_0's l2: 0.049928\n",
      "[75]\tvalid_0's l2: 0.0499278\n",
      "[76]\tvalid_0's l2: 0.0499275\n",
      "[77]\tvalid_0's l2: 0.0499274\n",
      "[78]\tvalid_0's l2: 0.0499273\n",
      "[79]\tvalid_0's l2: 0.0499272\n",
      "[80]\tvalid_0's l2: 0.0499269\n",
      "[81]\tvalid_0's l2: 0.049927\n",
      "[82]\tvalid_0's l2: 0.0499268\n",
      "[83]\tvalid_0's l2: 0.0499266\n",
      "[84]\tvalid_0's l2: 0.0499266\n",
      "[85]\tvalid_0's l2: 0.0499265\n",
      "[86]\tvalid_0's l2: 0.0499264\n",
      "[87]\tvalid_0's l2: 0.0499262\n",
      "[88]\tvalid_0's l2: 0.049926\n",
      "[89]\tvalid_0's l2: 0.0499257\n",
      "[90]\tvalid_0's l2: 0.0499255\n",
      "[91]\tvalid_0's l2: 0.0499252\n",
      "[92]\tvalid_0's l2: 0.0499249\n",
      "[93]\tvalid_0's l2: 0.0499251\n",
      "[94]\tvalid_0's l2: 0.0499252\n",
      "[95]\tvalid_0's l2: 0.0499252\n",
      "[96]\tvalid_0's l2: 0.0499251\n",
      "[97]\tvalid_0's l2: 0.0499248\n",
      "[98]\tvalid_0's l2: 0.0499246\n",
      "[99]\tvalid_0's l2: 0.0499244\n",
      "[100]\tvalid_0's l2: 0.0499244\n",
      "[101]\tvalid_0's l2: 0.0499242\n",
      "[102]\tvalid_0's l2: 0.0499243\n",
      "[103]\tvalid_0's l2: 0.0499243\n",
      "[104]\tvalid_0's l2: 0.049924\n",
      "[105]\tvalid_0's l2: 0.0499239\n",
      "[106]\tvalid_0's l2: 0.049924\n",
      "[107]\tvalid_0's l2: 0.049924\n",
      "[108]\tvalid_0's l2: 0.0499237\n",
      "[109]\tvalid_0's l2: 0.0499237\n",
      "[110]\tvalid_0's l2: 0.0499237\n",
      "[111]\tvalid_0's l2: 0.0499237\n",
      "[112]\tvalid_0's l2: 0.0499236\n",
      "[113]\tvalid_0's l2: 0.0499235\n",
      "[114]\tvalid_0's l2: 0.0499235\n",
      "[115]\tvalid_0's l2: 0.0499234\n",
      "[116]\tvalid_0's l2: 0.0499234\n",
      "[117]\tvalid_0's l2: 0.0499233\n",
      "[118]\tvalid_0's l2: 0.0499232\n",
      "[119]\tvalid_0's l2: 0.0499231\n",
      "[120]\tvalid_0's l2: 0.0499231\n",
      "[121]\tvalid_0's l2: 0.049923\n",
      "[122]\tvalid_0's l2: 0.0499228\n",
      "[123]\tvalid_0's l2: 0.0499228\n",
      "[124]\tvalid_0's l2: 0.0499228\n",
      "[125]\tvalid_0's l2: 0.0499226\n",
      "[126]\tvalid_0's l2: 0.0499225\n",
      "[127]\tvalid_0's l2: 0.0499223\n",
      "[128]\tvalid_0's l2: 0.0499222\n",
      "[129]\tvalid_0's l2: 0.0499221\n",
      "[130]\tvalid_0's l2: 0.049922\n",
      "[131]\tvalid_0's l2: 0.0499218\n",
      "[132]\tvalid_0's l2: 0.0499215\n",
      "[133]\tvalid_0's l2: 0.0499214\n",
      "[134]\tvalid_0's l2: 0.0499215\n",
      "[135]\tvalid_0's l2: 0.0499214\n",
      "[136]\tvalid_0's l2: 0.0499214\n",
      "[137]\tvalid_0's l2: 0.0499213\n",
      "[138]\tvalid_0's l2: 0.0499213\n",
      "[139]\tvalid_0's l2: 0.0499214\n",
      "[140]\tvalid_0's l2: 0.0499213\n",
      "[141]\tvalid_0's l2: 0.0499213\n",
      "[142]\tvalid_0's l2: 0.0499213\n",
      "[143]\tvalid_0's l2: 0.0499212\n",
      "[144]\tvalid_0's l2: 0.0499208\n",
      "[145]\tvalid_0's l2: 0.0499206\n",
      "[146]\tvalid_0's l2: 0.0499205\n",
      "[147]\tvalid_0's l2: 0.0499204\n",
      "[148]\tvalid_0's l2: 0.0499204\n",
      "[149]\tvalid_0's l2: 0.0499202\n",
      "[150]\tvalid_0's l2: 0.04992\n",
      "[151]\tvalid_0's l2: 0.0499201\n",
      "[152]\tvalid_0's l2: 0.0499201\n",
      "[153]\tvalid_0's l2: 0.0499203\n",
      "[154]\tvalid_0's l2: 0.0499204\n",
      "[155]\tvalid_0's l2: 0.0499202\n",
      "[156]\tvalid_0's l2: 0.04992\n",
      "[157]\tvalid_0's l2: 0.0499201\n",
      "[158]\tvalid_0's l2: 0.0499203\n",
      "[159]\tvalid_0's l2: 0.0499203\n",
      "[160]\tvalid_0's l2: 0.0499203\n",
      "[161]\tvalid_0's l2: 0.04992\n",
      "[162]\tvalid_0's l2: 0.04992\n",
      "[163]\tvalid_0's l2: 0.04992\n",
      "[164]\tvalid_0's l2: 0.0499201\n",
      "[165]\tvalid_0's l2: 0.0499201\n",
      "[166]\tvalid_0's l2: 0.04992\n",
      "[167]\tvalid_0's l2: 0.0499199\n",
      "[168]\tvalid_0's l2: 0.0499199\n",
      "[169]\tvalid_0's l2: 0.04992\n",
      "[170]\tvalid_0's l2: 0.0499196\n",
      "[171]\tvalid_0's l2: 0.0499195\n",
      "[172]\tvalid_0's l2: 0.0499197\n",
      "[173]\tvalid_0's l2: 0.0499195\n",
      "[174]\tvalid_0's l2: 0.0499192\n",
      "[175]\tvalid_0's l2: 0.0499193\n",
      "[176]\tvalid_0's l2: 0.0499193\n",
      "[177]\tvalid_0's l2: 0.0499194\n",
      "[178]\tvalid_0's l2: 0.0499194\n",
      "[179]\tvalid_0's l2: 0.0499193\n",
      "[180]\tvalid_0's l2: 0.0499193\n",
      "[181]\tvalid_0's l2: 0.0499194\n",
      "[182]\tvalid_0's l2: 0.0499192\n",
      "[183]\tvalid_0's l2: 0.0499192\n",
      "[184]\tvalid_0's l2: 0.0499192\n",
      "[185]\tvalid_0's l2: 0.0499193\n",
      "[186]\tvalid_0's l2: 0.0499192\n",
      "[187]\tvalid_0's l2: 0.0499191\n",
      "[188]\tvalid_0's l2: 0.0499191\n",
      "[189]\tvalid_0's l2: 0.0499193\n",
      "[190]\tvalid_0's l2: 0.0499192\n",
      "[191]\tvalid_0's l2: 0.0499193\n",
      "[192]\tvalid_0's l2: 0.0499193\n",
      "[193]\tvalid_0's l2: 0.0499193\n",
      "[194]\tvalid_0's l2: 0.0499194\n",
      "[195]\tvalid_0's l2: 0.0499194\n",
      "[196]\tvalid_0's l2: 0.0499195\n",
      "[197]\tvalid_0's l2: 0.0499195\n",
      "[198]\tvalid_0's l2: 0.0499194\n",
      "[199]\tvalid_0's l2: 0.0499194\n",
      "[200]\tvalid_0's l2: 0.0499192\n",
      "[201]\tvalid_0's l2: 0.0499192\n",
      "[202]\tvalid_0's l2: 0.0499191\n",
      "[203]\tvalid_0's l2: 0.0499189\n",
      "[204]\tvalid_0's l2: 0.0499192\n",
      "[205]\tvalid_0's l2: 0.0499193\n",
      "[206]\tvalid_0's l2: 0.0499193\n",
      "[207]\tvalid_0's l2: 0.0499191\n",
      "[208]\tvalid_0's l2: 0.0499192\n",
      "[209]\tvalid_0's l2: 0.0499193\n",
      "[210]\tvalid_0's l2: 0.0499193\n",
      "[211]\tvalid_0's l2: 0.0499193\n",
      "[212]\tvalid_0's l2: 0.0499191\n",
      "[213]\tvalid_0's l2: 0.0499191\n",
      "[214]\tvalid_0's l2: 0.049919\n",
      "[215]\tvalid_0's l2: 0.049919\n",
      "[216]\tvalid_0's l2: 0.0499191\n",
      "[217]\tvalid_0's l2: 0.049919\n",
      "[218]\tvalid_0's l2: 0.0499191\n",
      "[219]\tvalid_0's l2: 0.049919\n",
      "[220]\tvalid_0's l2: 0.0499192\n",
      "[221]\tvalid_0's l2: 0.0499193\n",
      "[222]\tvalid_0's l2: 0.0499192\n",
      "[223]\tvalid_0's l2: 0.0499193\n",
      "[224]\tvalid_0's l2: 0.0499191\n",
      "[225]\tvalid_0's l2: 0.0499191\n",
      "[226]\tvalid_0's l2: 0.0499194\n",
      "[227]\tvalid_0's l2: 0.0499194\n",
      "[228]\tvalid_0's l2: 0.0499193\n",
      "[229]\tvalid_0's l2: 0.0499193\n",
      "[230]\tvalid_0's l2: 0.0499192\n",
      "[231]\tvalid_0's l2: 0.0499191\n",
      "[232]\tvalid_0's l2: 0.0499191\n",
      "[233]\tvalid_0's l2: 0.0499191\n",
      "[234]\tvalid_0's l2: 0.0499192\n",
      "[235]\tvalid_0's l2: 0.0499193\n",
      "[236]\tvalid_0's l2: 0.0499193\n",
      "[237]\tvalid_0's l2: 0.0499194\n",
      "[238]\tvalid_0's l2: 0.0499192\n",
      "[239]\tvalid_0's l2: 0.0499192\n",
      "[240]\tvalid_0's l2: 0.0499191\n",
      "[241]\tvalid_0's l2: 0.0499189\n",
      "[242]\tvalid_0's l2: 0.0499188\n",
      "[243]\tvalid_0's l2: 0.0499189\n",
      "[244]\tvalid_0's l2: 0.0499189\n",
      "[245]\tvalid_0's l2: 0.0499189\n",
      "[246]\tvalid_0's l2: 0.0499189\n",
      "[247]\tvalid_0's l2: 0.0499191\n",
      "[248]\tvalid_0's l2: 0.0499192\n",
      "[249]\tvalid_0's l2: 0.0499191\n",
      "[250]\tvalid_0's l2: 0.049919\n",
      "[251]\tvalid_0's l2: 0.0499191\n",
      "[252]\tvalid_0's l2: 0.0499189\n",
      "[253]\tvalid_0's l2: 0.049919\n",
      "[254]\tvalid_0's l2: 0.0499192\n",
      "[255]\tvalid_0's l2: 0.049919\n",
      "[256]\tvalid_0's l2: 0.0499191\n",
      "[257]\tvalid_0's l2: 0.049919\n",
      "[258]\tvalid_0's l2: 0.0499192\n",
      "[259]\tvalid_0's l2: 0.0499192\n",
      "[260]\tvalid_0's l2: 0.0499191\n",
      "[261]\tvalid_0's l2: 0.049919\n",
      "[262]\tvalid_0's l2: 0.0499191\n",
      "[263]\tvalid_0's l2: 0.0499189\n",
      "[264]\tvalid_0's l2: 0.0499188\n",
      "[265]\tvalid_0's l2: 0.0499187\n",
      "[266]\tvalid_0's l2: 0.0499187\n",
      "[267]\tvalid_0's l2: 0.0499188\n",
      "[268]\tvalid_0's l2: 0.0499189\n",
      "[269]\tvalid_0's l2: 0.0499189\n",
      "[270]\tvalid_0's l2: 0.0499189\n",
      "[271]\tvalid_0's l2: 0.0499187\n",
      "[272]\tvalid_0's l2: 0.0499189\n",
      "[273]\tvalid_0's l2: 0.0499188\n",
      "[274]\tvalid_0's l2: 0.0499189\n",
      "[275]\tvalid_0's l2: 0.0499189\n",
      "[276]\tvalid_0's l2: 0.0499187\n",
      "[277]\tvalid_0's l2: 0.0499189\n",
      "[278]\tvalid_0's l2: 0.0499189\n",
      "[279]\tvalid_0's l2: 0.049919\n",
      "[280]\tvalid_0's l2: 0.049919\n",
      "[281]\tvalid_0's l2: 0.0499191\n",
      "[282]\tvalid_0's l2: 0.0499193\n",
      "[283]\tvalid_0's l2: 0.0499193\n",
      "[284]\tvalid_0's l2: 0.0499192\n",
      "[285]\tvalid_0's l2: 0.0499192\n",
      "[286]\tvalid_0's l2: 0.0499191\n",
      "[287]\tvalid_0's l2: 0.0499189\n",
      "[288]\tvalid_0's l2: 0.0499188\n",
      "[289]\tvalid_0's l2: 0.0499188\n",
      "[290]\tvalid_0's l2: 0.0499188\n",
      "[291]\tvalid_0's l2: 0.0499185\n",
      "[292]\tvalid_0's l2: 0.0499186\n",
      "[293]\tvalid_0's l2: 0.0499186\n",
      "[294]\tvalid_0's l2: 0.0499186\n",
      "[295]\tvalid_0's l2: 0.0499188\n",
      "[296]\tvalid_0's l2: 0.0499188\n",
      "[297]\tvalid_0's l2: 0.0499189\n",
      "[298]\tvalid_0's l2: 0.0499189\n",
      "[299]\tvalid_0's l2: 0.0499191\n",
      "[300]\tvalid_0's l2: 0.0499192\n",
      "[301]\tvalid_0's l2: 0.0499192\n",
      "[302]\tvalid_0's l2: 0.0499193\n",
      "[303]\tvalid_0's l2: 0.0499192\n",
      "[304]\tvalid_0's l2: 0.0499192\n",
      "[305]\tvalid_0's l2: 0.0499191\n",
      "[306]\tvalid_0's l2: 0.049919\n",
      "[307]\tvalid_0's l2: 0.049919\n",
      "[308]\tvalid_0's l2: 0.0499189\n",
      "[309]\tvalid_0's l2: 0.049919\n",
      "[310]\tvalid_0's l2: 0.0499191\n",
      "[311]\tvalid_0's l2: 0.0499192\n",
      "[312]\tvalid_0's l2: 0.049919\n",
      "[313]\tvalid_0's l2: 0.0499193\n",
      "[314]\tvalid_0's l2: 0.0499192\n",
      "[315]\tvalid_0's l2: 0.0499191\n",
      "[316]\tvalid_0's l2: 0.0499191\n",
      "[317]\tvalid_0's l2: 0.0499189\n",
      "[318]\tvalid_0's l2: 0.049919\n",
      "[319]\tvalid_0's l2: 0.0499188\n",
      "[320]\tvalid_0's l2: 0.0499187\n",
      "[321]\tvalid_0's l2: 0.0499187\n",
      "[322]\tvalid_0's l2: 0.0499187\n",
      "[323]\tvalid_0's l2: 0.0499189\n",
      "[324]\tvalid_0's l2: 0.049919\n",
      "[325]\tvalid_0's l2: 0.049919\n",
      "[326]\tvalid_0's l2: 0.049919\n",
      "[327]\tvalid_0's l2: 0.049919\n",
      "[328]\tvalid_0's l2: 0.0499189\n",
      "[329]\tvalid_0's l2: 0.049919\n",
      "[330]\tvalid_0's l2: 0.0499191\n",
      "[331]\tvalid_0's l2: 0.0499191\n",
      "[332]\tvalid_0's l2: 0.0499191\n",
      "[333]\tvalid_0's l2: 0.0499191\n",
      "[334]\tvalid_0's l2: 0.0499192\n",
      "[335]\tvalid_0's l2: 0.0499193\n",
      "[336]\tvalid_0's l2: 0.0499192\n",
      "[337]\tvalid_0's l2: 0.0499192\n",
      "[338]\tvalid_0's l2: 0.049919\n",
      "[339]\tvalid_0's l2: 0.0499191\n",
      "[340]\tvalid_0's l2: 0.0499191\n",
      "[341]\tvalid_0's l2: 0.0499189\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's l2: 0.0499185\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f386e919c10>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x_tr, x_val = data_dict['data'][t_idx], data_dict['data'][v_idx]\n",
    "y_tr, y_val = data_dict['target'][t_idx],data_dict['target'][v_idx]\n",
    "d_tr = lgb.Dataset(x_tr,label=y_tr)\n",
    "d_val = lgb.Dataset(x_val,label=y_val)\n",
    "clf = lgb.train(p_lgb,d_tr,1000,valid_sets=[d_val],early_stopping_rounds=50,verbose_eval=True)\n",
    "clf.save_model(f'./saved_models/trained/lgb.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'data': data, 'target': target,\n",
    "                 'features': features, 'era': era}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ae['input_size'] = len(features)\n",
    "p_ae['output_size'] = 1\n",
    "model = utils.load_model('./saved_models/trained/ae_state_dict.pth',\n",
    "                    p=p_ae, pl_lightning=False, model=SupAE)\n"
   ]
  },
  {
   "source": [
    "\n",
    "models_dict = {'lgb':[clf,p_lgb]}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_output = create_hidden_rep(model, data_dict)\n",
    "data_dict['hidden'],ae_preds = ae_output['hidden'], ae_output['preds']\n",
    "data_dict['hidden_true'] = True\n",
    "p_ae['hidden_len'] = data_dict['hidden'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 2., 1., ..., 4., 2., 3.],\n",
       "       [0., 0., 0., ..., 0., 1., 4.],\n",
       "       [1., 2., 1., ..., 0., 1., 3.],\n",
       "       ...,\n",
       "       [1., 4., 2., ..., 0., 3., 3.],\n",
       "       [0., 3., 3., ..., 3., 2., 2.],\n",
       "       [0., 2., 2., ..., 3., 1., 2.]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "data_dict['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(\n",
    "            data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
    "dataloaders = utils.create_dataloaders(\n",
    "            dataset, indexes={'train': t_idx}, batch_size=p['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_res = {'dim_1': 843, 'dim_2': 2637, 'dim_3': 1618, 'dim_4': 880, 'dim_5': 220, 'activation': nn.LeakyReLU, 'dropout': 0.453246718032545, 'lr': 0.04565788979670108, 'batch_size': 10836,'loss':nn.MSELoss,'embedding':True,'input_size':310,'output_size':1,'hidden_len':p_ae['hidden']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-bdad4d20804c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mResNet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_size'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0moutput_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'output_size'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mEarlyStopping\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'val_mse'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmin_delta\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.0005\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'min'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtrainer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mgpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mes\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdataloaders\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mval_dataloaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdataloaders\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'val'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34mf'./saved_models/trained/ResNet_state_dict.pth'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "model = resnet.ResNet(input_size = p['input_size'],output_size=p['output_size'],params=p)\n",
    "es = EarlyStopping(monitor='val_mse',patience=10,min_delta=0.0005,mode='min')\n",
    "trainer = pl.Trainer(max_epochs=100,gpus=1,callbacks=[es])\n",
    "trainer.fit(model,train_dataloader=dataloaders['train'],val_dataloaders=dataloaders['val'])\n",
    "torch.save(model.state_dict(),f'./saved_models/trained/ResNet_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#model = utils.load_model('./saved_models/trained/ResNet_state_dict.pth',\n",
    " #                   p=p, pl_lightning=False, model=resnet.ResNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = resnet.ResNet(input_size=p_res['input_size'],output_size=p_res['output_size'],params=p_res)\n",
    "model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\n",
    "model_ae = utils.load_model(path='./saved_models/trained/trained_ae.pth',p=p_ae,pl_lightning=False,model=SupAE)\n",
    "models_dict = {'ResNet':[model_res,p_res],'ae':[model_ae,p_ae]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/numerai_dataset_258/predictions/132.csv'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-ee752c9d09f8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_predictions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodels_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Numerai/data_loading/utils.py\u001B[0m in \u001B[0;36mcreate_predictions\u001B[0;34m(root_dir, models, hidden, ae)\u001B[0m\n\u001B[1;32m    270\u001B[0m         \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'prediction_lgb'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m         \u001B[0mpred_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{get_data_path(root_dir)}/predictions/{era[0]}'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m         \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{pred_path}.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mto_csv\u001B[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[1;32m   3385\u001B[0m         )\n\u001B[1;32m   3386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3387\u001B[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001B[0m\u001B[1;32m   3388\u001B[0m             \u001B[0mpath_or_buf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3389\u001B[0m             \u001B[0mline_terminator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mline_terminator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/io/formats/format.py\u001B[0m in \u001B[0;36mto_csv\u001B[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[1;32m   1081\u001B[0m             \u001B[0mformatter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfmt\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1082\u001B[0m         )\n\u001B[0;32m-> 1083\u001B[0;31m         \u001B[0mcsv_formatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1084\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1085\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcreated_buffer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    226\u001B[0m         \"\"\"\n\u001B[1;32m    227\u001B[0m         \u001B[0;31m# apply compression and byte/text conversion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 228\u001B[0;31m         with get_handle(\n\u001B[0m\u001B[1;32m    229\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/numerai_dataset_258/predictions/132.csv'"
     ]
    }
   ],
   "source": [
    "utils.create_predictions(models=models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2, train_loss=1.090]\n",
      "Epoch 0:  88%|████████▊ | 248/283 [00:12<00:01, 19.77it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Validating:  79%|███████▉  | 130/165 [00:03<00:00, 37.07it/s]\u001B[A\n",
      "Epoch 0:  89%|████████▉ | 253/283 [00:12<00:01, 19.97it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  91%|█████████ | 258/283 [00:12<00:01, 20.16it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  93%|█████████▎| 263/283 [00:12<00:00, 20.33it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  95%|█████████▍| 268/283 [00:13<00:00, 20.50it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Validating:  91%|█████████ | 150/165 [00:04<00:00, 37.26it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▋| 273/283 [00:13<00:00, 20.68it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  98%|█████████▊| 278/283 [00:13<00:00, 20.86it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0: 100%|██████████| 283/283 [00:13<00:00, 20.91it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.080, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  42%|████▏     | 118/283 [00:08<00:11, 14.24it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/165 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  42%|████▏     | 120/283 [00:08<00:11, 13.95it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  44%|████▍     | 125/283 [00:08<00:11, 14.30it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  46%|████▌     | 130/283 [00:08<00:10, 14.63it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  48%|████▊     | 135/283 [00:09<00:09, 14.97it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  49%|████▉     | 139/283 [00:09<00:09, 15.09it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "/home/james/.virtualenvs/Kaggle/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | loss_recon       | MSELoss    | 0     \n",
      "1 | loss_sup_ae      | MSELoss    | 0     \n",
      "2 | embedding_layers | ModuleList | 4.7 K \n",
      "3 | encoder          | Sequential | 1.3 M \n",
      "4 | regressor        | Sequential | 232   \n",
      "5 | decoder          | Sequential | 1.3 M \n",
      "------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.174    Total estimated model params size (MB)\n",
      "Epoch 0:   0%|          | 0/452 [00:00<?, ?it/s] Exception ignored in: <function _releaseLock at 0x7f63c342a550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Epoch 0:   0%|          | 0/452 [00:00<?, ?it/s]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | loss_recon       | MSELoss    | 0     \n",
      "1 | loss_sup_ae      | MSELoss    | 0     \n",
      "2 | embedding_layers | ModuleList | 4.7 K \n",
      "3 | encoder          | Sequential | 1.3 M \n",
      "4 | regressor        | Sequential | 232   \n",
      "5 | decoder          | Sequential | 1.3 M \n",
      "------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.174    Total estimated model params size (MB)\n",
      "Epoch 0:  32%|███▏      | 203/629 [00:14<00:30, 13.97it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  73%|███████▎  | 457/629 [00:32<00:12, 14.26it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  73%|███████▎  | 459/629 [00:32<00:11, 14.18it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  74%|███████▎  | 463/629 [00:32<00:11, 14.25it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  74%|███████▍  | 468/629 [00:32<00:11, 14.35it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  75%|███████▌  | 473/629 [00:32<00:10, 14.45it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  76%|███████▌  | 478/629 [00:32<00:10, 14.54it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  77%|███████▋  | 483/629 [00:33<00:09, 14.63it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  15%|█▌        | 26/172 [00:00<00:04, 34.06it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 488/629 [00:33<00:09, 14.72it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  78%|███████▊  | 493/629 [00:33<00:09, 14.81it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  79%|███████▉  | 498/629 [00:33<00:08, 14.90it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  80%|███████▉  | 503/629 [00:33<00:08, 14.99it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  27%|██▋       | 46/172 [00:01<00:03, 36.70it/s]\u001B[A\n",
      "Epoch 0:  81%|████████  | 508/629 [00:33<00:08, 15.08it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  82%|████████▏ | 513/629 [00:33<00:07, 15.17it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  82%|████████▏ | 518/629 [00:33<00:07, 15.26it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  83%|████████▎ | 523/629 [00:34<00:06, 15.34it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  38%|███▊      | 66/172 [00:02<00:02, 37.07it/s]\u001B[A\n",
      "Epoch 0:  84%|████████▍ | 528/629 [00:34<00:06, 15.43it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  85%|████████▍ | 533/629 [00:34<00:06, 15.51it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▌ | 538/629 [00:34<00:05, 15.60it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▋ | 543/629 [00:34<00:05, 15.68it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  50%|█████     | 86/172 [00:02<00:02, 36.73it/s]\u001B[A\n",
      "Epoch 0:  87%|████████▋ | 548/629 [00:34<00:05, 15.76it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  88%|████████▊ | 553/629 [00:34<00:04, 15.85it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  89%|████████▊ | 558/629 [00:35<00:04, 15.93it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  90%|████████▉ | 563/629 [00:35<00:04, 16.01it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  62%|██████▏   | 106/172 [00:03<00:01, 37.26it/s]\u001B[A\n",
      "Epoch 0:  90%|█████████ | 568/629 [00:35<00:03, 16.09it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  91%|█████████ | 573/629 [00:35<00:03, 16.18it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  92%|█████████▏| 578/629 [00:35<00:03, 16.26it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  93%|█████████▎| 583/629 [00:35<00:02, 16.34it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  93%|█████████▎| 588/629 [00:35<00:02, 16.42it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  94%|█████████▍| 593/629 [00:35<00:02, 16.50it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  95%|█████████▌| 598/629 [00:36<00:01, 16.58it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  96%|█████████▌| 603/629 [00:36<00:01, 16.66it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  85%|████████▍ | 146/172 [00:04<00:00, 39.22it/s]\u001B[A\n",
      "Epoch 0:  97%|█████████▋| 608/629 [00:36<00:01, 16.73it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  97%|█████████▋| 613/629 [00:36<00:00, 16.81it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  98%|█████████▊| 618/629 [00:36<00:00, 16.89it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  99%|█████████▉| 623/629 [00:36<00:00, 16.98it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0: 100%|██████████| 629/629 [00:36<00:00, 17.03it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  73%|███████▎  | 457/629 [00:31<00:11, 14.38it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  73%|███████▎  | 460/629 [00:32<00:11, 14.31it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  74%|███████▍  | 465/629 [00:32<00:11, 14.41it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  75%|███████▍  | 470/629 [00:32<00:10, 14.51it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 28.14it/s]\u001B[A\n",
      "Epoch 1:  76%|███████▌  | 475/629 [00:32<00:10, 14.60it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  76%|███████▋  | 480/629 [00:32<00:10, 14.70it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  77%|███████▋  | 485/629 [00:32<00:09, 14.79it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  78%|███████▊  | 490/629 [00:32<00:09, 14.88it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:03, 36.47it/s]\u001B[A\n",
      "Epoch 1:  79%|███████▊  | 495/629 [00:33<00:08, 14.98it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  79%|███████▉  | 500/629 [00:33<00:08, 15.07it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  80%|████████  | 505/629 [00:33<00:08, 15.15it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  81%|████████  | 510/629 [00:33<00:07, 15.24it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 37.17it/s]\u001B[A\n",
      "Epoch 1:  82%|████████▏ | 515/629 [00:33<00:07, 15.33it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  83%|████████▎ | 520/629 [00:33<00:07, 15.42it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  83%|████████▎ | 525/629 [00:33<00:06, 15.51it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  84%|████████▍ | 530/629 [00:33<00:06, 15.59it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  42%|████▏     | 73/172 [00:02<00:02, 37.23it/s]\u001B[A\n",
      "Epoch 1:  85%|████████▌ | 535/629 [00:34<00:05, 15.67it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  86%|████████▌ | 540/629 [00:34<00:05, 15.75it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  87%|████████▋ | 545/629 [00:34<00:05, 15.84it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  87%|████████▋ | 550/629 [00:34<00:04, 15.92it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  54%|█████▍    | 93/172 [00:02<00:02, 36.17it/s]\u001B[A\n",
      "Epoch 1:  88%|████████▊ | 555/629 [00:34<00:04, 16.00it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  89%|████████▉ | 560/629 [00:34<00:04, 16.08it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  90%|████████▉ | 565/629 [00:34<00:03, 16.16it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  91%|█████████ | 570/629 [00:35<00:03, 16.25it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  66%|██████▌   | 113/172 [00:03<00:01, 37.19it/s]\u001B[A\n",
      "Epoch 1:  91%|█████████▏| 575/629 [00:35<00:03, 16.33it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  92%|█████████▏| 580/629 [00:35<00:02, 16.40it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  93%|█████████▎| 585/629 [00:35<00:02, 16.48it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  94%|█████████▍| 590/629 [00:35<00:02, 16.56it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  77%|███████▋  | 133/172 [00:03<00:01, 37.21it/s]\u001B[A\n",
      "Epoch 1:  95%|█████████▍| 595/629 [00:35<00:02, 16.64it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  95%|█████████▌| 600/629 [00:35<00:01, 16.71it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  96%|█████████▌| 605/629 [00:36<00:01, 16.79it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  97%|█████████▋| 610/629 [00:36<00:01, 16.87it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  89%|████████▉ | 153/172 [00:04<00:00, 37.76it/s]\u001B[A\n",
      "Epoch 1:  98%|█████████▊| 615/629 [00:36<00:00, 16.95it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  99%|█████████▊| 620/629 [00:36<00:00, 17.03it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  99%|█████████▉| 625/629 [00:36<00:00, 17.11it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1: 100%|██████████| 629/629 [00:36<00:00, 17.14it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.100, sup_loss=0.0505, recon_loss=1.210] \n",
      "Epoch 2:  22%|██▏       | 139/629 [00:09<00:33, 14.44it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  73%|███████▎  | 457/629 [00:31<00:11, 14.52it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  73%|███████▎  | 460/629 [00:31<00:11, 14.45it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  74%|███████▍  | 465/629 [00:31<00:11, 14.55it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  75%|███████▍  | 470/629 [00:32<00:10, 14.64it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 26.86it/s]\u001B[A\n",
      "Epoch 2:  76%|███████▌  | 475/629 [00:32<00:10, 14.73it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  76%|███████▋  | 480/629 [00:32<00:10, 14.82it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  77%|███████▋  | 485/629 [00:32<00:09, 14.91it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  78%|███████▊  | 490/629 [00:32<00:09, 15.00it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  79%|███████▊  | 495/629 [00:32<00:08, 15.09it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  22%|██▏       | 38/172 [00:01<00:03, 34.91it/s]\u001B[A\n",
      "Epoch 2:  79%|███████▉  | 500/629 [00:32<00:08, 15.17it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  80%|████████  | 505/629 [00:33<00:08, 15.26it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  81%|████████  | 510/629 [00:33<00:07, 15.35it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  82%|████████▏ | 515/629 [00:33<00:07, 15.43it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  83%|████████▎ | 520/629 [00:33<00:07, 15.52it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  37%|███▋      | 63/172 [00:02<00:02, 36.66it/s]\u001B[A\n",
      "Epoch 2:  83%|████████▎ | 525/629 [00:33<00:06, 15.61it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  84%|████████▍ | 530/629 [00:33<00:06, 15.70it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  85%|████████▌ | 535/629 [00:33<00:05, 15.78it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  86%|████████▌ | 540/629 [00:34<00:05, 15.87it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  87%|████████▋ | 545/629 [00:34<00:05, 15.96it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  51%|█████     | 88/172 [00:02<00:02, 38.46it/s]\u001B[A\n",
      "Epoch 2:  87%|████████▋ | 550/629 [00:34<00:04, 16.04it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  88%|████████▊ | 555/629 [00:34<00:04, 16.13it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  89%|████████▉ | 560/629 [00:34<00:04, 16.21it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  90%|████████▉ | 565/629 [00:34<00:03, 16.29it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  63%|██████▎   | 108/172 [00:03<00:01, 37.40it/s]\u001B[A\n",
      "Epoch 2:  91%|█████████ | 570/629 [00:34<00:03, 16.37it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  91%|█████████▏| 575/629 [00:34<00:03, 16.45it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  92%|█████████▏| 580/629 [00:35<00:02, 16.53it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  93%|█████████▎| 585/629 [00:35<00:02, 16.61it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  74%|███████▍  | 128/172 [00:03<00:01, 37.43it/s]\u001B[A\n",
      "Epoch 2:  94%|█████████▍| 590/629 [00:35<00:02, 16.69it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  95%|█████████▍| 595/629 [00:35<00:02, 16.77it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  95%|█████████▌| 600/629 [00:35<00:01, 16.85it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  96%|█████████▌| 605/629 [00:35<00:01, 16.92it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  97%|█████████▋| 610/629 [00:35<00:01, 17.01it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  89%|████████▉ | 153/172 [00:04<00:00, 39.09it/s]\u001B[A\n",
      "Epoch 2:  98%|█████████▊| 615/629 [00:35<00:00, 17.08it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  99%|█████████▊| 620/629 [00:36<00:00, 17.17it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  99%|█████████▉| 625/629 [00:36<00:00, 17.25it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2: 100%|██████████| 629/629 [00:36<00:00, 17.27it/s, loss=1.11, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.110, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  73%|███████▎  | 457/629 [00:31<00:11, 14.60it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  73%|███████▎  | 460/629 [00:31<00:11, 14.52it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  74%|███████▍  | 465/629 [00:31<00:11, 14.62it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  75%|███████▍  | 470/629 [00:31<00:10, 14.71it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:06, 26.46it/s]\u001B[A\n",
      "Epoch 3:  76%|███████▌  | 475/629 [00:32<00:10, 14.80it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  76%|███████▋  | 480/629 [00:32<00:10, 14.90it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  77%|███████▋  | 485/629 [00:32<00:09, 14.99it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  78%|███████▊  | 490/629 [00:32<00:09, 15.08it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:03, 35.77it/s]\u001B[A\n",
      "Epoch 3:  79%|███████▊  | 495/629 [00:32<00:08, 15.18it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  79%|███████▉  | 500/629 [00:32<00:08, 15.27it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  80%|████████  | 505/629 [00:32<00:08, 15.36it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  81%|████████  | 510/629 [00:33<00:07, 15.45it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 36.94it/s]\u001B[A\n",
      "Epoch 3:  82%|████████▏ | 515/629 [00:33<00:07, 15.53it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  83%|████████▎ | 520/629 [00:33<00:06, 15.62it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  83%|████████▎ | 525/629 [00:33<00:06, 15.71it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  84%|████████▍ | 530/629 [00:33<00:06, 15.80it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  42%|████▏     | 73/172 [00:02<00:02, 37.35it/s]\u001B[A\n",
      "Epoch 3:  85%|████████▌ | 535/629 [00:33<00:05, 15.88it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  86%|████████▌ | 540/629 [00:33<00:05, 15.97it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  87%|████████▋ | 545/629 [00:33<00:05, 16.05it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  87%|████████▋ | 550/629 [00:34<00:04, 16.13it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  54%|█████▍    | 93/172 [00:02<00:02, 36.62it/s]\u001B[A\n",
      "Epoch 3:  88%|████████▊ | 555/629 [00:34<00:04, 16.22it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  89%|████████▉ | 560/629 [00:34<00:04, 16.30it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  90%|████████▉ | 565/629 [00:34<00:03, 16.38it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  91%|█████████ | 570/629 [00:34<00:03, 16.46it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  66%|██████▌   | 113/172 [00:03<00:01, 37.15it/s]\u001B[A\n",
      "Epoch 3:  91%|█████████▏| 575/629 [00:34<00:03, 16.54it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  92%|█████████▏| 580/629 [00:34<00:02, 16.62it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  93%|█████████▎| 585/629 [00:35<00:02, 16.70it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  94%|█████████▍| 590/629 [00:35<00:02, 16.78it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  77%|███████▋  | 133/172 [00:03<00:01, 37.47it/s]\u001B[A\n",
      "Epoch 3:  95%|█████████▍| 595/629 [00:35<00:02, 16.86it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  95%|█████████▌| 600/629 [00:35<00:01, 16.94it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  96%|█████████▌| 605/629 [00:35<00:01, 17.01it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  97%|█████████▋| 610/629 [00:35<00:01, 17.09it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  98%|█████████▊| 615/629 [00:35<00:00, 17.17it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  92%|█████████▏| 158/172 [00:04<00:00, 39.26it/s]\u001B[A\n",
      "Epoch 3:  99%|█████████▊| 620/629 [00:35<00:00, 17.25it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  99%|█████████▉| 625/629 [00:36<00:00, 17.33it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3: 100%|██████████| 629/629 [00:36<00:00, 17.36it/s, loss=1.12, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  73%|███████▎  | 457/629 [00:31<00:11, 14.49it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4:  73%|███████▎  | 460/629 [00:31<00:11, 14.42it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  74%|███████▍  | 465/629 [00:32<00:11, 14.52it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  75%|███████▍  | 470/629 [00:32<00:10, 14.61it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 26.54it/s]\u001B[A\n",
      "Epoch 4:  76%|███████▌  | 475/629 [00:32<00:10, 14.70it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  76%|███████▋  | 480/629 [00:32<00:10, 14.80it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  77%|███████▋  | 485/629 [00:32<00:09, 14.89it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  78%|███████▊  | 490/629 [00:32<00:09, 14.98it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:03, 35.10it/s]\u001B[A\n",
      "Epoch 4:  79%|███████▊  | 495/629 [00:32<00:08, 15.07it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  79%|███████▉  | 500/629 [00:32<00:08, 15.16it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  80%|████████  | 505/629 [00:33<00:08, 15.25it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  81%|████████  | 510/629 [00:33<00:07, 15.34it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 35.34it/s]\u001B[A\n",
      "Epoch 4:  82%|████████▏ | 515/629 [00:33<00:07, 15.42it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  83%|████████▎ | 520/629 [00:33<00:07, 15.51it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  83%|████████▎ | 525/629 [00:33<00:06, 15.61it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  84%|████████▍ | 530/629 [00:33<00:06, 15.69it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  85%|████████▌ | 535/629 [00:33<00:05, 15.78it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  45%|████▌     | 78/172 [00:02<00:02, 38.77it/s]\u001B[A\n",
      "Epoch 4:  86%|████████▌ | 540/629 [00:34<00:05, 15.87it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  87%|████████▋ | 545/629 [00:34<00:05, 15.95it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  87%|████████▋ | 550/629 [00:34<00:04, 16.03it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  88%|████████▊ | 555/629 [00:34<00:04, 16.12it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  57%|█████▋    | 98/172 [00:02<00:01, 37.44it/s]\u001B[A\n",
      "Epoch 4:  89%|████████▉ | 560/629 [00:34<00:04, 16.20it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  90%|████████▉ | 565/629 [00:34<00:03, 16.28it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  91%|█████████ | 570/629 [00:34<00:03, 16.36it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  91%|█████████▏| 575/629 [00:34<00:03, 16.44it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  69%|██████▊   | 118/172 [00:03<00:01, 37.02it/s]\u001B[A\n",
      "Epoch 4:  92%|█████████▏| 580/629 [00:35<00:02, 16.52it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  93%|█████████▎| 585/629 [00:35<00:02, 16.59it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  94%|█████████▍| 590/629 [00:35<00:02, 16.67it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  95%|█████████▍| 595/629 [00:35<00:02, 16.75it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  95%|█████████▌| 600/629 [00:35<00:01, 16.83it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  83%|████████▎ | 143/172 [00:04<00:00, 37.74it/s]\u001B[A\n",
      "Epoch 4:  96%|█████████▌| 605/629 [00:35<00:01, 16.91it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  97%|█████████▋| 610/629 [00:35<00:01, 16.98it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  98%|█████████▊| 615/629 [00:36<00:00, 17.06it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  99%|█████████▊| 620/629 [00:36<00:00, 17.14it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  99%|█████████▉| 625/629 [00:36<00:00, 17.22it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4: 100%|██████████| 629/629 [00:36<00:00, 17.24it/s, loss=1.14, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.130, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  73%|███████▎  | 457/629 [00:32<00:12, 14.24it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  73%|███████▎  | 460/629 [00:32<00:11, 14.16it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  74%|███████▍  | 465/629 [00:32<00:11, 14.26it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  75%|███████▍  | 470/629 [00:32<00:11, 14.36it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 26.90it/s]\u001B[A\n",
      "Epoch 5:  76%|███████▌  | 475/629 [00:32<00:10, 14.45it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  76%|███████▋  | 480/629 [00:33<00:10, 14.53it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  77%|███████▋  | 485/629 [00:33<00:09, 14.62it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  78%|███████▊  | 490/629 [00:33<00:09, 14.71it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:04, 34.60it/s]\u001B[A\n",
      "Epoch 5:  79%|███████▊  | 495/629 [00:33<00:09, 14.80it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  79%|███████▉  | 500/629 [00:33<00:08, 14.89it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  80%|████████  | 505/629 [00:33<00:08, 14.98it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  81%|████████  | 510/629 [00:33<00:07, 15.07it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 36.29it/s]\u001B[A\n",
      "Epoch 5:  82%|████████▏ | 515/629 [00:33<00:07, 15.15it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  83%|████████▎ | 520/629 [00:34<00:07, 15.24it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  83%|████████▎ | 525/629 [00:34<00:06, 15.33it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  84%|████████▍ | 530/629 [00:34<00:06, 15.41it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  42%|████▏     | 73/172 [00:02<00:02, 36.99it/s]\u001B[A\n",
      "Epoch 5:  85%|████████▌ | 535/629 [00:34<00:06, 15.50it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  86%|████████▌ | 540/629 [00:34<00:05, 15.58it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  87%|████████▋ | 545/629 [00:34<00:05, 15.66it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  87%|████████▋ | 550/629 [00:34<00:05, 15.74it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  54%|█████▍    | 93/172 [00:02<00:02, 35.48it/s]\u001B[A\n",
      "Epoch 5:  88%|████████▊ | 555/629 [00:35<00:04, 15.82it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  89%|████████▉ | 560/629 [00:35<00:04, 15.90it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  90%|████████▉ | 565/629 [00:35<00:04, 15.98it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  91%|█████████ | 570/629 [00:35<00:03, 16.05it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  66%|██████▌   | 113/172 [00:03<00:01, 34.25it/s]\u001B[A\n",
      "Epoch 5:  91%|█████████▏| 575/629 [00:35<00:03, 16.13it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  92%|█████████▏| 580/629 [00:35<00:03, 16.20it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  93%|█████████▎| 585/629 [00:35<00:02, 16.27it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  94%|█████████▍| 590/629 [00:36<00:02, 16.35it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  77%|███████▋  | 133/172 [00:03<00:01, 35.08it/s]\u001B[A\n",
      "Epoch 5:  95%|█████████▍| 595/629 [00:36<00:02, 16.42it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  95%|█████████▌| 600/629 [00:36<00:01, 16.50it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  96%|█████████▌| 605/629 [00:36<00:01, 16.58it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  97%|█████████▋| 610/629 [00:36<00:01, 16.65it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  98%|█████████▊| 615/629 [00:36<00:00, 16.74it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  99%|█████████▊| 620/629 [00:36<00:00, 16.82it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  99%|█████████▉| 625/629 [00:36<00:00, 16.90it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5: 100%|██████████| 629/629 [00:37<00:00, 16.92it/s, loss=1.16, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.160, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  73%|███████▎  | 457/629 [00:32<00:12, 13.93it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  73%|███████▎  | 460/629 [00:33<00:12, 13.86it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  74%|███████▍  | 465/629 [00:33<00:11, 13.95it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:   5%|▍         | 8/172 [00:00<00:08, 20.24it/s]\u001B[A\n",
      "Epoch 6:  75%|███████▍  | 470/629 [00:33<00:11, 14.05it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  76%|███████▌  | 475/629 [00:33<00:10, 14.15it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  76%|███████▋  | 480/629 [00:33<00:10, 14.24it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  77%|███████▋  | 485/629 [00:33<00:10, 14.32it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  16%|█▋        | 28/172 [00:01<00:04, 33.63it/s]\u001B[A\n",
      "Epoch 6:  78%|███████▊  | 490/629 [00:34<00:09, 14.41it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  79%|███████▊  | 495/629 [00:34<00:09, 14.50it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  79%|███████▉  | 500/629 [00:34<00:08, 14.59it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  80%|████████  | 505/629 [00:34<00:08, 14.68it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  28%|██▊       | 48/172 [00:01<00:03, 36.21it/s]\u001B[A\n",
      "Epoch 6:  81%|████████  | 510/629 [00:34<00:08, 14.77it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  82%|████████▏ | 515/629 [00:34<00:07, 14.85it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  83%|████████▎ | 520/629 [00:34<00:07, 14.94it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  83%|████████▎ | 525/629 [00:34<00:06, 15.02it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  40%|███▉      | 68/172 [00:02<00:02, 36.26it/s]\u001B[A\n",
      "Epoch 6:  84%|████████▍ | 530/629 [00:35<00:06, 15.11it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  85%|████████▌ | 535/629 [00:35<00:06, 15.19it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  86%|████████▌ | 540/629 [00:35<00:05, 15.26it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  87%|████████▋ | 545/629 [00:35<00:05, 15.34it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  51%|█████     | 88/172 [00:02<00:02, 34.74it/s]\u001B[A\n",
      "Epoch 6:  87%|████████▋ | 550/629 [00:35<00:05, 15.42it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  88%|████████▊ | 555/629 [00:35<00:04, 15.51it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  89%|████████▉ | 560/629 [00:35<00:04, 15.59it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  90%|████████▉ | 565/629 [00:36<00:04, 15.68it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  63%|██████▎   | 108/172 [00:03<00:01, 37.79it/s]\u001B[A\n",
      "Epoch 6:  91%|█████████ | 570/629 [00:36<00:03, 15.75it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  91%|█████████▏| 575/629 [00:36<00:03, 15.83it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  92%|█████████▏| 580/629 [00:36<00:03, 15.91it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  93%|█████████▎| 585/629 [00:36<00:02, 15.98it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  74%|███████▍  | 128/172 [00:03<00:01, 35.56it/s]\u001B[A\n",
      "Epoch 6:  94%|█████████▍| 590/629 [00:36<00:02, 16.06it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  95%|█████████▍| 595/629 [00:36<00:02, 16.13it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  95%|█████████▌| 600/629 [00:37<00:01, 16.21it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  96%|█████████▌| 605/629 [00:37<00:01, 16.28it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  86%|████████▌ | 148/172 [00:04<00:00, 36.73it/s]\u001B[A\n",
      "Epoch 6:  97%|█████████▋| 610/629 [00:37<00:01, 16.35it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  98%|█████████▊| 615/629 [00:37<00:00, 16.43it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  99%|█████████▊| 620/629 [00:37<00:00, 16.51it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  99%|█████████▉| 625/629 [00:37<00:00, 16.58it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6: 100%|██████████| 629/629 [00:37<00:00, 16.60it/s, loss=1.21, v_num=22, val_loss=1.090, val_sup_loss=0.0502, train_loss=1.200, sup_loss=0.0488, recon_loss=1.300]\n",
      "Epoch 7:   4%|▍         | 25/629 [00:02<00:49, 12.09it/s, loss=1.2, v_num=22, val_loss=1.090, val_sup_loss=0.0502, train_loss=1.200, sup_loss=0.0488, recon_loss=1.300]"
     ]
    }
   ],
   "source": [
    "p['input_size'] = len(features)\n",
    "p['output_size'] = 1\n",
    "train = True\n",
    "if train:\n",
    "    gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "    models=[]\n",
    "    for i, (train_idx,val_idx) in enumerate(gts.split(data,groups=era)):\n",
    "        dataset = utils.FinData(data=data, target=target, era=era)\n",
    "        dataloaders = utils.create_dataloaders(\n",
    "        dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
    "        model = SupAE(p)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=10,\n",
    "                            min_delta=0.005, mode='min')\n",
    "        trainer = pl.Trainer(max_epochs=100,\n",
    "                                gpus=1,\n",
    "                                callbacks=[es])\n",
    "        trainer.fit(\n",
    "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
    "        torch.save(model.state_dict(), f'saved_models/ae_fold_{i}_state_dict.pth')\n",
    "        models.append(model)\n",
    "else:\n",
    "    models_nn = utils.load_model('./saved_models/AE',p=p,pl_lightning=False,model=SupAE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = utils.load_data(root_dir='./data/',mode='test')\n",
    "df_test = df_test[df_test['data_type'] == 'validation']\n",
    "df = utils.load_data(root_dir='./data/',mode='train')\n",
    "data,target,features,era = utils.preprocess_data(df,nn=True)\n",
    "data_test,target_test,features_test,era_test = utils.preprocess_data(df_test,nn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([data,data_test],0)\n",
    "target = np.concatenate([target,target_test],0)\n",
    "era = np.concatenate([era,era_test],0)\n",
    "t_idx =np.where(era <121)[0].tolist()\n",
    "v_idx =np.where(era >=121)[0].tolist()\n",
    "data_dict = {'data': data, 'target': target,\n",
    "                 'features': features, 'era': era}\n",
    "#data = np.concatenate([data,data_dict['hidden']],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.linspace(start = 0, stop=data.shape[-1]-1,num=data.shape[-1],dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\tlearn: 0.2232421\ttest: 0.2234898\tbest: 0.2234898 (0)\ttotal: 306ms\tremaining: 5m 5s\n",
      "1:\tlearn: 0.2232420\ttest: 0.2234897\tbest: 0.2234897 (1)\ttotal: 315ms\tremaining: 2m 37s\n",
      "2:\tlearn: 0.2232133\ttest: 0.2234898\tbest: 0.2234897 (1)\ttotal: 626ms\tremaining: 3m 27s\n",
      "3:\tlearn: 0.2232133\ttest: 0.2234899\tbest: 0.2234897 (1)\ttotal: 637ms\tremaining: 2m 38s\n",
      "4:\tlearn: 0.2231877\ttest: 0.2234899\tbest: 0.2234897 (1)\ttotal: 940ms\tremaining: 3m 6s\n",
      "5:\tlearn: 0.2231877\ttest: 0.2234899\tbest: 0.2234897 (1)\ttotal: 946ms\tremaining: 2m 36s\n",
      "6:\tlearn: 0.2231589\ttest: 0.2234895\tbest: 0.2234895 (6)\ttotal: 1.26s\tremaining: 2m 58s\n",
      "7:\tlearn: 0.2231587\ttest: 0.2234895\tbest: 0.2234895 (7)\ttotal: 1.27s\tremaining: 2m 37s\n",
      "8:\tlearn: 0.2231306\ttest: 0.2234889\tbest: 0.2234889 (8)\ttotal: 1.58s\tremaining: 2m 53s\n",
      "9:\tlearn: 0.2231292\ttest: 0.2234889\tbest: 0.2234889 (8)\ttotal: 1.61s\tremaining: 2m 39s\n",
      "10:\tlearn: 0.2231292\ttest: 0.2234889\tbest: 0.2234889 (8)\ttotal: 1.62s\tremaining: 2m 25s\n",
      "11:\tlearn: 0.2230972\ttest: 0.2234887\tbest: 0.2234887 (11)\ttotal: 1.94s\tremaining: 2m 39s\n",
      "12:\tlearn: 0.2230967\ttest: 0.2234886\tbest: 0.2234886 (12)\ttotal: 1.95s\tremaining: 2m 28s\n",
      "13:\tlearn: 0.2230665\ttest: 0.2234883\tbest: 0.2234883 (13)\ttotal: 2.26s\tremaining: 2m 39s\n",
      "14:\tlearn: 0.2230664\ttest: 0.2234884\tbest: 0.2234883 (13)\ttotal: 2.27s\tremaining: 2m 29s\n",
      "15:\tlearn: 0.2230372\ttest: 0.2234888\tbest: 0.2234883 (13)\ttotal: 2.58s\tremaining: 2m 38s\n",
      "16:\tlearn: 0.2230371\ttest: 0.2234888\tbest: 0.2234883 (13)\ttotal: 2.59s\tremaining: 2m 29s\n",
      "17:\tlearn: 0.2230073\ttest: 0.2234888\tbest: 0.2234883 (13)\ttotal: 2.91s\tremaining: 2m 38s\n",
      "18:\tlearn: 0.2230069\ttest: 0.2234888\tbest: 0.2234883 (13)\ttotal: 2.93s\tremaining: 2m 31s\n",
      "19:\tlearn: 0.2229806\ttest: 0.2234886\tbest: 0.2234883 (13)\ttotal: 3.23s\tremaining: 2m 38s\n",
      "20:\tlearn: 0.2229806\ttest: 0.2234886\tbest: 0.2234883 (13)\ttotal: 3.23s\tremaining: 2m 30s\n",
      "21:\tlearn: 0.2229528\ttest: 0.2234884\tbest: 0.2234883 (13)\ttotal: 3.54s\tremaining: 2m 37s\n",
      "22:\tlearn: 0.2229514\ttest: 0.2234884\tbest: 0.2234883 (13)\ttotal: 3.57s\tremaining: 2m 31s\n",
      "23:\tlearn: 0.2229513\ttest: 0.2234883\tbest: 0.2234883 (13)\ttotal: 3.58s\tremaining: 2m 25s\n",
      "24:\tlearn: 0.2229218\ttest: 0.2234877\tbest: 0.2234877 (24)\ttotal: 3.88s\tremaining: 2m 31s\n",
      "25:\tlearn: 0.2229216\ttest: 0.2234876\tbest: 0.2234876 (25)\ttotal: 3.89s\tremaining: 2m 25s\n",
      "26:\tlearn: 0.2228872\ttest: 0.2234871\tbest: 0.2234871 (26)\ttotal: 4.21s\tremaining: 2m 31s\n",
      "27:\tlearn: 0.2228863\ttest: 0.2234871\tbest: 0.2234871 (26)\ttotal: 4.24s\tremaining: 2m 27s\n",
      "28:\tlearn: 0.2228863\ttest: 0.2234871\tbest: 0.2234871 (28)\ttotal: 4.25s\tremaining: 2m 22s\n",
      "29:\tlearn: 0.2228568\ttest: 0.2234869\tbest: 0.2234869 (29)\ttotal: 4.55s\tremaining: 2m 27s\n",
      "30:\tlearn: 0.2228568\ttest: 0.2234869\tbest: 0.2234869 (29)\ttotal: 4.57s\tremaining: 2m 22s\n",
      "31:\tlearn: 0.2228288\ttest: 0.2234866\tbest: 0.2234866 (31)\ttotal: 4.87s\tremaining: 2m 27s\n",
      "32:\tlearn: 0.2228288\ttest: 0.2234866\tbest: 0.2234866 (32)\ttotal: 4.88s\tremaining: 2m 23s\n",
      "33:\tlearn: 0.2228032\ttest: 0.2234864\tbest: 0.2234864 (33)\ttotal: 5.18s\tremaining: 2m 27s\n",
      "34:\tlearn: 0.2228031\ttest: 0.2234864\tbest: 0.2234864 (33)\ttotal: 5.19s\tremaining: 2m 23s\n",
      "35:\tlearn: 0.2227786\ttest: 0.2234865\tbest: 0.2234864 (33)\ttotal: 5.49s\tremaining: 2m 27s\n",
      "36:\tlearn: 0.2227782\ttest: 0.2234864\tbest: 0.2234864 (33)\ttotal: 5.5s\tremaining: 2m 23s\n",
      "37:\tlearn: 0.2227533\ttest: 0.2234863\tbest: 0.2234863 (37)\ttotal: 5.81s\tremaining: 2m 27s\n",
      "38:\tlearn: 0.2227514\ttest: 0.2234862\tbest: 0.2234862 (38)\ttotal: 5.85s\tremaining: 2m 24s\n",
      "39:\tlearn: 0.2227514\ttest: 0.2234862\tbest: 0.2234862 (39)\ttotal: 5.85s\tremaining: 2m 20s\n",
      "40:\tlearn: 0.2227193\ttest: 0.2234859\tbest: 0.2234859 (40)\ttotal: 6.17s\tremaining: 2m 24s\n",
      "41:\tlearn: 0.2227193\ttest: 0.2234859\tbest: 0.2234859 (40)\ttotal: 6.18s\tremaining: 2m 20s\n",
      "42:\tlearn: 0.2226935\ttest: 0.2234857\tbest: 0.2234857 (42)\ttotal: 6.48s\tremaining: 2m 24s\n",
      "43:\tlearn: 0.2226931\ttest: 0.2234856\tbest: 0.2234856 (43)\ttotal: 6.49s\tremaining: 2m 21s\n",
      "44:\tlearn: 0.2226602\ttest: 0.2234862\tbest: 0.2234856 (43)\ttotal: 6.83s\tremaining: 2m 24s\n",
      "45:\tlearn: 0.2226601\ttest: 0.2234861\tbest: 0.2234856 (43)\ttotal: 6.84s\tremaining: 2m 21s\n",
      "46:\tlearn: 0.2226296\ttest: 0.2234857\tbest: 0.2234856 (43)\ttotal: 7.16s\tremaining: 2m 25s\n",
      "47:\tlearn: 0.2226289\ttest: 0.2234858\tbest: 0.2234856 (43)\ttotal: 7.19s\tremaining: 2m 22s\n",
      "48:\tlearn: 0.2226009\ttest: 0.2234855\tbest: 0.2234855 (48)\ttotal: 7.5s\tremaining: 2m 25s\n",
      "49:\tlearn: 0.2226008\ttest: 0.2234855\tbest: 0.2234855 (49)\ttotal: 7.51s\tremaining: 2m 22s\n",
      "50:\tlearn: 0.2225738\ttest: 0.2234861\tbest: 0.2234855 (49)\ttotal: 7.82s\tremaining: 2m 25s\n",
      "51:\tlearn: 0.2225736\ttest: 0.2234860\tbest: 0.2234855 (49)\ttotal: 7.83s\tremaining: 2m 22s\n",
      "52:\tlearn: 0.2225413\ttest: 0.2234863\tbest: 0.2234855 (49)\ttotal: 8.15s\tremaining: 2m 25s\n",
      "53:\tlearn: 0.2225412\ttest: 0.2234863\tbest: 0.2234855 (49)\ttotal: 8.16s\tremaining: 2m 22s\n",
      "54:\tlearn: 0.2225112\ttest: 0.2234856\tbest: 0.2234855 (49)\ttotal: 8.47s\tremaining: 2m 25s\n",
      "55:\tlearn: 0.2225111\ttest: 0.2234856\tbest: 0.2234855 (49)\ttotal: 8.48s\tremaining: 2m 23s\n",
      "56:\tlearn: 0.2224836\ttest: 0.2234857\tbest: 0.2234855 (49)\ttotal: 8.79s\tremaining: 2m 25s\n",
      "57:\tlearn: 0.2224835\ttest: 0.2234857\tbest: 0.2234855 (49)\ttotal: 8.8s\tremaining: 2m 22s\n",
      "58:\tlearn: 0.2224574\ttest: 0.2234856\tbest: 0.2234855 (49)\ttotal: 9.11s\tremaining: 2m 25s\n",
      "59:\tlearn: 0.2224573\ttest: 0.2234855\tbest: 0.2234855 (49)\ttotal: 9.12s\tremaining: 2m 22s\n",
      "60:\tlearn: 0.2224326\ttest: 0.2234859\tbest: 0.2234855 (49)\ttotal: 9.43s\tremaining: 2m 25s\n",
      "61:\tlearn: 0.2224323\ttest: 0.2234857\tbest: 0.2234855 (49)\ttotal: 9.44s\tremaining: 2m 22s\n",
      "62:\tlearn: 0.2224032\ttest: 0.2234855\tbest: 0.2234855 (49)\ttotal: 9.75s\tremaining: 2m 25s\n",
      "63:\tlearn: 0.2224023\ttest: 0.2234855\tbest: 0.2234855 (63)\ttotal: 9.77s\tremaining: 2m 22s\n",
      "64:\tlearn: 0.2223732\ttest: 0.2234852\tbest: 0.2234852 (64)\ttotal: 10.1s\tremaining: 2m 25s\n",
      "65:\tlearn: 0.2223728\ttest: 0.2234852\tbest: 0.2234852 (65)\ttotal: 10.1s\tremaining: 2m 22s\n",
      "66:\tlearn: 0.2223485\ttest: 0.2234854\tbest: 0.2234852 (65)\ttotal: 10.4s\tremaining: 2m 24s\n",
      "67:\tlearn: 0.2223485\ttest: 0.2234854\tbest: 0.2234852 (65)\ttotal: 10.4s\tremaining: 2m 22s\n",
      "68:\tlearn: 0.2223226\ttest: 0.2234853\tbest: 0.2234852 (65)\ttotal: 10.7s\tremaining: 2m 24s\n",
      "69:\tlearn: 0.2223226\ttest: 0.2234853\tbest: 0.2234852 (65)\ttotal: 10.7s\tremaining: 2m 22s\n",
      "70:\tlearn: 0.2222934\ttest: 0.2234857\tbest: 0.2234852 (65)\ttotal: 11s\tremaining: 2m 24s\n",
      "71:\tlearn: 0.2222933\ttest: 0.2234857\tbest: 0.2234852 (65)\ttotal: 11.1s\tremaining: 2m 22s\n",
      "72:\tlearn: 0.2222607\ttest: 0.2234849\tbest: 0.2234849 (72)\ttotal: 11.4s\tremaining: 2m 24s\n",
      "73:\tlearn: 0.2222606\ttest: 0.2234849\tbest: 0.2234849 (73)\ttotal: 11.4s\tremaining: 2m 22s\n",
      "74:\tlearn: 0.2222334\ttest: 0.2234849\tbest: 0.2234849 (74)\ttotal: 11.7s\tremaining: 2m 24s\n",
      "75:\tlearn: 0.2222332\ttest: 0.2234849\tbest: 0.2234849 (75)\ttotal: 11.7s\tremaining: 2m 22s\n",
      "76:\tlearn: 0.2222031\ttest: 0.2234847\tbest: 0.2234847 (76)\ttotal: 12s\tremaining: 2m 23s\n",
      "77:\tlearn: 0.2222029\ttest: 0.2234847\tbest: 0.2234847 (76)\ttotal: 12s\tremaining: 2m 22s\n",
      "78:\tlearn: 0.2221708\ttest: 0.2234843\tbest: 0.2234843 (78)\ttotal: 12.3s\tremaining: 2m 23s\n",
      "79:\tlearn: 0.2221702\ttest: 0.2234843\tbest: 0.2234843 (79)\ttotal: 12.4s\tremaining: 2m 22s\n",
      "80:\tlearn: 0.2221425\ttest: 0.2234845\tbest: 0.2234843 (79)\ttotal: 12.7s\tremaining: 2m 23s\n",
      "81:\tlearn: 0.2221424\ttest: 0.2234845\tbest: 0.2234843 (79)\ttotal: 12.7s\tremaining: 2m 21s\n",
      "82:\tlearn: 0.2221120\ttest: 0.2234845\tbest: 0.2234843 (79)\ttotal: 13s\tremaining: 2m 23s\n",
      "83:\tlearn: 0.2221110\ttest: 0.2234844\tbest: 0.2234843 (79)\ttotal: 13s\tremaining: 2m 21s\n",
      "84:\tlearn: 0.2221110\ttest: 0.2234844\tbest: 0.2234843 (79)\ttotal: 13s\tremaining: 2m 20s\n",
      "85:\tlearn: 0.2220826\ttest: 0.2234842\tbest: 0.2234842 (85)\ttotal: 13.3s\tremaining: 2m 21s\n",
      "86:\tlearn: 0.2220825\ttest: 0.2234842\tbest: 0.2234842 (86)\ttotal: 13.3s\tremaining: 2m 20s\n",
      "87:\tlearn: 0.2220539\ttest: 0.2234840\tbest: 0.2234840 (87)\ttotal: 13.7s\tremaining: 2m 21s\n",
      "88:\tlearn: 0.2220539\ttest: 0.2234840\tbest: 0.2234840 (87)\ttotal: 13.7s\tremaining: 2m 19s\n",
      "89:\tlearn: 0.2220271\ttest: 0.2234839\tbest: 0.2234839 (89)\ttotal: 14s\tremaining: 2m 21s\n",
      "90:\tlearn: 0.2220269\ttest: 0.2234837\tbest: 0.2234837 (90)\ttotal: 14s\tremaining: 2m 19s\n",
      "91:\tlearn: 0.2220009\ttest: 0.2234838\tbest: 0.2234837 (90)\ttotal: 14.3s\tremaining: 2m 20s\n",
      "92:\tlearn: 0.2219982\ttest: 0.2234842\tbest: 0.2234837 (90)\ttotal: 14.3s\tremaining: 2m 19s\n",
      "93:\tlearn: 0.2219981\ttest: 0.2234842\tbest: 0.2234837 (90)\ttotal: 14.3s\tremaining: 2m 18s\n",
      "94:\tlearn: 0.2219706\ttest: 0.2234840\tbest: 0.2234837 (90)\ttotal: 14.6s\tremaining: 2m 19s\n",
      "95:\tlearn: 0.2219698\ttest: 0.2234839\tbest: 0.2234837 (90)\ttotal: 14.7s\tremaining: 2m 18s\n",
      "96:\tlearn: 0.2219361\ttest: 0.2234839\tbest: 0.2234837 (90)\ttotal: 15s\tremaining: 2m 19s\n",
      "97:\tlearn: 0.2219361\ttest: 0.2234839\tbest: 0.2234837 (90)\ttotal: 15s\tremaining: 2m 18s\n",
      "98:\tlearn: 0.2219108\ttest: 0.2234839\tbest: 0.2234837 (90)\ttotal: 15.3s\tremaining: 2m 19s\n",
      "99:\tlearn: 0.2219107\ttest: 0.2234838\tbest: 0.2234837 (90)\ttotal: 15.3s\tremaining: 2m 17s\n",
      "100:\tlearn: 0.2218827\ttest: 0.2234833\tbest: 0.2234833 (100)\ttotal: 15.6s\tremaining: 2m 18s\n",
      "101:\tlearn: 0.2218825\ttest: 0.2234833\tbest: 0.2234833 (101)\ttotal: 15.6s\tremaining: 2m 17s\n",
      "102:\tlearn: 0.2218541\ttest: 0.2234830\tbest: 0.2234830 (102)\ttotal: 15.9s\tremaining: 2m 18s\n",
      "103:\tlearn: 0.2218362\ttest: 0.2234831\tbest: 0.2234830 (102)\ttotal: 16.2s\tremaining: 2m 19s\n",
      "104:\tlearn: 0.2218362\ttest: 0.2234832\tbest: 0.2234830 (102)\ttotal: 16.2s\tremaining: 2m 18s\n",
      "105:\tlearn: 0.2218083\ttest: 0.2234830\tbest: 0.2234830 (105)\ttotal: 16.5s\tremaining: 2m 19s\n",
      "106:\tlearn: 0.2218083\ttest: 0.2234830\tbest: 0.2234830 (105)\ttotal: 16.5s\tremaining: 2m 17s\n",
      "107:\tlearn: 0.2217835\ttest: 0.2234827\tbest: 0.2234827 (107)\ttotal: 16.8s\tremaining: 2m 19s\n",
      "108:\tlearn: 0.2217835\ttest: 0.2234827\tbest: 0.2234827 (108)\ttotal: 16.8s\tremaining: 2m 17s\n",
      "109:\tlearn: 0.2217555\ttest: 0.2234826\tbest: 0.2234826 (109)\ttotal: 17.1s\tremaining: 2m 18s\n",
      "110:\tlearn: 0.2217554\ttest: 0.2234826\tbest: 0.2234826 (110)\ttotal: 17.2s\tremaining: 2m 17s\n",
      "111:\tlearn: 0.2217228\ttest: 0.2234825\tbest: 0.2234825 (111)\ttotal: 17.5s\tremaining: 2m 18s\n",
      "112:\tlearn: 0.2217226\ttest: 0.2234825\tbest: 0.2234825 (112)\ttotal: 17.5s\tremaining: 2m 17s\n",
      "113:\tlearn: 0.2216889\ttest: 0.2234829\tbest: 0.2234825 (112)\ttotal: 17.8s\tremaining: 2m 18s\n",
      "114:\tlearn: 0.2216845\ttest: 0.2234829\tbest: 0.2234825 (112)\ttotal: 17.9s\tremaining: 2m 17s\n",
      "115:\tlearn: 0.2216844\ttest: 0.2234828\tbest: 0.2234825 (112)\ttotal: 17.9s\tremaining: 2m 16s\n",
      "116:\tlearn: 0.2216537\ttest: 0.2234830\tbest: 0.2234825 (112)\ttotal: 18.2s\tremaining: 2m 17s\n",
      "117:\tlearn: 0.2216535\ttest: 0.2234830\tbest: 0.2234825 (112)\ttotal: 18.2s\tremaining: 2m 16s\n",
      "118:\tlearn: 0.2216263\ttest: 0.2234828\tbest: 0.2234825 (112)\ttotal: 18.6s\tremaining: 2m 17s\n",
      "119:\tlearn: 0.2216263\ttest: 0.2234827\tbest: 0.2234825 (112)\ttotal: 18.6s\tremaining: 2m 16s\n",
      "120:\tlearn: 0.2215931\ttest: 0.2234823\tbest: 0.2234823 (120)\ttotal: 18.9s\tremaining: 2m 17s\n",
      "121:\tlearn: 0.2215928\ttest: 0.2234823\tbest: 0.2234823 (121)\ttotal: 18.9s\tremaining: 2m 16s\n",
      "122:\tlearn: 0.2215664\ttest: 0.2234825\tbest: 0.2234823 (121)\ttotal: 19.2s\tremaining: 2m 16s\n",
      "123:\tlearn: 0.2215663\ttest: 0.2234825\tbest: 0.2234823 (121)\ttotal: 19.2s\tremaining: 2m 15s\n",
      "124:\tlearn: 0.2215356\ttest: 0.2234824\tbest: 0.2234823 (121)\ttotal: 19.5s\tremaining: 2m 16s\n",
      "125:\tlearn: 0.2215355\ttest: 0.2234825\tbest: 0.2234823 (121)\ttotal: 19.5s\tremaining: 2m 15s\n",
      "126:\tlearn: 0.2215066\ttest: 0.2234821\tbest: 0.2234821 (126)\ttotal: 19.8s\tremaining: 2m 16s\n",
      "127:\tlearn: 0.2215065\ttest: 0.2234821\tbest: 0.2234821 (127)\ttotal: 19.9s\tremaining: 2m 15s\n",
      "128:\tlearn: 0.2214798\ttest: 0.2234817\tbest: 0.2234817 (128)\ttotal: 20.2s\tremaining: 2m 16s\n",
      "129:\tlearn: 0.2214749\ttest: 0.2234814\tbest: 0.2234814 (129)\ttotal: 20.3s\tremaining: 2m 15s\n",
      "130:\tlearn: 0.2214749\ttest: 0.2234814\tbest: 0.2234814 (129)\ttotal: 20.3s\tremaining: 2m 14s\n",
      "131:\tlearn: 0.2214517\ttest: 0.2234814\tbest: 0.2234814 (129)\ttotal: 20.6s\tremaining: 2m 15s\n",
      "132:\tlearn: 0.2214515\ttest: 0.2234814\tbest: 0.2234814 (129)\ttotal: 20.6s\tremaining: 2m 14s\n",
      "133:\tlearn: 0.2214186\ttest: 0.2234811\tbest: 0.2234811 (133)\ttotal: 20.9s\tremaining: 2m 15s\n",
      "134:\tlearn: 0.2214153\ttest: 0.2234808\tbest: 0.2234808 (134)\ttotal: 21s\tremaining: 2m 14s\n",
      "135:\tlearn: 0.2214150\ttest: 0.2234808\tbest: 0.2234808 (135)\ttotal: 21s\tremaining: 2m 13s\n",
      "136:\tlearn: 0.2213906\ttest: 0.2234806\tbest: 0.2234806 (136)\ttotal: 21.3s\tremaining: 2m 14s\n",
      "137:\tlearn: 0.2213906\ttest: 0.2234806\tbest: 0.2234806 (136)\ttotal: 21.3s\tremaining: 2m 13s\n",
      "138:\tlearn: 0.2213594\ttest: 0.2234809\tbest: 0.2234806 (136)\ttotal: 21.6s\tremaining: 2m 13s\n",
      "139:\tlearn: 0.2213594\ttest: 0.2234809\tbest: 0.2234806 (136)\ttotal: 21.6s\tremaining: 2m 12s\n",
      "140:\tlearn: 0.2213273\ttest: 0.2234809\tbest: 0.2234806 (136)\ttotal: 22s\tremaining: 2m 13s\n",
      "141:\tlearn: 0.2213270\ttest: 0.2234808\tbest: 0.2234806 (136)\ttotal: 22s\tremaining: 2m 12s\n",
      "142:\tlearn: 0.2212963\ttest: 0.2234808\tbest: 0.2234806 (136)\ttotal: 22.3s\tremaining: 2m 13s\n",
      "143:\tlearn: 0.2212924\ttest: 0.2234805\tbest: 0.2234805 (143)\ttotal: 22.3s\tremaining: 2m 12s\n",
      "144:\tlearn: 0.2212924\ttest: 0.2234805\tbest: 0.2234805 (144)\ttotal: 22.3s\tremaining: 2m 11s\n",
      "145:\tlearn: 0.2212630\ttest: 0.2234802\tbest: 0.2234802 (145)\ttotal: 22.7s\tremaining: 2m 12s\n",
      "146:\tlearn: 0.2212617\ttest: 0.2234802\tbest: 0.2234802 (145)\ttotal: 22.7s\tremaining: 2m 11s\n",
      "147:\tlearn: 0.2212616\ttest: 0.2234802\tbest: 0.2234802 (147)\ttotal: 22.7s\tremaining: 2m 10s\n",
      "148:\tlearn: 0.2212382\ttest: 0.2234803\tbest: 0.2234802 (147)\ttotal: 23s\tremaining: 2m 11s\n",
      "149:\tlearn: 0.2212348\ttest: 0.2234802\tbest: 0.2234802 (147)\ttotal: 23s\tremaining: 2m 10s\n",
      "150:\tlearn: 0.2212348\ttest: 0.2234802\tbest: 0.2234802 (147)\ttotal: 23s\tremaining: 2m 9s\n",
      "151:\tlearn: 0.2212028\ttest: 0.2234799\tbest: 0.2234799 (151)\ttotal: 23.4s\tremaining: 2m 10s\n",
      "152:\tlearn: 0.2212027\ttest: 0.2234799\tbest: 0.2234799 (152)\ttotal: 23.4s\tremaining: 2m 9s\n",
      "153:\tlearn: 0.2211713\ttest: 0.2234799\tbest: 0.2234799 (152)\ttotal: 23.7s\tremaining: 2m 10s\n",
      "154:\tlearn: 0.2211711\ttest: 0.2234799\tbest: 0.2234799 (152)\ttotal: 23.7s\tremaining: 2m 9s\n",
      "155:\tlearn: 0.2211390\ttest: 0.2234798\tbest: 0.2234798 (155)\ttotal: 24s\tremaining: 2m 9s\n",
      "156:\tlearn: 0.2211388\ttest: 0.2234798\tbest: 0.2234798 (156)\ttotal: 24s\tremaining: 2m 9s\n",
      "157:\tlearn: 0.2211141\ttest: 0.2234793\tbest: 0.2234793 (157)\ttotal: 24.3s\tremaining: 2m 9s\n",
      "158:\tlearn: 0.2211135\ttest: 0.2234793\tbest: 0.2234793 (158)\ttotal: 24.4s\tremaining: 2m 8s\n",
      "159:\tlearn: 0.2210868\ttest: 0.2234789\tbest: 0.2234789 (159)\ttotal: 24.7s\tremaining: 2m 9s\n",
      "160:\tlearn: 0.2210849\ttest: 0.2234790\tbest: 0.2234789 (159)\ttotal: 24.7s\tremaining: 2m 8s\n",
      "161:\tlearn: 0.2210849\ttest: 0.2234790\tbest: 0.2234789 (159)\ttotal: 24.7s\tremaining: 2m 7s\n",
      "162:\tlearn: 0.2210539\ttest: 0.2234788\tbest: 0.2234788 (162)\ttotal: 25s\tremaining: 2m 8s\n",
      "163:\tlearn: 0.2210539\ttest: 0.2234787\tbest: 0.2234787 (163)\ttotal: 25s\tremaining: 2m 7s\n",
      "164:\tlearn: 0.2210232\ttest: 0.2234790\tbest: 0.2234787 (163)\ttotal: 25.4s\tremaining: 2m 8s\n",
      "165:\tlearn: 0.2210231\ttest: 0.2234790\tbest: 0.2234787 (163)\ttotal: 25.4s\tremaining: 2m 7s\n",
      "166:\tlearn: 0.2209930\ttest: 0.2234790\tbest: 0.2234787 (163)\ttotal: 25.7s\tremaining: 2m 8s\n",
      "167:\tlearn: 0.2209794\ttest: 0.2234789\tbest: 0.2234787 (163)\ttotal: 26s\tremaining: 2m 8s\n",
      "168:\tlearn: 0.2209794\ttest: 0.2234788\tbest: 0.2234787 (163)\ttotal: 26s\tremaining: 2m 7s\n",
      "169:\tlearn: 0.2209574\ttest: 0.2234783\tbest: 0.2234783 (169)\ttotal: 26.3s\tremaining: 2m 8s\n",
      "170:\tlearn: 0.2209573\ttest: 0.2234783\tbest: 0.2234783 (169)\ttotal: 26.3s\tremaining: 2m 7s\n",
      "171:\tlearn: 0.2209258\ttest: 0.2234783\tbest: 0.2234783 (171)\ttotal: 26.6s\tremaining: 2m 8s\n",
      "172:\tlearn: 0.2209256\ttest: 0.2234783\tbest: 0.2234783 (171)\ttotal: 26.6s\tremaining: 2m 7s\n",
      "173:\tlearn: 0.2208940\ttest: 0.2234778\tbest: 0.2234778 (173)\ttotal: 26.9s\tremaining: 2m 7s\n",
      "174:\tlearn: 0.2208936\ttest: 0.2234778\tbest: 0.2234778 (174)\ttotal: 26.9s\tremaining: 2m 6s\n",
      "175:\tlearn: 0.2208646\ttest: 0.2234775\tbest: 0.2234775 (175)\ttotal: 27.2s\tremaining: 2m 7s\n",
      "176:\tlearn: 0.2208594\ttest: 0.2234773\tbest: 0.2234773 (176)\ttotal: 27.4s\tremaining: 2m 7s\n",
      "177:\tlearn: 0.2208594\ttest: 0.2234774\tbest: 0.2234773 (176)\ttotal: 27.4s\tremaining: 2m 6s\n",
      "178:\tlearn: 0.2208315\ttest: 0.2234773\tbest: 0.2234773 (178)\ttotal: 27.7s\tremaining: 2m 7s\n",
      "179:\tlearn: 0.2208313\ttest: 0.2234772\tbest: 0.2234772 (179)\ttotal: 27.7s\tremaining: 2m 6s\n",
      "180:\tlearn: 0.2208084\ttest: 0.2234767\tbest: 0.2234767 (180)\ttotal: 28s\tremaining: 2m 6s\n",
      "181:\tlearn: 0.2208080\ttest: 0.2234766\tbest: 0.2234766 (181)\ttotal: 28s\tremaining: 2m 5s\n",
      "182:\tlearn: 0.2207789\ttest: 0.2234767\tbest: 0.2234766 (181)\ttotal: 28.3s\tremaining: 2m 6s\n",
      "183:\tlearn: 0.2207788\ttest: 0.2234767\tbest: 0.2234766 (181)\ttotal: 28.3s\tremaining: 2m 5s\n",
      "184:\tlearn: 0.2207519\ttest: 0.2234767\tbest: 0.2234766 (181)\ttotal: 28.6s\tremaining: 2m 6s\n",
      "185:\tlearn: 0.2207517\ttest: 0.2234767\tbest: 0.2234766 (181)\ttotal: 28.7s\tremaining: 2m 5s\n",
      "186:\tlearn: 0.2207181\ttest: 0.2234764\tbest: 0.2234764 (186)\ttotal: 29s\tremaining: 2m 6s\n",
      "187:\tlearn: 0.2207179\ttest: 0.2234763\tbest: 0.2234763 (187)\ttotal: 29s\tremaining: 2m 5s\n",
      "188:\tlearn: 0.2206905\ttest: 0.2234755\tbest: 0.2234755 (188)\ttotal: 29.3s\tremaining: 2m 5s\n",
      "189:\tlearn: 0.2206904\ttest: 0.2234755\tbest: 0.2234755 (188)\ttotal: 29.3s\tremaining: 2m 4s\n",
      "190:\tlearn: 0.2206636\ttest: 0.2234754\tbest: 0.2234754 (190)\ttotal: 29.6s\tremaining: 2m 5s\n",
      "191:\tlearn: 0.2206629\ttest: 0.2234754\tbest: 0.2234754 (191)\ttotal: 29.6s\tremaining: 2m 4s\n",
      "192:\tlearn: 0.2206371\ttest: 0.2234752\tbest: 0.2234752 (192)\ttotal: 29.9s\tremaining: 2m 5s\n",
      "193:\tlearn: 0.2206363\ttest: 0.2234751\tbest: 0.2234751 (193)\ttotal: 30s\tremaining: 2m 4s\n",
      "194:\tlearn: 0.2206070\ttest: 0.2234751\tbest: 0.2234751 (193)\ttotal: 30.3s\tremaining: 2m 5s\n",
      "195:\tlearn: 0.2205981\ttest: 0.2234750\tbest: 0.2234750 (195)\ttotal: 30.5s\tremaining: 2m 5s\n",
      "196:\tlearn: 0.2205980\ttest: 0.2234749\tbest: 0.2234749 (196)\ttotal: 30.5s\tremaining: 2m 4s\n",
      "197:\tlearn: 0.2205703\ttest: 0.2234749\tbest: 0.2234749 (197)\ttotal: 30.8s\tremaining: 2m 4s\n",
      "198:\tlearn: 0.2205703\ttest: 0.2234749\tbest: 0.2234749 (197)\ttotal: 30.8s\tremaining: 2m 4s\n",
      "199:\tlearn: 0.2205437\ttest: 0.2234749\tbest: 0.2234749 (197)\ttotal: 31.2s\tremaining: 2m 4s\n",
      "200:\tlearn: 0.2205433\ttest: 0.2234748\tbest: 0.2234748 (200)\ttotal: 31.2s\tremaining: 2m 3s\n",
      "201:\tlearn: 0.2205160\ttest: 0.2234747\tbest: 0.2234747 (201)\ttotal: 31.5s\tremaining: 2m 4s\n",
      "202:\tlearn: 0.2205157\ttest: 0.2234746\tbest: 0.2234746 (202)\ttotal: 31.5s\tremaining: 2m 3s\n",
      "203:\tlearn: 0.2204901\ttest: 0.2234743\tbest: 0.2234743 (203)\ttotal: 31.8s\tremaining: 2m 4s\n",
      "204:\tlearn: 0.2204901\ttest: 0.2234743\tbest: 0.2234743 (204)\ttotal: 31.8s\tremaining: 2m 3s\n",
      "205:\tlearn: 0.2204587\ttest: 0.2234742\tbest: 0.2234742 (205)\ttotal: 32.1s\tremaining: 2m 3s\n",
      "206:\tlearn: 0.2204587\ttest: 0.2234742\tbest: 0.2234742 (205)\ttotal: 32.1s\tremaining: 2m 3s\n",
      "207:\tlearn: 0.2204361\ttest: 0.2234744\tbest: 0.2234742 (205)\ttotal: 32.4s\tremaining: 2m 3s\n",
      "208:\tlearn: 0.2204359\ttest: 0.2234743\tbest: 0.2234742 (205)\ttotal: 32.4s\tremaining: 2m 2s\n",
      "209:\tlearn: 0.2204055\ttest: 0.2234741\tbest: 0.2234741 (209)\ttotal: 32.7s\tremaining: 2m 3s\n",
      "210:\tlearn: 0.2204046\ttest: 0.2234740\tbest: 0.2234740 (210)\ttotal: 32.8s\tremaining: 2m 2s\n",
      "211:\tlearn: 0.2203749\ttest: 0.2234739\tbest: 0.2234739 (211)\ttotal: 33.1s\tremaining: 2m 2s\n",
      "212:\tlearn: 0.2203748\ttest: 0.2234739\tbest: 0.2234739 (211)\ttotal: 33.1s\tremaining: 2m 2s\n",
      "213:\tlearn: 0.2203494\ttest: 0.2234740\tbest: 0.2234739 (211)\ttotal: 33.4s\tremaining: 2m 2s\n",
      "214:\tlearn: 0.2203493\ttest: 0.2234740\tbest: 0.2234739 (211)\ttotal: 33.4s\tremaining: 2m 1s\n",
      "215:\tlearn: 0.2203256\ttest: 0.2234732\tbest: 0.2234732 (215)\ttotal: 33.7s\tremaining: 2m 2s\n",
      "216:\tlearn: 0.2203251\ttest: 0.2234731\tbest: 0.2234731 (216)\ttotal: 33.7s\tremaining: 2m 1s\n",
      "217:\tlearn: 0.2202960\ttest: 0.2234733\tbest: 0.2234731 (216)\ttotal: 34s\tremaining: 2m 2s\n",
      "218:\tlearn: 0.2202959\ttest: 0.2234733\tbest: 0.2234731 (216)\ttotal: 34s\tremaining: 2m 1s\n",
      "219:\tlearn: 0.2202708\ttest: 0.2234734\tbest: 0.2234731 (216)\ttotal: 34.4s\tremaining: 2m 1s\n",
      "220:\tlearn: 0.2202705\ttest: 0.2234733\tbest: 0.2234731 (216)\ttotal: 34.4s\tremaining: 2m 1s\n",
      "221:\tlearn: 0.2202468\ttest: 0.2234730\tbest: 0.2234730 (221)\ttotal: 34.7s\tremaining: 2m 1s\n",
      "222:\tlearn: 0.2202466\ttest: 0.2234729\tbest: 0.2234729 (222)\ttotal: 34.7s\tremaining: 2m\n",
      "223:\tlearn: 0.2202207\ttest: 0.2234726\tbest: 0.2234726 (223)\ttotal: 35s\tremaining: 2m 1s\n",
      "224:\tlearn: 0.2202204\ttest: 0.2234726\tbest: 0.2234726 (224)\ttotal: 35s\tremaining: 2m\n",
      "225:\tlearn: 0.2201946\ttest: 0.2234722\tbest: 0.2234722 (225)\ttotal: 35.3s\tremaining: 2m\n",
      "226:\tlearn: 0.2201946\ttest: 0.2234722\tbest: 0.2234722 (226)\ttotal: 35.3s\tremaining: 2m\n",
      "227:\tlearn: 0.2201647\ttest: 0.2234724\tbest: 0.2234722 (226)\ttotal: 35.6s\tremaining: 2m\n",
      "228:\tlearn: 0.2201645\ttest: 0.2234723\tbest: 0.2234722 (226)\ttotal: 35.6s\tremaining: 1m 59s\n",
      "229:\tlearn: 0.2201398\ttest: 0.2234718\tbest: 0.2234718 (229)\ttotal: 35.9s\tremaining: 2m\n",
      "230:\tlearn: 0.2201396\ttest: 0.2234717\tbest: 0.2234717 (230)\ttotal: 36s\tremaining: 1m 59s\n",
      "231:\tlearn: 0.2201157\ttest: 0.2234717\tbest: 0.2234717 (230)\ttotal: 36.3s\tremaining: 2m\n",
      "232:\tlearn: 0.2201128\ttest: 0.2234716\tbest: 0.2234716 (232)\ttotal: 36.3s\tremaining: 1m 59s\n",
      "233:\tlearn: 0.2201128\ttest: 0.2234716\tbest: 0.2234716 (233)\ttotal: 36.3s\tremaining: 1m 58s\n",
      "234:\tlearn: 0.2200871\ttest: 0.2234713\tbest: 0.2234713 (234)\ttotal: 36.6s\tremaining: 1m 59s\n",
      "235:\tlearn: 0.2200853\ttest: 0.2234713\tbest: 0.2234713 (235)\ttotal: 36.7s\tremaining: 1m 58s\n",
      "236:\tlearn: 0.2200853\ttest: 0.2234713\tbest: 0.2234713 (236)\ttotal: 36.7s\tremaining: 1m 58s\n",
      "237:\tlearn: 0.2200561\ttest: 0.2234714\tbest: 0.2234713 (236)\ttotal: 37s\tremaining: 1m 58s\n",
      "238:\tlearn: 0.2200558\ttest: 0.2234714\tbest: 0.2234713 (236)\ttotal: 37s\tremaining: 1m 57s\n",
      "239:\tlearn: 0.2200289\ttest: 0.2234718\tbest: 0.2234713 (236)\ttotal: 37.3s\tremaining: 1m 58s\n",
      "240:\tlearn: 0.2200288\ttest: 0.2234717\tbest: 0.2234713 (236)\ttotal: 37.3s\tremaining: 1m 57s\n",
      "241:\tlearn: 0.2200001\ttest: 0.2234721\tbest: 0.2234713 (236)\ttotal: 37.6s\tremaining: 1m 57s\n",
      "242:\tlearn: 0.2200001\ttest: 0.2234721\tbest: 0.2234713 (236)\ttotal: 37.6s\tremaining: 1m 57s\n",
      "243:\tlearn: 0.2199754\ttest: 0.2234721\tbest: 0.2234713 (236)\ttotal: 37.9s\tremaining: 1m 57s\n",
      "244:\tlearn: 0.2199752\ttest: 0.2234721\tbest: 0.2234713 (236)\ttotal: 38s\tremaining: 1m 56s\n",
      "245:\tlearn: 0.2199471\ttest: 0.2234723\tbest: 0.2234713 (236)\ttotal: 38.3s\tremaining: 1m 57s\n",
      "246:\tlearn: 0.2199471\ttest: 0.2234723\tbest: 0.2234713 (236)\ttotal: 38.3s\tremaining: 1m 56s\n",
      "247:\tlearn: 0.2199225\ttest: 0.2234724\tbest: 0.2234713 (236)\ttotal: 38.6s\tremaining: 1m 57s\n",
      "248:\tlearn: 0.2199225\ttest: 0.2234724\tbest: 0.2234713 (236)\ttotal: 38.6s\tremaining: 1m 56s\n",
      "249:\tlearn: 0.2198942\ttest: 0.2234726\tbest: 0.2234713 (236)\ttotal: 38.9s\tremaining: 1m 56s\n",
      "250:\tlearn: 0.2198942\ttest: 0.2234726\tbest: 0.2234713 (236)\ttotal: 38.9s\tremaining: 1m 56s\n",
      "251:\tlearn: 0.2198695\ttest: 0.2234726\tbest: 0.2234713 (236)\ttotal: 39.2s\tremaining: 1m 56s\n",
      "252:\tlearn: 0.2198694\ttest: 0.2234726\tbest: 0.2234713 (236)\ttotal: 39.2s\tremaining: 1m 55s\n",
      "253:\tlearn: 0.2198387\ttest: 0.2234723\tbest: 0.2234713 (236)\ttotal: 39.6s\tremaining: 1m 56s\n",
      "254:\tlearn: 0.2198387\ttest: 0.2234723\tbest: 0.2234713 (236)\ttotal: 39.6s\tremaining: 1m 55s\n",
      "255:\tlearn: 0.2198148\ttest: 0.2234718\tbest: 0.2234713 (236)\ttotal: 39.9s\tremaining: 1m 55s\n",
      "256:\tlearn: 0.2198117\ttest: 0.2234719\tbest: 0.2234713 (236)\ttotal: 39.9s\tremaining: 1m 55s\n",
      "257:\tlearn: 0.2198117\ttest: 0.2234719\tbest: 0.2234713 (236)\ttotal: 40s\tremaining: 1m 54s\n",
      "258:\tlearn: 0.2197785\ttest: 0.2234721\tbest: 0.2234713 (236)\ttotal: 40.3s\tremaining: 1m 55s\n",
      "259:\tlearn: 0.2197784\ttest: 0.2234721\tbest: 0.2234713 (236)\ttotal: 40.3s\tremaining: 1m 54s\n",
      "260:\tlearn: 0.2197529\ttest: 0.2234715\tbest: 0.2234713 (236)\ttotal: 40.6s\tremaining: 1m 54s\n",
      "261:\tlearn: 0.2197520\ttest: 0.2234715\tbest: 0.2234713 (236)\ttotal: 40.6s\tremaining: 1m 54s\n",
      "262:\tlearn: 0.2197234\ttest: 0.2234713\tbest: 0.2234713 (236)\ttotal: 40.9s\tremaining: 1m 54s\n",
      "263:\tlearn: 0.2197229\ttest: 0.2234713\tbest: 0.2234713 (263)\ttotal: 41s\tremaining: 1m 54s\n",
      "264:\tlearn: 0.2196909\ttest: 0.2234709\tbest: 0.2234709 (264)\ttotal: 41.3s\tremaining: 1m 54s\n",
      "265:\tlearn: 0.2196909\ttest: 0.2234709\tbest: 0.2234709 (265)\ttotal: 41.3s\tremaining: 1m 53s\n",
      "266:\tlearn: 0.2196636\ttest: 0.2234708\tbest: 0.2234708 (266)\ttotal: 41.6s\tremaining: 1m 54s\n",
      "267:\tlearn: 0.2196633\ttest: 0.2234708\tbest: 0.2234708 (267)\ttotal: 41.6s\tremaining: 1m 53s\n",
      "268:\tlearn: 0.2196401\ttest: 0.2234709\tbest: 0.2234708 (267)\ttotal: 41.9s\tremaining: 1m 53s\n",
      "269:\tlearn: 0.2196369\ttest: 0.2234708\tbest: 0.2234708 (267)\ttotal: 42s\tremaining: 1m 53s\n",
      "270:\tlearn: 0.2196353\ttest: 0.2234708\tbest: 0.2234708 (270)\ttotal: 42s\tremaining: 1m 53s\n",
      "271:\tlearn: 0.2196352\ttest: 0.2234708\tbest: 0.2234708 (271)\ttotal: 42s\tremaining: 1m 52s\n",
      "272:\tlearn: 0.2196088\ttest: 0.2234712\tbest: 0.2234708 (271)\ttotal: 42.4s\tremaining: 1m 52s\n",
      "273:\tlearn: 0.2196087\ttest: 0.2234711\tbest: 0.2234708 (271)\ttotal: 42.4s\tremaining: 1m 52s\n",
      "274:\tlearn: 0.2195831\ttest: 0.2234709\tbest: 0.2234708 (271)\ttotal: 42.7s\tremaining: 1m 52s\n",
      "275:\tlearn: 0.2195830\ttest: 0.2234708\tbest: 0.2234708 (271)\ttotal: 42.7s\tremaining: 1m 51s\n",
      "276:\tlearn: 0.2195554\ttest: 0.2234713\tbest: 0.2234708 (271)\ttotal: 43s\tremaining: 1m 52s\n",
      "277:\tlearn: 0.2195553\ttest: 0.2234713\tbest: 0.2234708 (271)\ttotal: 43s\tremaining: 1m 51s\n",
      "278:\tlearn: 0.2195232\ttest: 0.2234716\tbest: 0.2234708 (271)\ttotal: 43.3s\tremaining: 1m 52s\n",
      "279:\tlearn: 0.2195231\ttest: 0.2234715\tbest: 0.2234708 (271)\ttotal: 43.3s\tremaining: 1m 51s\n",
      "280:\tlearn: 0.2194956\ttest: 0.2234706\tbest: 0.2234706 (280)\ttotal: 43.7s\tremaining: 1m 51s\n",
      "281:\tlearn: 0.2194954\ttest: 0.2234706\tbest: 0.2234706 (281)\ttotal: 43.7s\tremaining: 1m 51s\n",
      "282:\tlearn: 0.2194724\ttest: 0.2234706\tbest: 0.2234706 (282)\ttotal: 44s\tremaining: 1m 51s\n",
      "283:\tlearn: 0.2194724\ttest: 0.2234706\tbest: 0.2234706 (282)\ttotal: 44s\tremaining: 1m 50s\n",
      "284:\tlearn: 0.2194460\ttest: 0.2234702\tbest: 0.2234702 (284)\ttotal: 44.3s\tremaining: 1m 51s\n",
      "285:\tlearn: 0.2194457\ttest: 0.2234702\tbest: 0.2234702 (284)\ttotal: 44.3s\tremaining: 1m 50s\n",
      "286:\tlearn: 0.2194145\ttest: 0.2234701\tbest: 0.2234701 (286)\ttotal: 44.6s\tremaining: 1m 50s\n",
      "287:\tlearn: 0.2194124\ttest: 0.2234700\tbest: 0.2234700 (287)\ttotal: 44.7s\tremaining: 1m 50s\n",
      "288:\tlearn: 0.2194123\ttest: 0.2234699\tbest: 0.2234699 (288)\ttotal: 44.7s\tremaining: 1m 49s\n",
      "289:\tlearn: 0.2193853\ttest: 0.2234694\tbest: 0.2234694 (289)\ttotal: 45s\tremaining: 1m 50s\n",
      "290:\tlearn: 0.2193849\ttest: 0.2234693\tbest: 0.2234693 (290)\ttotal: 45s\tremaining: 1m 49s\n",
      "291:\tlearn: 0.2193514\ttest: 0.2234689\tbest: 0.2234689 (291)\ttotal: 45.3s\tremaining: 1m 49s\n",
      "292:\tlearn: 0.2193511\ttest: 0.2234689\tbest: 0.2234689 (292)\ttotal: 45.4s\tremaining: 1m 49s\n",
      "293:\tlearn: 0.2193213\ttest: 0.2234692\tbest: 0.2234689 (292)\ttotal: 45.7s\tremaining: 1m 49s\n",
      "294:\tlearn: 0.2193213\ttest: 0.2234691\tbest: 0.2234689 (292)\ttotal: 45.7s\tremaining: 1m 49s\n",
      "295:\tlearn: 0.2192933\ttest: 0.2234694\tbest: 0.2234689 (292)\ttotal: 46s\tremaining: 1m 49s\n",
      "296:\tlearn: 0.2192909\ttest: 0.2234695\tbest: 0.2234689 (292)\ttotal: 46.1s\tremaining: 1m 49s\n",
      "297:\tlearn: 0.2192909\ttest: 0.2234695\tbest: 0.2234689 (292)\ttotal: 46.1s\tremaining: 1m 48s\n",
      "298:\tlearn: 0.2192652\ttest: 0.2234693\tbest: 0.2234689 (292)\ttotal: 46.4s\tremaining: 1m 48s\n",
      "299:\tlearn: 0.2192646\ttest: 0.2234693\tbest: 0.2234689 (292)\ttotal: 46.4s\tremaining: 1m 48s\n",
      "300:\tlearn: 0.2192391\ttest: 0.2234690\tbest: 0.2234689 (292)\ttotal: 46.7s\tremaining: 1m 48s\n",
      "301:\tlearn: 0.2192390\ttest: 0.2234689\tbest: 0.2234689 (292)\ttotal: 46.7s\tremaining: 1m 47s\n",
      "302:\tlearn: 0.2192167\ttest: 0.2234687\tbest: 0.2234687 (302)\ttotal: 47s\tremaining: 1m 48s\n",
      "303:\tlearn: 0.2192167\ttest: 0.2234687\tbest: 0.2234687 (303)\ttotal: 47s\tremaining: 1m 47s\n",
      "304:\tlearn: 0.2191910\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 47.3s\tremaining: 1m 47s\n",
      "305:\tlearn: 0.2191867\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 47.5s\tremaining: 1m 47s\n",
      "306:\tlearn: 0.2191866\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 47.5s\tremaining: 1m 47s\n",
      "307:\tlearn: 0.2191579\ttest: 0.2234689\tbest: 0.2234685 (304)\ttotal: 47.8s\tremaining: 1m 47s\n",
      "308:\tlearn: 0.2191578\ttest: 0.2234689\tbest: 0.2234685 (304)\ttotal: 47.8s\tremaining: 1m 46s\n",
      "309:\tlearn: 0.2191333\ttest: 0.2234693\tbest: 0.2234685 (304)\ttotal: 48.1s\tremaining: 1m 47s\n",
      "310:\tlearn: 0.2191327\ttest: 0.2234693\tbest: 0.2234685 (304)\ttotal: 48.1s\tremaining: 1m 46s\n",
      "311:\tlearn: 0.2191045\ttest: 0.2234695\tbest: 0.2234685 (304)\ttotal: 48.4s\tremaining: 1m 46s\n",
      "312:\tlearn: 0.2191045\ttest: 0.2234695\tbest: 0.2234685 (304)\ttotal: 48.4s\tremaining: 1m 46s\n",
      "313:\tlearn: 0.2190811\ttest: 0.2234690\tbest: 0.2234685 (304)\ttotal: 48.8s\tremaining: 1m 46s\n",
      "314:\tlearn: 0.2190811\ttest: 0.2234689\tbest: 0.2234685 (304)\ttotal: 48.8s\tremaining: 1m 46s\n",
      "315:\tlearn: 0.2190535\ttest: 0.2234692\tbest: 0.2234685 (304)\ttotal: 49.1s\tremaining: 1m 46s\n",
      "316:\tlearn: 0.2190522\ttest: 0.2234691\tbest: 0.2234685 (304)\ttotal: 49.1s\tremaining: 1m 45s\n",
      "317:\tlearn: 0.2190215\ttest: 0.2234691\tbest: 0.2234685 (304)\ttotal: 49.4s\tremaining: 1m 46s\n",
      "318:\tlearn: 0.2190212\ttest: 0.2234691\tbest: 0.2234685 (304)\ttotal: 49.4s\tremaining: 1m 45s\n",
      "319:\tlearn: 0.2189928\ttest: 0.2234694\tbest: 0.2234685 (304)\ttotal: 49.8s\tremaining: 1m 45s\n",
      "320:\tlearn: 0.2189926\ttest: 0.2234693\tbest: 0.2234685 (304)\ttotal: 49.8s\tremaining: 1m 45s\n",
      "321:\tlearn: 0.2189680\ttest: 0.2234688\tbest: 0.2234685 (304)\ttotal: 50.1s\tremaining: 1m 45s\n",
      "322:\tlearn: 0.2189629\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 50.3s\tremaining: 1m 45s\n",
      "323:\tlearn: 0.2189628\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 50.3s\tremaining: 1m 44s\n",
      "324:\tlearn: 0.2189348\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 50.6s\tremaining: 1m 45s\n",
      "325:\tlearn: 0.2189346\ttest: 0.2234685\tbest: 0.2234685 (304)\ttotal: 50.6s\tremaining: 1m 44s\n",
      "326:\tlearn: 0.2189040\ttest: 0.2234686\tbest: 0.2234685 (304)\ttotal: 50.9s\tremaining: 1m 44s\n",
      "327:\tlearn: 0.2189040\ttest: 0.2234686\tbest: 0.2234685 (304)\ttotal: 51s\tremaining: 1m 44s\n",
      "328:\tlearn: 0.2188790\ttest: 0.2234682\tbest: 0.2234682 (328)\ttotal: 51.3s\tremaining: 1m 44s\n",
      "329:\tlearn: 0.2188790\ttest: 0.2234682\tbest: 0.2234682 (328)\ttotal: 51.3s\tremaining: 1m 44s\n",
      "330:\tlearn: 0.2188519\ttest: 0.2234679\tbest: 0.2234679 (330)\ttotal: 51.6s\tremaining: 1m 44s\n",
      "331:\tlearn: 0.2188506\ttest: 0.2234678\tbest: 0.2234678 (331)\ttotal: 51.6s\tremaining: 1m 43s\n",
      "332:\tlearn: 0.2188506\ttest: 0.2234678\tbest: 0.2234678 (332)\ttotal: 51.6s\tremaining: 1m 43s\n",
      "333:\tlearn: 0.2188200\ttest: 0.2234684\tbest: 0.2234678 (332)\ttotal: 51.9s\tremaining: 1m 43s\n",
      "334:\tlearn: 0.2188200\ttest: 0.2234685\tbest: 0.2234678 (332)\ttotal: 51.9s\tremaining: 1m 43s\n",
      "335:\tlearn: 0.2187943\ttest: 0.2234686\tbest: 0.2234678 (332)\ttotal: 52.3s\tremaining: 1m 43s\n",
      "336:\tlearn: 0.2187942\ttest: 0.2234687\tbest: 0.2234678 (332)\ttotal: 52.3s\tremaining: 1m 42s\n",
      "337:\tlearn: 0.2187643\ttest: 0.2234685\tbest: 0.2234678 (332)\ttotal: 52.6s\tremaining: 1m 42s\n",
      "338:\tlearn: 0.2187643\ttest: 0.2234686\tbest: 0.2234678 (332)\ttotal: 52.6s\tremaining: 1m 42s\n",
      "339:\tlearn: 0.2187303\ttest: 0.2234690\tbest: 0.2234678 (332)\ttotal: 52.9s\tremaining: 1m 42s\n",
      "340:\tlearn: 0.2187257\ttest: 0.2234687\tbest: 0.2234678 (332)\ttotal: 53s\tremaining: 1m 42s\n",
      "341:\tlearn: 0.2187257\ttest: 0.2234686\tbest: 0.2234678 (332)\ttotal: 53s\tremaining: 1m 41s\n",
      "342:\tlearn: 0.2186971\ttest: 0.2234682\tbest: 0.2234678 (332)\ttotal: 53.3s\tremaining: 1m 42s\n",
      "343:\tlearn: 0.2186971\ttest: 0.2234682\tbest: 0.2234678 (332)\ttotal: 53.3s\tremaining: 1m 41s\n",
      "344:\tlearn: 0.2186706\ttest: 0.2234685\tbest: 0.2234678 (332)\ttotal: 53.6s\tremaining: 1m 41s\n",
      "345:\tlearn: 0.2186706\ttest: 0.2234685\tbest: 0.2234678 (332)\ttotal: 53.6s\tremaining: 1m 41s\n",
      "346:\tlearn: 0.2186402\ttest: 0.2234684\tbest: 0.2234678 (332)\ttotal: 54s\tremaining: 1m 41s\n",
      "347:\tlearn: 0.2186401\ttest: 0.2234683\tbest: 0.2234678 (332)\ttotal: 54s\tremaining: 1m 41s\n",
      "348:\tlearn: 0.2186107\ttest: 0.2234681\tbest: 0.2234678 (332)\ttotal: 54.3s\tremaining: 1m 41s\n",
      "349:\tlearn: 0.2186106\ttest: 0.2234681\tbest: 0.2234678 (332)\ttotal: 54.3s\tremaining: 1m 40s\n",
      "350:\tlearn: 0.2185789\ttest: 0.2234686\tbest: 0.2234678 (332)\ttotal: 54.6s\tremaining: 1m 40s\n",
      "351:\tlearn: 0.2185789\ttest: 0.2234685\tbest: 0.2234678 (332)\ttotal: 54.6s\tremaining: 1m 40s\n",
      "352:\tlearn: 0.2185505\ttest: 0.2234683\tbest: 0.2234678 (332)\ttotal: 54.9s\tremaining: 1m 40s\n",
      "353:\tlearn: 0.2185504\ttest: 0.2234683\tbest: 0.2234678 (332)\ttotal: 55s\tremaining: 1m 40s\n",
      "354:\tlearn: 0.2185199\ttest: 0.2234681\tbest: 0.2234678 (332)\ttotal: 55.3s\tremaining: 1m 40s\n",
      "355:\tlearn: 0.2185199\ttest: 0.2234681\tbest: 0.2234678 (332)\ttotal: 55.3s\tremaining: 1m 40s\n",
      "356:\tlearn: 0.2184908\ttest: 0.2234681\tbest: 0.2234678 (332)\ttotal: 55.6s\tremaining: 1m 40s\n",
      "357:\tlearn: 0.2184906\ttest: 0.2234679\tbest: 0.2234678 (332)\ttotal: 55.6s\tremaining: 1m 39s\n",
      "358:\tlearn: 0.2184630\ttest: 0.2234677\tbest: 0.2234677 (358)\ttotal: 55.9s\tremaining: 1m 39s\n",
      "359:\tlearn: 0.2184620\ttest: 0.2234676\tbest: 0.2234676 (359)\ttotal: 56s\tremaining: 1m 39s\n",
      "360:\tlearn: 0.2184311\ttest: 0.2234674\tbest: 0.2234674 (360)\ttotal: 56.3s\tremaining: 1m 39s\n",
      "361:\tlearn: 0.2184309\ttest: 0.2234674\tbest: 0.2234674 (361)\ttotal: 56.3s\tremaining: 1m 39s\n",
      "362:\tlearn: 0.2184020\ttest: 0.2234678\tbest: 0.2234674 (361)\ttotal: 56.6s\tremaining: 1m 39s\n",
      "363:\tlearn: 0.2183946\ttest: 0.2234679\tbest: 0.2234674 (361)\ttotal: 56.8s\tremaining: 1m 39s\n",
      "364:\tlearn: 0.2183920\ttest: 0.2234680\tbest: 0.2234674 (361)\ttotal: 56.9s\tremaining: 1m 39s\n",
      "365:\tlearn: 0.2183919\ttest: 0.2234680\tbest: 0.2234674 (361)\ttotal: 56.9s\tremaining: 1m 38s\n",
      "366:\tlearn: 0.2183686\ttest: 0.2234681\tbest: 0.2234674 (361)\ttotal: 57.2s\tremaining: 1m 38s\n",
      "367:\tlearn: 0.2183681\ttest: 0.2234682\tbest: 0.2234674 (361)\ttotal: 57.2s\tremaining: 1m 38s\n",
      "368:\tlearn: 0.2183344\ttest: 0.2234686\tbest: 0.2234674 (361)\ttotal: 57.6s\tremaining: 1m 38s\n",
      "369:\tlearn: 0.2183344\ttest: 0.2234686\tbest: 0.2234674 (361)\ttotal: 57.6s\tremaining: 1m 38s\n",
      "370:\tlearn: 0.2183104\ttest: 0.2234685\tbest: 0.2234674 (361)\ttotal: 57.9s\tremaining: 1m 38s\n",
      "371:\tlearn: 0.2183097\ttest: 0.2234684\tbest: 0.2234674 (361)\ttotal: 57.9s\tremaining: 1m 37s\n",
      "372:\tlearn: 0.2182813\ttest: 0.2234682\tbest: 0.2234674 (361)\ttotal: 58.2s\tremaining: 1m 37s\n",
      "373:\tlearn: 0.2182810\ttest: 0.2234681\tbest: 0.2234674 (361)\ttotal: 58.2s\tremaining: 1m 37s\n",
      "374:\tlearn: 0.2182552\ttest: 0.2234681\tbest: 0.2234674 (361)\ttotal: 58.5s\tremaining: 1m 37s\n",
      "375:\tlearn: 0.2182551\ttest: 0.2234680\tbest: 0.2234674 (361)\ttotal: 58.6s\tremaining: 1m 37s\n",
      "376:\tlearn: 0.2182266\ttest: 0.2234675\tbest: 0.2234674 (361)\ttotal: 58.9s\tremaining: 1m 37s\n",
      "377:\tlearn: 0.2182260\ttest: 0.2234675\tbest: 0.2234674 (361)\ttotal: 58.9s\tremaining: 1m 36s\n",
      "378:\tlearn: 0.2181992\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 59.2s\tremaining: 1m 37s\n",
      "379:\tlearn: 0.2181991\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 59.2s\tremaining: 1m 36s\n",
      "380:\tlearn: 0.2181727\ttest: 0.2234678\tbest: 0.2234674 (361)\ttotal: 59.5s\tremaining: 1m 36s\n",
      "381:\tlearn: 0.2181727\ttest: 0.2234678\tbest: 0.2234674 (361)\ttotal: 59.5s\tremaining: 1m 36s\n",
      "382:\tlearn: 0.2181428\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 59.8s\tremaining: 1m 36s\n",
      "383:\tlearn: 0.2181428\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 59.8s\tremaining: 1m 36s\n",
      "384:\tlearn: 0.2181157\ttest: 0.2234680\tbest: 0.2234674 (361)\ttotal: 1m\tremaining: 1m 36s\n",
      "385:\tlearn: 0.2181156\ttest: 0.2234680\tbest: 0.2234674 (361)\ttotal: 1m\tremaining: 1m 35s\n",
      "386:\tlearn: 0.2180885\ttest: 0.2234677\tbest: 0.2234674 (361)\ttotal: 1m\tremaining: 1m 35s\n",
      "387:\tlearn: 0.2180885\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 1m\tremaining: 1m 35s\n",
      "388:\tlearn: 0.2180620\ttest: 0.2234675\tbest: 0.2234674 (361)\ttotal: 1m\tremaining: 1m 35s\n",
      "389:\tlearn: 0.2180614\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 1m\tremaining: 1m 35s\n",
      "390:\tlearn: 0.2180352\ttest: 0.2234676\tbest: 0.2234674 (361)\ttotal: 1m 1s\tremaining: 1m 35s\n",
      "391:\tlearn: 0.2180351\ttest: 0.2234675\tbest: 0.2234674 (361)\ttotal: 1m 1s\tremaining: 1m 34s\n",
      "392:\tlearn: 0.2180045\ttest: 0.2234674\tbest: 0.2234674 (361)\ttotal: 1m 1s\tremaining: 1m 34s\n",
      "393:\tlearn: 0.2179943\ttest: 0.2234672\tbest: 0.2234672 (393)\ttotal: 1m 1s\tremaining: 1m 34s\n",
      "394:\tlearn: 0.2179943\ttest: 0.2234672\tbest: 0.2234672 (393)\ttotal: 1m 1s\tremaining: 1m 34s\n",
      "395:\tlearn: 0.2179680\ttest: 0.2234676\tbest: 0.2234672 (393)\ttotal: 1m 2s\tremaining: 1m 34s\n",
      "396:\tlearn: 0.2179680\ttest: 0.2234676\tbest: 0.2234672 (393)\ttotal: 1m 2s\tremaining: 1m 34s\n",
      "397:\tlearn: 0.2179418\ttest: 0.2234672\tbest: 0.2234672 (393)\ttotal: 1m 2s\tremaining: 1m 34s\n",
      "398:\tlearn: 0.2179417\ttest: 0.2234673\tbest: 0.2234672 (393)\ttotal: 1m 2s\tremaining: 1m 33s\n",
      "399:\tlearn: 0.2179164\ttest: 0.2234672\tbest: 0.2234672 (393)\ttotal: 1m 2s\tremaining: 1m 34s\n",
      "400:\tlearn: 0.2179162\ttest: 0.2234672\tbest: 0.2234672 (393)\ttotal: 1m 2s\tremaining: 1m 33s\n",
      "401:\tlearn: 0.2178862\ttest: 0.2234666\tbest: 0.2234666 (401)\ttotal: 1m 3s\tremaining: 1m 33s\n",
      "402:\tlearn: 0.2178860\ttest: 0.2234665\tbest: 0.2234665 (402)\ttotal: 1m 3s\tremaining: 1m 33s\n",
      "403:\tlearn: 0.2178610\ttest: 0.2234663\tbest: 0.2234663 (403)\ttotal: 1m 3s\tremaining: 1m 33s\n",
      "404:\tlearn: 0.2178609\ttest: 0.2234662\tbest: 0.2234662 (404)\ttotal: 1m 3s\tremaining: 1m 33s\n",
      "405:\tlearn: 0.2178328\ttest: 0.2234664\tbest: 0.2234662 (404)\ttotal: 1m 3s\tremaining: 1m 33s\n",
      "406:\tlearn: 0.2178327\ttest: 0.2234664\tbest: 0.2234662 (404)\ttotal: 1m 3s\tremaining: 1m 32s\n",
      "407:\tlearn: 0.2178053\ttest: 0.2234662\tbest: 0.2234662 (404)\ttotal: 1m 4s\tremaining: 1m 32s\n",
      "408:\tlearn: 0.2178052\ttest: 0.2234662\tbest: 0.2234662 (408)\ttotal: 1m 4s\tremaining: 1m 32s\n",
      "409:\tlearn: 0.2177784\ttest: 0.2234661\tbest: 0.2234661 (409)\ttotal: 1m 4s\tremaining: 1m 32s\n",
      "410:\tlearn: 0.2177782\ttest: 0.2234660\tbest: 0.2234660 (410)\ttotal: 1m 4s\tremaining: 1m 32s\n",
      "411:\tlearn: 0.2177519\ttest: 0.2234659\tbest: 0.2234659 (411)\ttotal: 1m 4s\tremaining: 1m 32s\n",
      "412:\tlearn: 0.2177518\ttest: 0.2234659\tbest: 0.2234659 (411)\ttotal: 1m 4s\tremaining: 1m 31s\n",
      "413:\tlearn: 0.2177274\ttest: 0.2234662\tbest: 0.2234659 (411)\ttotal: 1m 4s\tremaining: 1m 31s\n",
      "414:\tlearn: 0.2177273\ttest: 0.2234662\tbest: 0.2234659 (411)\ttotal: 1m 4s\tremaining: 1m 31s\n",
      "415:\tlearn: 0.2177015\ttest: 0.2234659\tbest: 0.2234659 (411)\ttotal: 1m 5s\tremaining: 1m 31s\n",
      "416:\tlearn: 0.2177015\ttest: 0.2234658\tbest: 0.2234658 (416)\ttotal: 1m 5s\tremaining: 1m 31s\n",
      "417:\tlearn: 0.2176725\ttest: 0.2234660\tbest: 0.2234658 (416)\ttotal: 1m 5s\tremaining: 1m 31s\n",
      "418:\tlearn: 0.2176716\ttest: 0.2234660\tbest: 0.2234658 (416)\ttotal: 1m 5s\tremaining: 1m 31s\n",
      "419:\tlearn: 0.2176439\ttest: 0.2234662\tbest: 0.2234658 (416)\ttotal: 1m 5s\tremaining: 1m 31s\n",
      "420:\tlearn: 0.2176438\ttest: 0.2234662\tbest: 0.2234658 (416)\ttotal: 1m 5s\tremaining: 1m 30s\n",
      "421:\tlearn: 0.2176166\ttest: 0.2234660\tbest: 0.2234658 (416)\ttotal: 1m 6s\tremaining: 1m 30s\n",
      "422:\tlearn: 0.2176165\ttest: 0.2234660\tbest: 0.2234658 (416)\ttotal: 1m 6s\tremaining: 1m 30s\n",
      "423:\tlearn: 0.2175902\ttest: 0.2234655\tbest: 0.2234655 (423)\ttotal: 1m 6s\tremaining: 1m 30s\n",
      "424:\tlearn: 0.2175895\ttest: 0.2234654\tbest: 0.2234654 (424)\ttotal: 1m 6s\tremaining: 1m 30s\n",
      "425:\tlearn: 0.2175622\ttest: 0.2234657\tbest: 0.2234654 (424)\ttotal: 1m 6s\tremaining: 1m 30s\n",
      "426:\tlearn: 0.2175621\ttest: 0.2234657\tbest: 0.2234654 (424)\ttotal: 1m 6s\tremaining: 1m 29s\n",
      "427:\tlearn: 0.2175333\ttest: 0.2234655\tbest: 0.2234654 (424)\ttotal: 1m 7s\tremaining: 1m 29s\n",
      "428:\tlearn: 0.2175332\ttest: 0.2234654\tbest: 0.2234654 (428)\ttotal: 1m 7s\tremaining: 1m 29s\n",
      "429:\tlearn: 0.2175077\ttest: 0.2234652\tbest: 0.2234652 (429)\ttotal: 1m 7s\tremaining: 1m 29s\n",
      "430:\tlearn: 0.2175077\ttest: 0.2234652\tbest: 0.2234652 (430)\ttotal: 1m 7s\tremaining: 1m 29s\n",
      "431:\tlearn: 0.2174759\ttest: 0.2234654\tbest: 0.2234652 (430)\ttotal: 1m 7s\tremaining: 1m 29s\n",
      "432:\tlearn: 0.2174760\ttest: 0.2234654\tbest: 0.2234652 (430)\ttotal: 1m 7s\tremaining: 1m 28s\n",
      "433:\tlearn: 0.2174474\ttest: 0.2234655\tbest: 0.2234652 (430)\ttotal: 1m 8s\tremaining: 1m 28s\n",
      "434:\tlearn: 0.2174469\ttest: 0.2234655\tbest: 0.2234652 (430)\ttotal: 1m 8s\tremaining: 1m 28s\n",
      "435:\tlearn: 0.2174206\ttest: 0.2234653\tbest: 0.2234652 (430)\ttotal: 1m 8s\tremaining: 1m 28s\n",
      "436:\tlearn: 0.2174202\ttest: 0.2234652\tbest: 0.2234652 (436)\ttotal: 1m 8s\tremaining: 1m 28s\n",
      "437:\tlearn: 0.2173871\ttest: 0.2234654\tbest: 0.2234652 (436)\ttotal: 1m 8s\tremaining: 1m 28s\n",
      "438:\tlearn: 0.2173856\ttest: 0.2234657\tbest: 0.2234652 (436)\ttotal: 1m 8s\tremaining: 1m 28s\n",
      "439:\tlearn: 0.2173856\ttest: 0.2234656\tbest: 0.2234652 (436)\ttotal: 1m 8s\tremaining: 1m 27s\n",
      "440:\tlearn: 0.2173601\ttest: 0.2234650\tbest: 0.2234650 (440)\ttotal: 1m 9s\tremaining: 1m 27s\n",
      "441:\tlearn: 0.2173601\ttest: 0.2234650\tbest: 0.2234650 (440)\ttotal: 1m 9s\tremaining: 1m 27s\n",
      "442:\tlearn: 0.2173337\ttest: 0.2234650\tbest: 0.2234650 (442)\ttotal: 1m 9s\tremaining: 1m 27s\n",
      "443:\tlearn: 0.2173335\ttest: 0.2234649\tbest: 0.2234649 (443)\ttotal: 1m 9s\tremaining: 1m 27s\n",
      "444:\tlearn: 0.2173040\ttest: 0.2234647\tbest: 0.2234647 (444)\ttotal: 1m 9s\tremaining: 1m 27s\n",
      "445:\tlearn: 0.2173038\ttest: 0.2234647\tbest: 0.2234647 (444)\ttotal: 1m 9s\tremaining: 1m 26s\n",
      "446:\tlearn: 0.2172766\ttest: 0.2234650\tbest: 0.2234647 (444)\ttotal: 1m 10s\tremaining: 1m 26s\n",
      "447:\tlearn: 0.2172756\ttest: 0.2234649\tbest: 0.2234647 (444)\ttotal: 1m 10s\tremaining: 1m 26s\n",
      "448:\tlearn: 0.2172513\ttest: 0.2234646\tbest: 0.2234646 (448)\ttotal: 1m 10s\tremaining: 1m 26s\n",
      "449:\tlearn: 0.2172511\ttest: 0.2234646\tbest: 0.2234646 (449)\ttotal: 1m 10s\tremaining: 1m 26s\n",
      "450:\tlearn: 0.2172263\ttest: 0.2234647\tbest: 0.2234646 (449)\ttotal: 1m 10s\tremaining: 1m 26s\n",
      "451:\tlearn: 0.2172262\ttest: 0.2234646\tbest: 0.2234646 (449)\ttotal: 1m 10s\tremaining: 1m 25s\n",
      "452:\tlearn: 0.2171958\ttest: 0.2234640\tbest: 0.2234640 (452)\ttotal: 1m 11s\tremaining: 1m 25s\n",
      "453:\tlearn: 0.2171957\ttest: 0.2234639\tbest: 0.2234639 (453)\ttotal: 1m 11s\tremaining: 1m 25s\n",
      "454:\tlearn: 0.2171685\ttest: 0.2234642\tbest: 0.2234639 (453)\ttotal: 1m 11s\tremaining: 1m 25s\n",
      "455:\tlearn: 0.2171685\ttest: 0.2234642\tbest: 0.2234639 (453)\ttotal: 1m 11s\tremaining: 1m 25s\n",
      "456:\tlearn: 0.2171416\ttest: 0.2234642\tbest: 0.2234639 (453)\ttotal: 1m 11s\tremaining: 1m 25s\n",
      "457:\tlearn: 0.2171415\ttest: 0.2234642\tbest: 0.2234639 (453)\ttotal: 1m 11s\tremaining: 1m 25s\n",
      "458:\tlearn: 0.2171149\ttest: 0.2234645\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 25s\n",
      "459:\tlearn: 0.2171132\ttest: 0.2234646\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 24s\n",
      "460:\tlearn: 0.2171132\ttest: 0.2234646\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 24s\n",
      "461:\tlearn: 0.2170824\ttest: 0.2234649\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 24s\n",
      "462:\tlearn: 0.2170820\ttest: 0.2234648\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 24s\n",
      "463:\tlearn: 0.2170561\ttest: 0.2234648\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 24s\n",
      "464:\tlearn: 0.2170546\ttest: 0.2234649\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 23s\n",
      "465:\tlearn: 0.2170546\ttest: 0.2234649\tbest: 0.2234639 (453)\ttotal: 1m 12s\tremaining: 1m 23s\n",
      "466:\tlearn: 0.2170253\ttest: 0.2234649\tbest: 0.2234639 (453)\ttotal: 1m 13s\tremaining: 1m 23s\n",
      "467:\tlearn: 0.2170251\ttest: 0.2234649\tbest: 0.2234639 (453)\ttotal: 1m 13s\tremaining: 1m 23s\n",
      "468:\tlearn: 0.2169960\ttest: 0.2234648\tbest: 0.2234639 (453)\ttotal: 1m 13s\tremaining: 1m 23s\n",
      "469:\tlearn: 0.2169958\ttest: 0.2234648\tbest: 0.2234639 (453)\ttotal: 1m 13s\tremaining: 1m 23s\n",
      "470:\tlearn: 0.2169675\ttest: 0.2234650\tbest: 0.2234639 (453)\ttotal: 1m 13s\tremaining: 1m 23s\n",
      "471:\tlearn: 0.2169673\ttest: 0.2234650\tbest: 0.2234639 (453)\ttotal: 1m 13s\tremaining: 1m 22s\n",
      "472:\tlearn: 0.2169387\ttest: 0.2234654\tbest: 0.2234639 (453)\ttotal: 1m 14s\tremaining: 1m 22s\n",
      "473:\tlearn: 0.2169386\ttest: 0.2234653\tbest: 0.2234639 (453)\ttotal: 1m 14s\tremaining: 1m 22s\n",
      "474:\tlearn: 0.2169125\ttest: 0.2234652\tbest: 0.2234639 (453)\ttotal: 1m 14s\tremaining: 1m 22s\n",
      "475:\tlearn: 0.2169124\ttest: 0.2234652\tbest: 0.2234639 (453)\ttotal: 1m 14s\tremaining: 1m 22s\n",
      "476:\tlearn: 0.2168822\ttest: 0.2234651\tbest: 0.2234639 (453)\ttotal: 1m 14s\tremaining: 1m 22s\n",
      "477:\tlearn: 0.2168821\ttest: 0.2234651\tbest: 0.2234639 (453)\ttotal: 1m 14s\tremaining: 1m 21s\n",
      "478:\tlearn: 0.2168544\ttest: 0.2234645\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 21s\n",
      "479:\tlearn: 0.2168543\ttest: 0.2234645\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 21s\n",
      "480:\tlearn: 0.2168261\ttest: 0.2234647\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 21s\n",
      "481:\tlearn: 0.2168259\ttest: 0.2234647\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 21s\n",
      "482:\tlearn: 0.2167965\ttest: 0.2234652\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 21s\n",
      "483:\tlearn: 0.2167949\ttest: 0.2234651\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 21s\n",
      "484:\tlearn: 0.2167948\ttest: 0.2234651\tbest: 0.2234639 (453)\ttotal: 1m 15s\tremaining: 1m 20s\n",
      "485:\tlearn: 0.2167717\ttest: 0.2234657\tbest: 0.2234639 (453)\ttotal: 1m 16s\tremaining: 1m 20s\n",
      "486:\tlearn: 0.2167712\ttest: 0.2234656\tbest: 0.2234639 (453)\ttotal: 1m 16s\tremaining: 1m 20s\n",
      "487:\tlearn: 0.2167474\ttest: 0.2234657\tbest: 0.2234639 (453)\ttotal: 1m 16s\tremaining: 1m 20s\n",
      "488:\tlearn: 0.2167472\ttest: 0.2234658\tbest: 0.2234639 (453)\ttotal: 1m 16s\tremaining: 1m 20s\n",
      "489:\tlearn: 0.2167220\ttest: 0.2234658\tbest: 0.2234639 (453)\ttotal: 1m 16s\tremaining: 1m 20s\n",
      "490:\tlearn: 0.2167220\ttest: 0.2234658\tbest: 0.2234639 (453)\ttotal: 1m 16s\tremaining: 1m 19s\n",
      "491:\tlearn: 0.2166975\ttest: 0.2234657\tbest: 0.2234639 (453)\ttotal: 1m 17s\tremaining: 1m 19s\n",
      "492:\tlearn: 0.2166964\ttest: 0.2234658\tbest: 0.2234639 (453)\ttotal: 1m 17s\tremaining: 1m 19s\n",
      "493:\tlearn: 0.2166665\ttest: 0.2234667\tbest: 0.2234639 (453)\ttotal: 1m 17s\tremaining: 1m 19s\n",
      "494:\tlearn: 0.2166658\ttest: 0.2234667\tbest: 0.2234639 (453)\ttotal: 1m 17s\tremaining: 1m 19s\n",
      "495:\tlearn: 0.2166401\ttest: 0.2234662\tbest: 0.2234639 (453)\ttotal: 1m 17s\tremaining: 1m 19s\n",
      "496:\tlearn: 0.2166401\ttest: 0.2234662\tbest: 0.2234639 (453)\ttotal: 1m 17s\tremaining: 1m 18s\n",
      "497:\tlearn: 0.2166122\ttest: 0.2234659\tbest: 0.2234639 (453)\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "498:\tlearn: 0.2166106\ttest: 0.2234658\tbest: 0.2234639 (453)\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "499:\tlearn: 0.2166105\ttest: 0.2234658\tbest: 0.2234639 (453)\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "500:\tlearn: 0.2165848\ttest: 0.2234652\tbest: 0.2234639 (453)\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "501:\tlearn: 0.2165848\ttest: 0.2234652\tbest: 0.2234639 (453)\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "502:\tlearn: 0.2165591\ttest: 0.2234654\tbest: 0.2234639 (453)\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "503:\tlearn: 0.2165549\ttest: 0.2234653\tbest: 0.2234639 (453)\ttotal: 1m 19s\tremaining: 1m 17s\n",
      "bestTest = 0.223463924\n",
      "bestIteration = 453\n",
      "Shrink model to first 454 iterations.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7fe044d379a0>"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "model = cat.CatBoostRegressor(loss_function='RMSE',eval_metric='RMSE',iterations=1000,learning_rate=0.0025, random_seed=0, subsample=0.8,depth=16, task_type='GPU')\n",
    "model.fit(X=data[t_idx],y=target[t_idx], eval_set=(data[v_idx],target[v_idx]),early_stopping_rounds=50, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(data[v_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 501808, number of used features: 310\n",
      "[LightGBM] [Info] Start training from score 0.499997\n",
      "[1]\tvalid_0's huber: 0.0249734\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalid_0's huber: 0.0249731\n",
      "[3]\tvalid_0's huber: 0.0249728\n",
      "[4]\tvalid_0's huber: 0.0249723\n",
      "[5]\tvalid_0's huber: 0.0249721\n",
      "[6]\tvalid_0's huber: 0.0249718\n",
      "[7]\tvalid_0's huber: 0.0249715\n",
      "[8]\tvalid_0's huber: 0.0249712\n",
      "[9]\tvalid_0's huber: 0.024971\n",
      "[10]\tvalid_0's huber: 0.0249707\n",
      "[11]\tvalid_0's huber: 0.0249705\n",
      "[12]\tvalid_0's huber: 0.02497\n",
      "[13]\tvalid_0's huber: 0.0249697\n",
      "[14]\tvalid_0's huber: 0.0249694\n",
      "[15]\tvalid_0's huber: 0.0249691\n",
      "[16]\tvalid_0's huber: 0.0249689\n",
      "[17]\tvalid_0's huber: 0.0249689\n",
      "[18]\tvalid_0's huber: 0.0249688\n",
      "[19]\tvalid_0's huber: 0.0249685\n",
      "[20]\tvalid_0's huber: 0.0249682\n",
      "[21]\tvalid_0's huber: 0.024968\n",
      "[22]\tvalid_0's huber: 0.0249679\n",
      "[23]\tvalid_0's huber: 0.0249675\n",
      "[24]\tvalid_0's huber: 0.0249673\n",
      "[25]\tvalid_0's huber: 0.024967\n",
      "[26]\tvalid_0's huber: 0.0249669\n",
      "[27]\tvalid_0's huber: 0.0249666\n",
      "[28]\tvalid_0's huber: 0.0249665\n",
      "[29]\tvalid_0's huber: 0.0249664\n",
      "[30]\tvalid_0's huber: 0.0249661\n",
      "[31]\tvalid_0's huber: 0.0249658\n",
      "[32]\tvalid_0's huber: 0.0249658\n",
      "[33]\tvalid_0's huber: 0.0249655\n",
      "[34]\tvalid_0's huber: 0.0249654\n",
      "[35]\tvalid_0's huber: 0.0249652\n",
      "[36]\tvalid_0's huber: 0.0249651\n",
      "[37]\tvalid_0's huber: 0.0249648\n",
      "[38]\tvalid_0's huber: 0.0249647\n",
      "[39]\tvalid_0's huber: 0.0249647\n",
      "[40]\tvalid_0's huber: 0.0249644\n",
      "[41]\tvalid_0's huber: 0.0249644\n",
      "[42]\tvalid_0's huber: 0.0249644\n",
      "[43]\tvalid_0's huber: 0.024964\n",
      "[44]\tvalid_0's huber: 0.0249637\n",
      "[45]\tvalid_0's huber: 0.0249635\n",
      "[46]\tvalid_0's huber: 0.0249633\n",
      "[47]\tvalid_0's huber: 0.0249631\n",
      "[48]\tvalid_0's huber: 0.024963\n",
      "[49]\tvalid_0's huber: 0.0249627\n",
      "[50]\tvalid_0's huber: 0.0249625\n",
      "[51]\tvalid_0's huber: 0.0249622\n",
      "[52]\tvalid_0's huber: 0.0249621\n",
      "[53]\tvalid_0's huber: 0.0249621\n",
      "[54]\tvalid_0's huber: 0.0249622\n",
      "[55]\tvalid_0's huber: 0.0249621\n",
      "[56]\tvalid_0's huber: 0.024962\n",
      "[57]\tvalid_0's huber: 0.0249619\n",
      "[58]\tvalid_0's huber: 0.0249617\n",
      "[59]\tvalid_0's huber: 0.0249617\n",
      "[60]\tvalid_0's huber: 0.0249617\n",
      "[61]\tvalid_0's huber: 0.0249614\n",
      "[62]\tvalid_0's huber: 0.0249614\n",
      "[63]\tvalid_0's huber: 0.0249611\n",
      "[64]\tvalid_0's huber: 0.024961\n",
      "[65]\tvalid_0's huber: 0.0249609\n",
      "[66]\tvalid_0's huber: 0.0249607\n",
      "[67]\tvalid_0's huber: 0.0249604\n",
      "[68]\tvalid_0's huber: 0.0249602\n",
      "[69]\tvalid_0's huber: 0.0249601\n",
      "[70]\tvalid_0's huber: 0.0249601\n",
      "[71]\tvalid_0's huber: 0.0249599\n",
      "[72]\tvalid_0's huber: 0.0249598\n",
      "[73]\tvalid_0's huber: 0.0249595\n",
      "[74]\tvalid_0's huber: 0.0249594\n",
      "[75]\tvalid_0's huber: 0.0249594\n",
      "[76]\tvalid_0's huber: 0.0249592\n",
      "[77]\tvalid_0's huber: 0.0249593\n",
      "[78]\tvalid_0's huber: 0.0249592\n",
      "[79]\tvalid_0's huber: 0.0249592\n",
      "[80]\tvalid_0's huber: 0.0249592\n",
      "[81]\tvalid_0's huber: 0.0249591\n",
      "[82]\tvalid_0's huber: 0.0249592\n",
      "[83]\tvalid_0's huber: 0.024959\n",
      "[84]\tvalid_0's huber: 0.0249589\n",
      "[85]\tvalid_0's huber: 0.024959\n",
      "[86]\tvalid_0's huber: 0.024959\n",
      "[87]\tvalid_0's huber: 0.0249591\n",
      "[88]\tvalid_0's huber: 0.024959\n",
      "[89]\tvalid_0's huber: 0.024959\n",
      "[90]\tvalid_0's huber: 0.0249589\n",
      "[91]\tvalid_0's huber: 0.0249589\n",
      "[92]\tvalid_0's huber: 0.0249587\n",
      "[93]\tvalid_0's huber: 0.0249586\n",
      "[94]\tvalid_0's huber: 0.0249585\n",
      "[95]\tvalid_0's huber: 0.0249585\n",
      "[96]\tvalid_0's huber: 0.0249583\n",
      "[97]\tvalid_0's huber: 0.0249584\n",
      "[98]\tvalid_0's huber: 0.0249584\n",
      "[99]\tvalid_0's huber: 0.0249582\n",
      "[100]\tvalid_0's huber: 0.0249583\n",
      "[101]\tvalid_0's huber: 0.0249584\n",
      "[102]\tvalid_0's huber: 0.0249583\n",
      "[103]\tvalid_0's huber: 0.0249581\n",
      "[104]\tvalid_0's huber: 0.0249581\n",
      "[105]\tvalid_0's huber: 0.024958\n",
      "[106]\tvalid_0's huber: 0.0249578\n",
      "[107]\tvalid_0's huber: 0.0249576\n",
      "[108]\tvalid_0's huber: 0.0249574\n",
      "[109]\tvalid_0's huber: 0.0249573\n",
      "[110]\tvalid_0's huber: 0.0249572\n",
      "[111]\tvalid_0's huber: 0.024957\n",
      "[112]\tvalid_0's huber: 0.024957\n",
      "[113]\tvalid_0's huber: 0.0249569\n",
      "[114]\tvalid_0's huber: 0.0249568\n",
      "[115]\tvalid_0's huber: 0.0249567\n",
      "[116]\tvalid_0's huber: 0.0249568\n",
      "[117]\tvalid_0's huber: 0.0249569\n",
      "[118]\tvalid_0's huber: 0.0249568\n",
      "[119]\tvalid_0's huber: 0.0249569\n",
      "[120]\tvalid_0's huber: 0.0249568\n",
      "[121]\tvalid_0's huber: 0.0249567\n",
      "[122]\tvalid_0's huber: 0.0249567\n",
      "[123]\tvalid_0's huber: 0.0249568\n",
      "[124]\tvalid_0's huber: 0.0249569\n",
      "[125]\tvalid_0's huber: 0.0249569\n",
      "[126]\tvalid_0's huber: 0.024957\n",
      "[127]\tvalid_0's huber: 0.0249569\n",
      "[128]\tvalid_0's huber: 0.0249569\n",
      "[129]\tvalid_0's huber: 0.0249569\n",
      "[130]\tvalid_0's huber: 0.0249568\n",
      "[131]\tvalid_0's huber: 0.0249567\n",
      "[132]\tvalid_0's huber: 0.0249568\n",
      "[133]\tvalid_0's huber: 0.0249568\n",
      "[134]\tvalid_0's huber: 0.0249569\n",
      "[135]\tvalid_0's huber: 0.0249569\n",
      "[136]\tvalid_0's huber: 0.0249567\n",
      "[137]\tvalid_0's huber: 0.0249567\n",
      "[138]\tvalid_0's huber: 0.0249567\n",
      "[139]\tvalid_0's huber: 0.0249568\n",
      "[140]\tvalid_0's huber: 0.0249568\n",
      "[141]\tvalid_0's huber: 0.0249569\n",
      "[142]\tvalid_0's huber: 0.0249569\n",
      "[143]\tvalid_0's huber: 0.0249569\n",
      "[144]\tvalid_0's huber: 0.0249568\n",
      "[145]\tvalid_0's huber: 0.0249568\n",
      "[146]\tvalid_0's huber: 0.0249568\n",
      "[147]\tvalid_0's huber: 0.0249569\n",
      "[148]\tvalid_0's huber: 0.024957\n",
      "[149]\tvalid_0's huber: 0.0249569\n",
      "[150]\tvalid_0's huber: 0.0249567\n",
      "[151]\tvalid_0's huber: 0.0249566\n",
      "[152]\tvalid_0's huber: 0.0249565\n",
      "[153]\tvalid_0's huber: 0.0249565\n",
      "[154]\tvalid_0's huber: 0.0249565\n",
      "[155]\tvalid_0's huber: 0.0249566\n",
      "[156]\tvalid_0's huber: 0.0249566\n",
      "[157]\tvalid_0's huber: 0.0249566\n",
      "[158]\tvalid_0's huber: 0.0249565\n",
      "[159]\tvalid_0's huber: 0.0249564\n",
      "[160]\tvalid_0's huber: 0.0249565\n",
      "[161]\tvalid_0's huber: 0.0249566\n",
      "[162]\tvalid_0's huber: 0.0249567\n",
      "[163]\tvalid_0's huber: 0.0249567\n",
      "[164]\tvalid_0's huber: 0.0249567\n",
      "[165]\tvalid_0's huber: 0.0249568\n",
      "[166]\tvalid_0's huber: 0.0249566\n",
      "[167]\tvalid_0's huber: 0.0249566\n",
      "[168]\tvalid_0's huber: 0.0249567\n",
      "[169]\tvalid_0's huber: 0.0249566\n",
      "[170]\tvalid_0's huber: 0.0249567\n",
      "[171]\tvalid_0's huber: 0.0249568\n",
      "[172]\tvalid_0's huber: 0.0249567\n",
      "[173]\tvalid_0's huber: 0.0249566\n",
      "[174]\tvalid_0's huber: 0.0249567\n",
      "[175]\tvalid_0's huber: 0.0249568\n",
      "[176]\tvalid_0's huber: 0.0249567\n",
      "[177]\tvalid_0's huber: 0.0249567\n",
      "[178]\tvalid_0's huber: 0.0249567\n",
      "[179]\tvalid_0's huber: 0.0249568\n",
      "[180]\tvalid_0's huber: 0.0249568\n",
      "[181]\tvalid_0's huber: 0.0249568\n",
      "[182]\tvalid_0's huber: 0.0249567\n",
      "[183]\tvalid_0's huber: 0.0249567\n",
      "[184]\tvalid_0's huber: 0.0249568\n",
      "[185]\tvalid_0's huber: 0.0249567\n",
      "[186]\tvalid_0's huber: 0.0249567\n",
      "[187]\tvalid_0's huber: 0.0249565\n",
      "[188]\tvalid_0's huber: 0.0249566\n",
      "[189]\tvalid_0's huber: 0.0249566\n",
      "[190]\tvalid_0's huber: 0.0249567\n",
      "[191]\tvalid_0's huber: 0.0249568\n",
      "[192]\tvalid_0's huber: 0.0249569\n",
      "[193]\tvalid_0's huber: 0.0249568\n",
      "[194]\tvalid_0's huber: 0.0249567\n",
      "[195]\tvalid_0's huber: 0.0249566\n",
      "[196]\tvalid_0's huber: 0.0249564\n",
      "[197]\tvalid_0's huber: 0.0249564\n",
      "[198]\tvalid_0's huber: 0.0249563\n",
      "[199]\tvalid_0's huber: 0.0249564\n",
      "[200]\tvalid_0's huber: 0.0249566\n",
      "[201]\tvalid_0's huber: 0.0249565\n",
      "[202]\tvalid_0's huber: 0.0249564\n",
      "[203]\tvalid_0's huber: 0.0249565\n",
      "[204]\tvalid_0's huber: 0.0249566\n",
      "[205]\tvalid_0's huber: 0.0249567\n",
      "[206]\tvalid_0's huber: 0.0249568\n",
      "[207]\tvalid_0's huber: 0.0249567\n",
      "[208]\tvalid_0's huber: 0.0249567\n",
      "[209]\tvalid_0's huber: 0.0249568\n",
      "[210]\tvalid_0's huber: 0.0249568\n",
      "[211]\tvalid_0's huber: 0.0249568\n",
      "[212]\tvalid_0's huber: 0.0249566\n",
      "[213]\tvalid_0's huber: 0.0249568\n",
      "[214]\tvalid_0's huber: 0.0249569\n",
      "[215]\tvalid_0's huber: 0.0249569\n",
      "[216]\tvalid_0's huber: 0.0249569\n",
      "[217]\tvalid_0's huber: 0.024957\n",
      "[218]\tvalid_0's huber: 0.024957\n",
      "[219]\tvalid_0's huber: 0.0249571\n",
      "[220]\tvalid_0's huber: 0.0249573\n",
      "[221]\tvalid_0's huber: 0.0249575\n",
      "[222]\tvalid_0's huber: 0.0249575\n",
      "[223]\tvalid_0's huber: 0.0249575\n",
      "[224]\tvalid_0's huber: 0.0249576\n",
      "[225]\tvalid_0's huber: 0.0249577\n",
      "[226]\tvalid_0's huber: 0.0249576\n",
      "[227]\tvalid_0's huber: 0.0249576\n",
      "[228]\tvalid_0's huber: 0.0249577\n",
      "[229]\tvalid_0's huber: 0.0249577\n",
      "[230]\tvalid_0's huber: 0.0249578\n",
      "[231]\tvalid_0's huber: 0.024958\n",
      "[232]\tvalid_0's huber: 0.024958\n",
      "[233]\tvalid_0's huber: 0.024958\n",
      "[234]\tvalid_0's huber: 0.0249579\n",
      "[235]\tvalid_0's huber: 0.0249581\n",
      "[236]\tvalid_0's huber: 0.0249582\n",
      "[237]\tvalid_0's huber: 0.0249583\n",
      "[238]\tvalid_0's huber: 0.0249584\n",
      "[239]\tvalid_0's huber: 0.0249585\n",
      "[240]\tvalid_0's huber: 0.0249585\n",
      "[241]\tvalid_0's huber: 0.0249586\n",
      "[242]\tvalid_0's huber: 0.0249586\n",
      "[243]\tvalid_0's huber: 0.0249586\n",
      "[244]\tvalid_0's huber: 0.0249586\n",
      "[245]\tvalid_0's huber: 0.0249586\n",
      "[246]\tvalid_0's huber: 0.0249586\n",
      "[247]\tvalid_0's huber: 0.0249585\n",
      "[248]\tvalid_0's huber: 0.0249584\n",
      "Early stopping, best iteration is:\n",
      "[198]\tvalid_0's huber: 0.0249563\n",
      "[0]\teval-rmse:0.22349\n",
      "[1]\teval-rmse:0.22349\n",
      "[2]\teval-rmse:0.22349\n",
      "[3]\teval-rmse:0.22349\n",
      "[4]\teval-rmse:0.22349\n",
      "[5]\teval-rmse:0.22348\n",
      "[6]\teval-rmse:0.22348\n",
      "[7]\teval-rmse:0.22348\n",
      "[8]\teval-rmse:0.22348\n",
      "[9]\teval-rmse:0.22348\n",
      "[10]\teval-rmse:0.22348\n",
      "[11]\teval-rmse:0.22348\n",
      "[12]\teval-rmse:0.22348\n",
      "[13]\teval-rmse:0.22348\n",
      "[14]\teval-rmse:0.22348\n",
      "[15]\teval-rmse:0.22348\n",
      "[16]\teval-rmse:0.22348\n",
      "[17]\teval-rmse:0.22347\n",
      "[18]\teval-rmse:0.22347\n",
      "[19]\teval-rmse:0.22347\n",
      "[20]\teval-rmse:0.22347\n",
      "[21]\teval-rmse:0.22347\n",
      "[22]\teval-rmse:0.22347\n",
      "[23]\teval-rmse:0.22347\n",
      "[24]\teval-rmse:0.22347\n",
      "[25]\teval-rmse:0.22347\n",
      "[26]\teval-rmse:0.22347\n",
      "[27]\teval-rmse:0.22347\n",
      "[28]\teval-rmse:0.22347\n",
      "[29]\teval-rmse:0.22347\n",
      "[30]\teval-rmse:0.22347\n",
      "[31]\teval-rmse:0.22347\n",
      "[32]\teval-rmse:0.22346\n",
      "[33]\teval-rmse:0.22346\n",
      "[34]\teval-rmse:0.22346\n",
      "[35]\teval-rmse:0.22346\n",
      "[36]\teval-rmse:0.22346\n",
      "[37]\teval-rmse:0.22346\n",
      "[38]\teval-rmse:0.22346\n",
      "[39]\teval-rmse:0.22346\n",
      "[40]\teval-rmse:0.22346\n",
      "[41]\teval-rmse:0.22346\n",
      "[42]\teval-rmse:0.22346\n",
      "[43]\teval-rmse:0.22346\n",
      "[44]\teval-rmse:0.22346\n",
      "[45]\teval-rmse:0.22346\n",
      "[46]\teval-rmse:0.22346\n",
      "[47]\teval-rmse:0.22346\n",
      "[48]\teval-rmse:0.22346\n",
      "[49]\teval-rmse:0.22346\n",
      "[50]\teval-rmse:0.22346\n",
      "[51]\teval-rmse:0.22346\n",
      "[52]\teval-rmse:0.22346\n",
      "[53]\teval-rmse:0.22346\n",
      "[54]\teval-rmse:0.22345\n",
      "[55]\teval-rmse:0.22345\n",
      "[56]\teval-rmse:0.22345\n",
      "[57]\teval-rmse:0.22345\n",
      "[58]\teval-rmse:0.22345\n",
      "[59]\teval-rmse:0.22345\n",
      "[60]\teval-rmse:0.22345\n",
      "[61]\teval-rmse:0.22345\n",
      "[62]\teval-rmse:0.22345\n",
      "[63]\teval-rmse:0.22345\n",
      "[64]\teval-rmse:0.22345\n",
      "[65]\teval-rmse:0.22345\n",
      "[66]\teval-rmse:0.22345\n",
      "[67]\teval-rmse:0.22345\n",
      "[68]\teval-rmse:0.22345\n",
      "[69]\teval-rmse:0.22345\n",
      "[70]\teval-rmse:0.22345\n",
      "[71]\teval-rmse:0.22345\n",
      "[72]\teval-rmse:0.22345\n",
      "[73]\teval-rmse:0.22345\n",
      "[74]\teval-rmse:0.22345\n",
      "[75]\teval-rmse:0.22345\n",
      "[76]\teval-rmse:0.22345\n",
      "[77]\teval-rmse:0.22345\n",
      "[78]\teval-rmse:0.22345\n",
      "[79]\teval-rmse:0.22345\n",
      "[80]\teval-rmse:0.22345\n",
      "[81]\teval-rmse:0.22345\n",
      "[82]\teval-rmse:0.22344\n",
      "[83]\teval-rmse:0.22344\n",
      "[84]\teval-rmse:0.22344\n",
      "[85]\teval-rmse:0.22344\n",
      "[86]\teval-rmse:0.22344\n",
      "[87]\teval-rmse:0.22344\n",
      "[88]\teval-rmse:0.22344\n",
      "[89]\teval-rmse:0.22344\n",
      "[90]\teval-rmse:0.22344\n",
      "[91]\teval-rmse:0.22344\n",
      "[92]\teval-rmse:0.22344\n",
      "[93]\teval-rmse:0.22344\n",
      "[94]\teval-rmse:0.22344\n",
      "[95]\teval-rmse:0.22344\n",
      "[96]\teval-rmse:0.22344\n",
      "[97]\teval-rmse:0.22344\n",
      "[98]\teval-rmse:0.22344\n",
      "[99]\teval-rmse:0.22344\n",
      "[100]\teval-rmse:0.22344\n",
      "[101]\teval-rmse:0.22344\n",
      "[102]\teval-rmse:0.22344\n",
      "[103]\teval-rmse:0.22344\n",
      "[104]\teval-rmse:0.22344\n",
      "[105]\teval-rmse:0.22344\n",
      "[106]\teval-rmse:0.22344\n",
      "[107]\teval-rmse:0.22344\n",
      "[108]\teval-rmse:0.22344\n",
      "[109]\teval-rmse:0.22344\n",
      "[110]\teval-rmse:0.22344\n",
      "[111]\teval-rmse:0.22344\n",
      "[112]\teval-rmse:0.22344\n",
      "[113]\teval-rmse:0.22344\n",
      "[114]\teval-rmse:0.22344\n",
      "[115]\teval-rmse:0.22344\n",
      "[116]\teval-rmse:0.22344\n",
      "[117]\teval-rmse:0.22343\n",
      "[118]\teval-rmse:0.22343\n",
      "[119]\teval-rmse:0.22343\n",
      "[120]\teval-rmse:0.22343\n",
      "[121]\teval-rmse:0.22343\n",
      "[122]\teval-rmse:0.22343\n",
      "[123]\teval-rmse:0.22343\n",
      "[124]\teval-rmse:0.22343\n",
      "[125]\teval-rmse:0.22343\n",
      "[126]\teval-rmse:0.22343\n",
      "[127]\teval-rmse:0.22343\n",
      "[128]\teval-rmse:0.22343\n",
      "[129]\teval-rmse:0.22343\n",
      "[130]\teval-rmse:0.22343\n",
      "[131]\teval-rmse:0.22343\n",
      "[132]\teval-rmse:0.22343\n",
      "[133]\teval-rmse:0.22343\n",
      "[134]\teval-rmse:0.22343\n",
      "[135]\teval-rmse:0.22343\n",
      "[136]\teval-rmse:0.22343\n",
      "[137]\teval-rmse:0.22343\n",
      "[138]\teval-rmse:0.22343\n",
      "[139]\teval-rmse:0.22343\n",
      "[140]\teval-rmse:0.22343\n",
      "[141]\teval-rmse:0.22343\n",
      "[142]\teval-rmse:0.22343\n",
      "[143]\teval-rmse:0.22343\n",
      "[144]\teval-rmse:0.22343\n",
      "[145]\teval-rmse:0.22343\n",
      "[146]\teval-rmse:0.22343\n",
      "[147]\teval-rmse:0.22343\n",
      "[148]\teval-rmse:0.22343\n",
      "[149]\teval-rmse:0.22343\n",
      "[150]\teval-rmse:0.22343\n",
      "[151]\teval-rmse:0.22343\n",
      "[152]\teval-rmse:0.22343\n",
      "[153]\teval-rmse:0.22343\n",
      "[154]\teval-rmse:0.22343\n",
      "[155]\teval-rmse:0.22343\n",
      "[156]\teval-rmse:0.22343\n",
      "[157]\teval-rmse:0.22343\n",
      "[158]\teval-rmse:0.22343\n",
      "[159]\teval-rmse:0.22343\n",
      "[160]\teval-rmse:0.22343\n",
      "[161]\teval-rmse:0.22343\n",
      "[162]\teval-rmse:0.22343\n",
      "[163]\teval-rmse:0.22343\n",
      "[164]\teval-rmse:0.22343\n",
      "[165]\teval-rmse:0.22343\n",
      "[166]\teval-rmse:0.22343\n",
      "[167]\teval-rmse:0.22343\n",
      "[168]\teval-rmse:0.22343\n",
      "[169]\teval-rmse:0.22343\n",
      "[170]\teval-rmse:0.22343\n",
      "[171]\teval-rmse:0.22343\n",
      "[172]\teval-rmse:0.22343\n",
      "[173]\teval-rmse:0.22343\n",
      "[174]\teval-rmse:0.22343\n",
      "[175]\teval-rmse:0.22343\n",
      "[176]\teval-rmse:0.22343\n",
      "[177]\teval-rmse:0.22343\n",
      "[178]\teval-rmse:0.22343\n",
      "[179]\teval-rmse:0.22343\n",
      "[180]\teval-rmse:0.22343\n",
      "[181]\teval-rmse:0.22343\n",
      "[182]\teval-rmse:0.22343\n",
      "[183]\teval-rmse:0.22343\n",
      "[184]\teval-rmse:0.22343\n",
      "[185]\teval-rmse:0.22343\n",
      "[186]\teval-rmse:0.22343\n",
      "[187]\teval-rmse:0.22343\n",
      "[188]\teval-rmse:0.22343\n",
      "[189]\teval-rmse:0.22343\n",
      "[190]\teval-rmse:0.22343\n",
      "[191]\teval-rmse:0.22343\n",
      "[192]\teval-rmse:0.22343\n",
      "[193]\teval-rmse:0.22343\n",
      "[194]\teval-rmse:0.22343\n",
      "[195]\teval-rmse:0.22343\n",
      "[196]\teval-rmse:0.22343\n",
      "[197]\teval-rmse:0.22343\n",
      "[198]\teval-rmse:0.22343\n",
      "[199]\teval-rmse:0.22343\n",
      "[200]\teval-rmse:0.22343\n",
      "[201]\teval-rmse:0.22343\n",
      "[202]\teval-rmse:0.22343\n",
      "[203]\teval-rmse:0.22343\n",
      "[204]\teval-rmse:0.22343\n",
      "[205]\teval-rmse:0.22343\n",
      "[206]\teval-rmse:0.22343\n",
      "[207]\teval-rmse:0.22343\n",
      "[208]\teval-rmse:0.22343\n",
      "[209]\teval-rmse:0.22343\n",
      "[210]\teval-rmse:0.22343\n",
      "[211]\teval-rmse:0.22343\n",
      "[212]\teval-rmse:0.22343\n",
      "[213]\teval-rmse:0.22343\n",
      "[214]\teval-rmse:0.22343\n",
      "[215]\teval-rmse:0.22343\n",
      "[216]\teval-rmse:0.22343\n",
      "[217]\teval-rmse:0.22343\n",
      "[218]\teval-rmse:0.22343\n",
      "[219]\teval-rmse:0.22343\n",
      "[220]\teval-rmse:0.22343\n",
      "[221]\teval-rmse:0.22343\n"
     ]
    }
   ],
   "source": [
    "#data_,_,_,_ = utils.preprocess_data(df,nn=True)\n",
    "p_xgb = joblib.load('./hpo/params/xgb_hpo_2021-04-12.pkl').best_params\n",
    "p_lgb = joblib.load('./hpo/params/lgb_hpo_2021-04-12.pkl').best_params\n",
    "p_lgb['objective']= 'huber'\n",
    "p_lgb['metric']= 'huber'\n",
    "p_lgb['n_jobs']=-1\n",
    "p_xgb['booster']='gbtree'\n",
    "p_xgb['tree_method']='gpu_hist'\n",
    "p_xgb['verbosity']=1\n",
    "p_xgb['n_jobs']=10\n",
    "p_xgb['objective'] = 'reg:pseudohubererror'\n",
    "p_xgb['eval_metric']='rmse'\n",
    "\"\"\"p = {'learning_rate': 0.050803514080008098,\n",
    "         'max_leaves': 50,\n",
    "         'bagging_fraction': 0.9011886437667906,\n",
    "         'bagging_freq': 5,\n",
    "         'feature_fraction': 0.7287921216266973,\n",
    "         'min_data_in_leaf': 396,\n",
    "         'lambda_l1': 0.02217696438630042,\n",
    "         'lambda_l2': 0.03985756503899372,\n",
    "         'boosting': 'gbdt',\n",
    "         'max_depth': 100,\n",
    "         'objective': 'regression',\n",
    "         'metric': 'mse',\n",
    "         'n_jobs':-1}\n",
    "p_xgb = {'learning_rate': 0.00775963914833853, 'max_depth': 10, 'max_leaves': 43, 'subsample': 0.571736653601962, 'colsample_bytree': 0.7848536668456085, 'min_child_weight': 34, 'lambda': 0.19056674702246584, 'alpha': 0.061222309705225526,'objective':'reg:squarederror',\n",
    "         'booster':          'gbtree',\n",
    "         'tree_method':      'gpu_hist',\n",
    "         'verbosity':        1,\n",
    "         'n_jobs':           10,\n",
    "         'eval_metric':      'rmse'}\"\"\"\n",
    "\n",
    "models_gbm = {'xgb':[],'lgb':[]}\n",
    "scores = {'xgb':[],'lgb':[]}\n",
    "preds = {'xgb':[],'lgb':[]}\n",
    "x_tr, x_val = data[t_idx], data[v_idx]\n",
    "y_tr, y_val = target[t_idx],target[v_idx]\n",
    "d_tr = lgb.Dataset(x_tr,label=y_tr)\n",
    "d_val = lgb.Dataset(x_val,label=y_val)\n",
    "clf_lgb= lgb.train(p_lgb,d_tr,1000,valid_sets=[d_val],early_stopping_rounds=50,verbose_eval=True)\n",
    "d_tr = xgb.DMatrix(x_tr, label=y_tr)\n",
    "d_val = xgb.DMatrix(x_val, label=y_val)\n",
    "clf_xgb = xgb.train(p_xgb, d_tr, 1000, [\n",
    "    (d_val, 'eval')], early_stopping_rounds=50, verbose_eval=True)\n",
    "preds_lgb = clf_lgb.predict(x_val)\n",
    "score_lgb = mean_squared_error(y_val,preds_lgb)\n",
    "preds_xgb = clf_xgb.predict(d_val)\n",
    "score_xgb = mean_squared_error(y_val, preds_xgb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {'xgb':[],'lgb':[]}\n",
    "models_gbm['lgb'].append(clf_lgb)\n",
    "models_gbm['xgb'].append(clf_xgb)\n",
    "scores['lgb'].append(score_lgb)\n",
    "scores['xgb'].append(score_xgb)\n",
    "preds['lgb'].append(preds_lgb)\n",
    "preds['xgb'].append(preds_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "\n",
    "# convenience method for scoring\n",
    "def scoring(df):\n",
    "    return correlation(df['prediction'], df['target'])\n",
    "\n",
    "\n",
    "# Payout is just the score cliped at +/-25%\n",
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target = target[v_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ae = ae_preds[v_idx].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_median = np.mean([preds_xgb,preds_lgb],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame({'pred':preds,'target':val_target,'era':era[v_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_era = df_preds.groupby('era')[['pred', 'target']].apply(\n",
    "        lambda x: correlation(x['pred'], x['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio = corr_per_era.mean()/corr_per_era.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_max = (corr_per_era +\n",
    "                   1).cumprod().rolling(window=100, min_periods=1).max()\n",
    "daily_values = (corr_per_era + 1).cumprod()\n",
    "max_drawdown = (rolling_max - daily_values).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5994183888492634"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.09594917771034872"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "max_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "era\n",
       "121    0.021332\n",
       "122    0.020814\n",
       "123    0.043069\n",
       "124    0.025774\n",
       "125    0.026407\n",
       "126    0.017629\n",
       "127    0.014410\n",
       "128    0.060124\n",
       "129   -0.013057\n",
       "130    0.027118\n",
       "131    0.050449\n",
       "132    0.056295\n",
       "197   -0.014955\n",
       "198   -0.001228\n",
       "199   -0.039116\n",
       "200   -0.003235\n",
       "201   -0.008183\n",
       "202    0.033003\n",
       "203    0.009287\n",
       "204    0.030738\n",
       "205    0.002030\n",
       "206    0.005624\n",
       "207    0.042192\n",
       "208    0.053453\n",
       "209    0.048515\n",
       "210   -0.015003\n",
       "211   -0.044093\n",
       "212    0.013495\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "corr_per_era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.016531712503909533"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "corr_per_era.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "feature with idx 197 is not used in model",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-84-aec5a5d763de>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot_partial_dependence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/catboost/core.py\u001B[0m in \u001B[0;36mplot_partial_dependence\u001B[0;34m(self, data, features, plot, plot_file, thread_count)\u001B[0m\n\u001B[1;32m   3148\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3150\u001B[0;31m         \u001B[0mfeatures_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetFeatureIndices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3151\u001B[0m         \u001B[0mborders\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_borders\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfeatures_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3152\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures_idx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/catboost/core.py\u001B[0m in \u001B[0;36mgetFeatureIndices\u001B[0;34m(features)\u001B[0m\n\u001B[1;32m   3105\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mgetFeatureIndices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3106\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3107\u001B[0;31m                 \u001B[0mfeatures_idxs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mgetFeatureIdx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeature\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3108\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3109\u001B[0m                 \u001B[0mfeatures_idxs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mgetFeatureIdx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/catboost/core.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   3105\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mgetFeatureIndices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3106\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3107\u001B[0;31m                 \u001B[0mfeatures_idxs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mgetFeatureIdx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeature\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3108\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3109\u001B[0m                 \u001B[0mfeatures_idxs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mgetFeatureIdx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/Kaggle/lib/python3.8/site-packages/catboost/core.py\u001B[0m in \u001B[0;36mgetFeatureIdx\u001B[0;34m(feature)\u001B[0m\n\u001B[1;32m   3100\u001B[0m                 \u001B[0mfeature_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfeature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3101\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mfeature_idx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_borders\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"only float features indexes are supported\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3102\u001B[0;31m             \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_borders\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfeature_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"feature with idx {} is not used in model\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeature_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3103\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfeature_idx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: feature with idx 197 is not used in model"
     ]
    }
   ],
   "source": [
    "model.plot_partial_dependence(data[t_idx],features=features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.0218 + 0.0238\n",
    "b = 0.0218/a\n",
    "c = 0.0238/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4780701754385964"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "1 - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.02381347835443424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_,df,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = data\n",
    "output_size= 1\n",
    "p = {'dim_1': 1500,\n",
    "    'dim_2': 1000,\n",
    "    'dim_3': 500,\n",
    "    'dim_4': 250,\n",
    "    'dim_5': 150,\n",
    "    'activation': nn.SiLU,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 0.05,\n",
    "    'label_smoothing':0.1,\n",
    "    'amsgrad': False,\n",
    "    'batch_size': 5000,\n",
    "    'loss':nn.MSELoss(),\n",
    "    'embedding':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(data=data, target=target, era=era)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_nn[-1]\n",
    "model.eval().to('cuda')\n",
    "index = np.linspace(0,dataset.data.shape[0],dataset.data.shape[0],dtype='int')"
   ]
  },
  {
   "source": [
    "batch_size = 5000\n",
    "dataloaders = utils.create_dataloaders(dataset,{'train':index.tolist()},batch_size=batch_size)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens =[]\n",
    "for i, batch in enumerate(dataloaders['train']):\n",
    "    _,hidden,_,_ = model(batch['data'].view(batch['data'].size(1),-1).to('cuda'))\n",
    "    hiddens.append(hidden.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = np.array([hiddens[i][j] for i in range(len(hiddens)) for j in range(len(hiddens[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.concatenate([data,hiddens],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "dataset = utils.FinData(data=data, target=target, era=era)\n",
    "models=[]\n",
    "for i, (train_idx,val_idx) in enumerate(gts.split(data,groups=era)):\n",
    "    \n",
    "    dataloaders = utils.create_dataloaders(\n",
    "    dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
    "    model = resnet.ResNet(input_size=input_size,output_size=output_size,params=p)\n",
    "    #model.apply(lambda x: utils.init_weights(x,'relu'))\n",
    "    es = EarlyStopping(monitor='val_mse', patience=10,\n",
    "                        min_delta=0.005, mode='min')\n",
    "    trainer = pl.Trainer(max_epochs=100,\n",
    "                            gpus=1,\n",
    "                            callbacks=[es])\n",
    "    trainer.fit(\n",
    "        model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
    "    torch.save(model.state_dict(), f'saved_models/Resnet/fold_{i}_state_dict.pth')\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data,target,features,era = utils.preprocess_data(df.iloc[:10000,],test=True,ordinal=True)\n",
    "data_,_,_,_=utils.preprocess_data(df.iloc[:10000,],test=True,nn=True)\n",
    "\"\"\"\n",
    "data = data[0:1000]\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = torch.tensor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "\n",
    "# convenience method for scoring\n",
    "def scoring(df):\n",
    "    return correlation(df['preds'], df['target'])\n",
    "\n",
    "\n",
    "# Payout is just the score cliped at +/-25%\n",
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models_nn:\n",
    "    model.eval()\n",
    "    preds.append(model(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [preds[i][1].detach().numpy() for i in range(len(preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = np.mean(predictions,axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame.from_dict({'era':era,'preds':predictions,'target':target.T})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrs = df_preds.groupby('era').apply(scoring)\n",
    "corrs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "payout(corrs).mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = models_nn[0](data).reshape(-1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models_gbm:\n",
    "    preds.append(model.predict(data_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_gbm = np.mean(preds,axis=0)\n",
    "df_gbm = pd.DataFrame.from_dict({'era':era,'preds':predictions_gbm,'target':target.T})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_gbm = df_gbm.groupby('era').apply(scoring)\n",
    "corrs_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payout(corrs_gbm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['data_type']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wojhed-riDni0-hopnok"
   ]
  }
 ]
}