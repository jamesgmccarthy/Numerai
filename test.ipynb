{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd043a852801e6dcfd8abe5f98f6ea4b643de796ad34bd55e09581c652994132134",
   "display_name": "Python 3.8.5 64-bit ('Kaggle': virtualenvwrapper)"
  },
  "metadata": {
   "interpreter": {
    "hash": "43a852801e6dcfd8abe5f98f6ea4b643de796ad34bd55e09581c652994132134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import copy\n",
    "from data_loading import utils, purged_group_time_series\n",
    "from models import resnet, lightning_nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import joblib\n",
    "from models.SupervisedAutoEncoder import SupAE, create_hidden_rep\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a29cb076622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_models/trained/trained_ae.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_ae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl_lightning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSupAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'ResNet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ae'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_ae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_ae\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_res' is not defined"
     ]
    }
   ],
   "source": [
    "p_ae = {'dim_1': 675, 'dim_2': 400, 'dim_3': 224, 'hidden': 162,\n",
    "         'activation': nn.ReLU, 'dropout': 0.2916447561918717, 'lr': 0.030272591341587315,\n",
    "         'recon_loss_factor': 0.4447516076774931, 'batch_size': 1252, 'loss_sup_ae': nn.MSELoss,\n",
    "         'loss_recon': nn.MSELoss,\n",
    "         'embedding': True,\n",
    "         'input_size':310,'output_size':1}\n",
    "\n",
    "#p_res = {'dim_1': 843, 'dim_2': 2637, 'dim_3': 1618, 'dim_4': 880, 'dim_5': 220, 'activation': nn.LeakyReLU, 'dropout': 0.453246718032545, 'lr': 0.04565788979670108, 'batch_size': 10836,'loss':nn.MSELoss,'embedding':True,'input_size':310,'output_size':1,'hidden_len':p_ae['hidden']}\n",
    "#model_res = resnet.ResNet(input_size=p_res['input_size'],output_size=p_res['output_size'],params=p_res)\n",
    "#model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\n",
    "model_ae = utils.load_model(path='./saved_models/trained/trained_ae.pth',p=p_ae,pl_lightning=False,model=SupAE)\n",
    "models_dict = {'ResNet':[model_res,p_res],'ae':[model_ae,p_ae]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils.create_predictions(models=models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id  prediction\n",
       "0     n002a15bc5575bbb    0.494021\n",
       "1     n00309caaa0f955e    0.494062\n",
       "2     n00576b397182463    0.494071\n",
       "3     n00633405d59c6a1    0.494200\n",
       "4     n008c2eefc8911c7    0.494225\n",
       "...                ...         ...\n",
       "5034  nffd9a4732077912    0.493923\n",
       "5035  nffe5dd1fd84013b    0.494102\n",
       "5036  nffeea35c1a65c79    0.494011\n",
       "5037  nffefe9565943594    0.494187\n",
       "5038  nfffb698f6d0d9f5    0.494123\n",
       "\n",
       "[1682185 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n002a15bc5575bbb</td>\n      <td>0.494021</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n00309caaa0f955e</td>\n      <td>0.494062</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n00576b397182463</td>\n      <td>0.494071</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n00633405d59c6a1</td>\n      <td>0.494200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n008c2eefc8911c7</td>\n      <td>0.494225</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5034</th>\n      <td>nffd9a4732077912</td>\n      <td>0.493923</td>\n    </tr>\n    <tr>\n      <th>5035</th>\n      <td>nffe5dd1fd84013b</td>\n      <td>0.494102</td>\n    </tr>\n    <tr>\n      <th>5036</th>\n      <td>nffeea35c1a65c79</td>\n      <td>0.494011</td>\n    </tr>\n    <tr>\n      <th>5037</th>\n      <td>nffefe9565943594</td>\n      <td>0.494187</td>\n    </tr>\n    <tr>\n      <th>5038</th>\n      <td>nfffb698f6d0d9f5</td>\n      <td>0.494123</td>\n    </tr>\n  </tbody>\n</table>\n<p>1682185 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "utils.create_prediction_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = f\"{utils.get_data_path(root_dir='./data')}/predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eras = df[df['data_type']=='validation']['era'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = os.listdir(pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [f\"era{era.replace('.csv','')}\" for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [era for era in eras if era in val_eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras =[era.replace('era','') for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [f'{era}.csv' for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = utils.create_prediction_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "\n",
    "# convenience method for scoring\n",
    "def scoring(df):\n",
    "    return correlation(df['prediction'], df['target'])\n",
    "\n",
    "\n",
    "# Payout is just the score cliped at +/-25%\n",
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_preds, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.0051975484992210575"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "scores = df.groupby('era').apply(scoring)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id  prediction\n",
       "0     n002a15bc5575bbb    0.494021\n",
       "1     n00309caaa0f955e    0.494062\n",
       "2     n00576b397182463    0.494071\n",
       "3     n00633405d59c6a1    0.494200\n",
       "4     n008c2eefc8911c7    0.494225\n",
       "...                ...         ...\n",
       "5034  nffd9a4732077912    0.493923\n",
       "5035  nffe5dd1fd84013b    0.494102\n",
       "5036  nffeea35c1a65c79    0.494011\n",
       "5037  nffefe9565943594    0.494187\n",
       "5038  nfffb698f6d0d9f5    0.494123\n",
       "\n",
       "[1682185 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n002a15bc5575bbb</td>\n      <td>0.494021</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n00309caaa0f955e</td>\n      <td>0.494062</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n00576b397182463</td>\n      <td>0.494071</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n00633405d59c6a1</td>\n      <td>0.494200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n008c2eefc8911c7</td>\n      <td>0.494225</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5034</th>\n      <td>nffd9a4732077912</td>\n      <td>0.493923</td>\n    </tr>\n    <tr>\n      <th>5035</th>\n      <td>nffe5dd1fd84013b</td>\n      <td>0.494102</td>\n    </tr>\n    <tr>\n      <th>5036</th>\n      <td>nffeea35c1a65c79</td>\n      <td>0.494011</td>\n    </tr>\n    <tr>\n      <th>5037</th>\n      <td>nffefe9565943594</td>\n      <td>0.494187</td>\n    </tr>\n    <tr>\n      <th>5038</th>\n      <td>nfffb698f6d0d9f5</td>\n      <td>0.494123</td>\n    </tr>\n  </tbody>\n</table>\n<p>1682185 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')\n",
    "df = df[df['data_type'] == 'validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-cd23c978c5ec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['era'][df['era'] == 'eraX'] = 'era999'\n"
     ]
    }
   ],
   "source": [
    "df['era'][df['era'] == 'eraX'] = 'era999'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndata_ = copy.deepcopy(data)\\ntarget_ = copy.deepcopy(target)\\noe = OrdinalEncoder()\\ndata = oe.fit_transform(data)\\ntarget=oe.fit_transform(target_.values.reshape(-1,1)).reshape(-1)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data,target,features,era = utils.preprocess_data(df,ordinal=True)\n",
    "data_test,target_test,features_test,era_test = utils.preprocess_data(df_test,ordinal=True)\n",
    "\"\"\"\n",
    "data_ = copy.deepcopy(data)\n",
    "target_ = copy.deepcopy(target)\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "target=oe.fit_transform(target_.values.reshape(-1,1)).reshape(-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df ,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autoencodr training\n",
    "data = np.concatenate([data,data_test],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.concatenate([target,target_test],0)\n",
    "era = np.concatenate([era,era_test],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_idx = np.arange(start=0,stop=len(era),step=1).tolist()\n",
    "t_idx =np.where(era <121)[0].tolist()\n",
    "v_idx =np.where(era >=121)[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = joblib.load('./hpo/params/SupAEnn_hpo_2021-04-05.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['activation'] = nn.ReLU\n",
    "p['loss_sup_ae']= nn.MSELoss\n",
    "p['loss_recon']= nn.MSELoss\n",
    "p['embedding']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "dataset = utils.FinData(data = data,target=target,era=era)\n",
    "dataloaders = utils.create_dataloaders(dataset,indexes={'train':t_idx,'val':v_idx},batch_size=p['batch_size'])\n",
    "p['input_size'] = len(features)\n",
    "p['output_size'] = 1\n",
    "model = SupAE(params=p)\n",
    "es = EarlyStopping(monitor='val_sup_loss',patience=10,min_delta=0.0005,mode='min')\n",
    "trainer = pl.Trainer(max_epochs=100,gpus=1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n         121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloaders['val']))\n",
    "batch['era']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | loss_recon       | MSELoss    | 0     \n",
      "1 | loss_sup_ae      | MSELoss    | 0     \n",
      "2 | embedding_layers | ModuleList | 4.7 K \n",
      "3 | encoder          | Sequential | 1.3 M \n",
      "4 | regressor        | Sequential | 232   \n",
      "5 | decoder          | Sequential | 1.3 M \n",
      "------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.174    Total estimated model params size (MB)\n",
      "/home/james/.virtualenvs/Kaggle/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  79%|███████▊  | 807/1028 [00:53<00:14, 15.15it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/222 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  79%|███████▊  | 809/1028 [00:53<00:14, 15.08it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  79%|███████▉  | 814/1028 [00:53<00:14, 15.14it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  80%|███████▉  | 819/1028 [00:53<00:13, 15.20it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  80%|████████  | 824/1028 [00:54<00:13, 15.26it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  81%|████████  | 829/1028 [00:54<00:12, 15.31it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  81%|████████  | 834/1028 [00:54<00:12, 15.37it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  82%|████████▏ | 839/1028 [00:54<00:12, 15.43it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  82%|████████▏ | 844/1028 [00:54<00:11, 15.49it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  83%|████████▎ | 849/1028 [00:54<00:11, 15.54it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  83%|████████▎ | 854/1028 [00:54<00:11, 15.60it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  84%|████████▎ | 859/1028 [00:54<00:10, 15.65it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  84%|████████▍ | 864/1028 [00:55<00:10, 15.71it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  85%|████████▍ | 869/1028 [00:55<00:10, 15.76it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  85%|████████▌ | 874/1028 [00:55<00:09, 15.82it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▌ | 879/1028 [00:55<00:09, 15.87it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▌ | 884/1028 [00:55<00:09, 15.93it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▋ | 889/1028 [00:55<00:08, 15.98it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  87%|████████▋ | 894/1028 [00:55<00:08, 16.04it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  87%|████████▋ | 899/1028 [00:55<00:08, 16.09it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  88%|████████▊ | 904/1028 [00:55<00:07, 16.14it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  88%|████████▊ | 909/1028 [00:56<00:07, 16.20it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  89%|████████▉ | 914/1028 [00:56<00:07, 16.25it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  89%|████████▉ | 919/1028 [00:56<00:06, 16.30it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  90%|████████▉ | 924/1028 [00:56<00:06, 16.36it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  90%|█████████ | 929/1028 [00:56<00:06, 16.41it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  91%|█████████ | 934/1028 [00:56<00:05, 16.46it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  91%|█████████▏| 939/1028 [00:56<00:05, 16.51it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Validating:  60%|█████▉    | 133/222 [00:03<00:02, 39.56it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 944/1028 [00:56<00:05, 16.56it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  92%|█████████▏| 949/1028 [00:57<00:04, 16.61it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  93%|█████████▎| 954/1028 [00:57<00:04, 16.66it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  93%|█████████▎| 959/1028 [00:57<00:04, 16.72it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  94%|█████████▍| 964/1028 [00:57<00:03, 16.77it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  94%|█████████▍| 969/1028 [00:57<00:03, 16.82it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  95%|█████████▍| 974/1028 [00:57<00:03, 16.87it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  95%|█████████▌| 979/1028 [00:57<00:02, 16.92it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  96%|█████████▌| 984/1028 [00:57<00:02, 16.97it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  96%|█████████▌| 989/1028 [00:58<00:02, 17.01it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Validating:  82%|████████▏ | 183/222 [00:04<00:00, 39.43it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 994/1028 [00:58<00:01, 17.06it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  97%|█████████▋| 999/1028 [00:58<00:01, 17.11it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  98%|█████████▊| 1004/1028 [00:58<00:01, 17.16it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  98%|█████████▊| 1009/1028 [00:58<00:01, 17.21it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  99%|█████████▊| 1014/1028 [00:58<00:00, 17.26it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0:  99%|█████████▉| 1019/1028 [00:58<00:00, 17.31it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0: 100%|█████████▉| 1024/1028 [00:58<00:00, 17.36it/s, loss=1.11, v_num=19, val_loss=1.070, val_sup_loss=0.201, train_loss=1.110]\n",
      "Epoch 0: 100%|██████████| 1028/1028 [00:59<00:00, 17.37it/s, loss=1.11, v_num=19, val_loss=0.990, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0531, recon_loss=1.210]\n",
      "Epoch 1:  15%|█▌        | 155/1028 [00:10<00:57, 15.07it/s, loss=1.11, v_num=19, val_loss=0.990, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0531, recon_loss=1.210] \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,train_dataloader = dataloaders['train'],val_dataloaders=dataloaders['val'])\n",
    "torch.save(model.state_dict(),f'./saved_models/trained/ae_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xgb = joblib.load('./hpo/params/xgb_hpo_2021-04-05.pkl').best_params\n",
    "p_lgb = joblib.load('./hpo/params/lgb_hpo_2021-04-05.pkl').best_params\n",
    "p_lgb['boosting']= 'gbdt'\n",
    "p_lgb['max_depth']= 50\n",
    "p_lgb['objective']= 'regression'\n",
    "p_lgb['metric']= 'mse'\n",
    "p_lgb['n_jobs']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict = {'data': data, 'target': target,\n",
    "                                 'features': features, 'era': era}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 501808, number of used features: 310\n",
      "[LightGBM] [Info] Start training from score 0.499997\n",
      "[1]\tvalid_0's l2: 0.0499474\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalid_0's l2: 0.049947\n",
      "[3]\tvalid_0's l2: 0.0499466\n",
      "[4]\tvalid_0's l2: 0.0499462\n",
      "[5]\tvalid_0's l2: 0.0499458\n",
      "[6]\tvalid_0's l2: 0.0499456\n",
      "[7]\tvalid_0's l2: 0.0499454\n",
      "[8]\tvalid_0's l2: 0.0499451\n",
      "[9]\tvalid_0's l2: 0.0499445\n",
      "[10]\tvalid_0's l2: 0.049944\n",
      "[11]\tvalid_0's l2: 0.0499438\n",
      "[12]\tvalid_0's l2: 0.0499434\n",
      "[13]\tvalid_0's l2: 0.0499432\n",
      "[14]\tvalid_0's l2: 0.049943\n",
      "[15]\tvalid_0's l2: 0.0499427\n",
      "[16]\tvalid_0's l2: 0.0499423\n",
      "[17]\tvalid_0's l2: 0.0499421\n",
      "[18]\tvalid_0's l2: 0.0499417\n",
      "[19]\tvalid_0's l2: 0.0499412\n",
      "[20]\tvalid_0's l2: 0.0499409\n",
      "[21]\tvalid_0's l2: 0.0499404\n",
      "[22]\tvalid_0's l2: 0.0499403\n",
      "[23]\tvalid_0's l2: 0.0499398\n",
      "[24]\tvalid_0's l2: 0.0499395\n",
      "[25]\tvalid_0's l2: 0.0499397\n",
      "[26]\tvalid_0's l2: 0.0499393\n",
      "[27]\tvalid_0's l2: 0.0499392\n",
      "[28]\tvalid_0's l2: 0.0499389\n",
      "[29]\tvalid_0's l2: 0.0499387\n",
      "[30]\tvalid_0's l2: 0.0499386\n",
      "[31]\tvalid_0's l2: 0.0499383\n",
      "[32]\tvalid_0's l2: 0.0499378\n",
      "[33]\tvalid_0's l2: 0.0499376\n",
      "[34]\tvalid_0's l2: 0.0499376\n",
      "[35]\tvalid_0's l2: 0.0499373\n",
      "[36]\tvalid_0's l2: 0.0499369\n",
      "[37]\tvalid_0's l2: 0.0499367\n",
      "[38]\tvalid_0's l2: 0.0499361\n",
      "[39]\tvalid_0's l2: 0.0499357\n",
      "[40]\tvalid_0's l2: 0.0499354\n",
      "[41]\tvalid_0's l2: 0.0499352\n",
      "[42]\tvalid_0's l2: 0.0499348\n",
      "[43]\tvalid_0's l2: 0.0499345\n",
      "[44]\tvalid_0's l2: 0.0499343\n",
      "[45]\tvalid_0's l2: 0.0499339\n",
      "[46]\tvalid_0's l2: 0.0499337\n",
      "[47]\tvalid_0's l2: 0.0499335\n",
      "[48]\tvalid_0's l2: 0.0499332\n",
      "[49]\tvalid_0's l2: 0.049933\n",
      "[50]\tvalid_0's l2: 0.0499329\n",
      "[51]\tvalid_0's l2: 0.0499327\n",
      "[52]\tvalid_0's l2: 0.0499323\n",
      "[53]\tvalid_0's l2: 0.049932\n",
      "[54]\tvalid_0's l2: 0.0499317\n",
      "[55]\tvalid_0's l2: 0.0499315\n",
      "[56]\tvalid_0's l2: 0.0499313\n",
      "[57]\tvalid_0's l2: 0.0499311\n",
      "[58]\tvalid_0's l2: 0.049931\n",
      "[59]\tvalid_0's l2: 0.0499311\n",
      "[60]\tvalid_0's l2: 0.0499307\n",
      "[61]\tvalid_0's l2: 0.0499305\n",
      "[62]\tvalid_0's l2: 0.0499303\n",
      "[63]\tvalid_0's l2: 0.0499302\n",
      "[64]\tvalid_0's l2: 0.04993\n",
      "[65]\tvalid_0's l2: 0.0499297\n",
      "[66]\tvalid_0's l2: 0.0499294\n",
      "[67]\tvalid_0's l2: 0.0499293\n",
      "[68]\tvalid_0's l2: 0.0499291\n",
      "[69]\tvalid_0's l2: 0.049929\n",
      "[70]\tvalid_0's l2: 0.049929\n",
      "[71]\tvalid_0's l2: 0.0499288\n",
      "[72]\tvalid_0's l2: 0.0499284\n",
      "[73]\tvalid_0's l2: 0.0499283\n",
      "[74]\tvalid_0's l2: 0.049928\n",
      "[75]\tvalid_0's l2: 0.0499278\n",
      "[76]\tvalid_0's l2: 0.0499275\n",
      "[77]\tvalid_0's l2: 0.0499274\n",
      "[78]\tvalid_0's l2: 0.0499273\n",
      "[79]\tvalid_0's l2: 0.0499272\n",
      "[80]\tvalid_0's l2: 0.0499269\n",
      "[81]\tvalid_0's l2: 0.049927\n",
      "[82]\tvalid_0's l2: 0.0499268\n",
      "[83]\tvalid_0's l2: 0.0499266\n",
      "[84]\tvalid_0's l2: 0.0499266\n",
      "[85]\tvalid_0's l2: 0.0499265\n",
      "[86]\tvalid_0's l2: 0.0499264\n",
      "[87]\tvalid_0's l2: 0.0499262\n",
      "[88]\tvalid_0's l2: 0.049926\n",
      "[89]\tvalid_0's l2: 0.0499257\n",
      "[90]\tvalid_0's l2: 0.0499255\n",
      "[91]\tvalid_0's l2: 0.0499252\n",
      "[92]\tvalid_0's l2: 0.0499249\n",
      "[93]\tvalid_0's l2: 0.0499251\n",
      "[94]\tvalid_0's l2: 0.0499252\n",
      "[95]\tvalid_0's l2: 0.0499252\n",
      "[96]\tvalid_0's l2: 0.0499251\n",
      "[97]\tvalid_0's l2: 0.0499248\n",
      "[98]\tvalid_0's l2: 0.0499246\n",
      "[99]\tvalid_0's l2: 0.0499244\n",
      "[100]\tvalid_0's l2: 0.0499244\n",
      "[101]\tvalid_0's l2: 0.0499242\n",
      "[102]\tvalid_0's l2: 0.0499243\n",
      "[103]\tvalid_0's l2: 0.0499243\n",
      "[104]\tvalid_0's l2: 0.049924\n",
      "[105]\tvalid_0's l2: 0.0499239\n",
      "[106]\tvalid_0's l2: 0.049924\n",
      "[107]\tvalid_0's l2: 0.049924\n",
      "[108]\tvalid_0's l2: 0.0499237\n",
      "[109]\tvalid_0's l2: 0.0499237\n",
      "[110]\tvalid_0's l2: 0.0499237\n",
      "[111]\tvalid_0's l2: 0.0499237\n",
      "[112]\tvalid_0's l2: 0.0499236\n",
      "[113]\tvalid_0's l2: 0.0499235\n",
      "[114]\tvalid_0's l2: 0.0499235\n",
      "[115]\tvalid_0's l2: 0.0499234\n",
      "[116]\tvalid_0's l2: 0.0499234\n",
      "[117]\tvalid_0's l2: 0.0499233\n",
      "[118]\tvalid_0's l2: 0.0499232\n",
      "[119]\tvalid_0's l2: 0.0499231\n",
      "[120]\tvalid_0's l2: 0.0499231\n",
      "[121]\tvalid_0's l2: 0.049923\n",
      "[122]\tvalid_0's l2: 0.0499228\n",
      "[123]\tvalid_0's l2: 0.0499228\n",
      "[124]\tvalid_0's l2: 0.0499228\n",
      "[125]\tvalid_0's l2: 0.0499226\n",
      "[126]\tvalid_0's l2: 0.0499225\n",
      "[127]\tvalid_0's l2: 0.0499223\n",
      "[128]\tvalid_0's l2: 0.0499222\n",
      "[129]\tvalid_0's l2: 0.0499221\n",
      "[130]\tvalid_0's l2: 0.049922\n",
      "[131]\tvalid_0's l2: 0.0499218\n",
      "[132]\tvalid_0's l2: 0.0499215\n",
      "[133]\tvalid_0's l2: 0.0499214\n",
      "[134]\tvalid_0's l2: 0.0499215\n",
      "[135]\tvalid_0's l2: 0.0499214\n",
      "[136]\tvalid_0's l2: 0.0499214\n",
      "[137]\tvalid_0's l2: 0.0499213\n",
      "[138]\tvalid_0's l2: 0.0499213\n",
      "[139]\tvalid_0's l2: 0.0499214\n",
      "[140]\tvalid_0's l2: 0.0499213\n",
      "[141]\tvalid_0's l2: 0.0499213\n",
      "[142]\tvalid_0's l2: 0.0499213\n",
      "[143]\tvalid_0's l2: 0.0499212\n",
      "[144]\tvalid_0's l2: 0.0499208\n",
      "[145]\tvalid_0's l2: 0.0499206\n",
      "[146]\tvalid_0's l2: 0.0499205\n",
      "[147]\tvalid_0's l2: 0.0499204\n",
      "[148]\tvalid_0's l2: 0.0499204\n",
      "[149]\tvalid_0's l2: 0.0499202\n",
      "[150]\tvalid_0's l2: 0.04992\n",
      "[151]\tvalid_0's l2: 0.0499201\n",
      "[152]\tvalid_0's l2: 0.0499201\n",
      "[153]\tvalid_0's l2: 0.0499203\n",
      "[154]\tvalid_0's l2: 0.0499204\n",
      "[155]\tvalid_0's l2: 0.0499202\n",
      "[156]\tvalid_0's l2: 0.04992\n",
      "[157]\tvalid_0's l2: 0.0499201\n",
      "[158]\tvalid_0's l2: 0.0499203\n",
      "[159]\tvalid_0's l2: 0.0499203\n",
      "[160]\tvalid_0's l2: 0.0499203\n",
      "[161]\tvalid_0's l2: 0.04992\n",
      "[162]\tvalid_0's l2: 0.04992\n",
      "[163]\tvalid_0's l2: 0.04992\n",
      "[164]\tvalid_0's l2: 0.0499201\n",
      "[165]\tvalid_0's l2: 0.0499201\n",
      "[166]\tvalid_0's l2: 0.04992\n",
      "[167]\tvalid_0's l2: 0.0499199\n",
      "[168]\tvalid_0's l2: 0.0499199\n",
      "[169]\tvalid_0's l2: 0.04992\n",
      "[170]\tvalid_0's l2: 0.0499196\n",
      "[171]\tvalid_0's l2: 0.0499195\n",
      "[172]\tvalid_0's l2: 0.0499197\n",
      "[173]\tvalid_0's l2: 0.0499195\n",
      "[174]\tvalid_0's l2: 0.0499192\n",
      "[175]\tvalid_0's l2: 0.0499193\n",
      "[176]\tvalid_0's l2: 0.0499193\n",
      "[177]\tvalid_0's l2: 0.0499194\n",
      "[178]\tvalid_0's l2: 0.0499194\n",
      "[179]\tvalid_0's l2: 0.0499193\n",
      "[180]\tvalid_0's l2: 0.0499193\n",
      "[181]\tvalid_0's l2: 0.0499194\n",
      "[182]\tvalid_0's l2: 0.0499192\n",
      "[183]\tvalid_0's l2: 0.0499192\n",
      "[184]\tvalid_0's l2: 0.0499192\n",
      "[185]\tvalid_0's l2: 0.0499193\n",
      "[186]\tvalid_0's l2: 0.0499192\n",
      "[187]\tvalid_0's l2: 0.0499191\n",
      "[188]\tvalid_0's l2: 0.0499191\n",
      "[189]\tvalid_0's l2: 0.0499193\n",
      "[190]\tvalid_0's l2: 0.0499192\n",
      "[191]\tvalid_0's l2: 0.0499193\n",
      "[192]\tvalid_0's l2: 0.0499193\n",
      "[193]\tvalid_0's l2: 0.0499193\n",
      "[194]\tvalid_0's l2: 0.0499194\n",
      "[195]\tvalid_0's l2: 0.0499194\n",
      "[196]\tvalid_0's l2: 0.0499195\n",
      "[197]\tvalid_0's l2: 0.0499195\n",
      "[198]\tvalid_0's l2: 0.0499194\n",
      "[199]\tvalid_0's l2: 0.0499194\n",
      "[200]\tvalid_0's l2: 0.0499192\n",
      "[201]\tvalid_0's l2: 0.0499192\n",
      "[202]\tvalid_0's l2: 0.0499191\n",
      "[203]\tvalid_0's l2: 0.0499189\n",
      "[204]\tvalid_0's l2: 0.0499192\n",
      "[205]\tvalid_0's l2: 0.0499193\n",
      "[206]\tvalid_0's l2: 0.0499193\n",
      "[207]\tvalid_0's l2: 0.0499191\n",
      "[208]\tvalid_0's l2: 0.0499192\n",
      "[209]\tvalid_0's l2: 0.0499193\n",
      "[210]\tvalid_0's l2: 0.0499193\n",
      "[211]\tvalid_0's l2: 0.0499193\n",
      "[212]\tvalid_0's l2: 0.0499191\n",
      "[213]\tvalid_0's l2: 0.0499191\n",
      "[214]\tvalid_0's l2: 0.049919\n",
      "[215]\tvalid_0's l2: 0.049919\n",
      "[216]\tvalid_0's l2: 0.0499191\n",
      "[217]\tvalid_0's l2: 0.049919\n",
      "[218]\tvalid_0's l2: 0.0499191\n",
      "[219]\tvalid_0's l2: 0.049919\n",
      "[220]\tvalid_0's l2: 0.0499192\n",
      "[221]\tvalid_0's l2: 0.0499193\n",
      "[222]\tvalid_0's l2: 0.0499192\n",
      "[223]\tvalid_0's l2: 0.0499193\n",
      "[224]\tvalid_0's l2: 0.0499191\n",
      "[225]\tvalid_0's l2: 0.0499191\n",
      "[226]\tvalid_0's l2: 0.0499194\n",
      "[227]\tvalid_0's l2: 0.0499194\n",
      "[228]\tvalid_0's l2: 0.0499193\n",
      "[229]\tvalid_0's l2: 0.0499193\n",
      "[230]\tvalid_0's l2: 0.0499192\n",
      "[231]\tvalid_0's l2: 0.0499191\n",
      "[232]\tvalid_0's l2: 0.0499191\n",
      "[233]\tvalid_0's l2: 0.0499191\n",
      "[234]\tvalid_0's l2: 0.0499192\n",
      "[235]\tvalid_0's l2: 0.0499193\n",
      "[236]\tvalid_0's l2: 0.0499193\n",
      "[237]\tvalid_0's l2: 0.0499194\n",
      "[238]\tvalid_0's l2: 0.0499192\n",
      "[239]\tvalid_0's l2: 0.0499192\n",
      "[240]\tvalid_0's l2: 0.0499191\n",
      "[241]\tvalid_0's l2: 0.0499189\n",
      "[242]\tvalid_0's l2: 0.0499188\n",
      "[243]\tvalid_0's l2: 0.0499189\n",
      "[244]\tvalid_0's l2: 0.0499189\n",
      "[245]\tvalid_0's l2: 0.0499189\n",
      "[246]\tvalid_0's l2: 0.0499189\n",
      "[247]\tvalid_0's l2: 0.0499191\n",
      "[248]\tvalid_0's l2: 0.0499192\n",
      "[249]\tvalid_0's l2: 0.0499191\n",
      "[250]\tvalid_0's l2: 0.049919\n",
      "[251]\tvalid_0's l2: 0.0499191\n",
      "[252]\tvalid_0's l2: 0.0499189\n",
      "[253]\tvalid_0's l2: 0.049919\n",
      "[254]\tvalid_0's l2: 0.0499192\n",
      "[255]\tvalid_0's l2: 0.049919\n",
      "[256]\tvalid_0's l2: 0.0499191\n",
      "[257]\tvalid_0's l2: 0.049919\n",
      "[258]\tvalid_0's l2: 0.0499192\n",
      "[259]\tvalid_0's l2: 0.0499192\n",
      "[260]\tvalid_0's l2: 0.0499191\n",
      "[261]\tvalid_0's l2: 0.049919\n",
      "[262]\tvalid_0's l2: 0.0499191\n",
      "[263]\tvalid_0's l2: 0.0499189\n",
      "[264]\tvalid_0's l2: 0.0499188\n",
      "[265]\tvalid_0's l2: 0.0499187\n",
      "[266]\tvalid_0's l2: 0.0499187\n",
      "[267]\tvalid_0's l2: 0.0499188\n",
      "[268]\tvalid_0's l2: 0.0499189\n",
      "[269]\tvalid_0's l2: 0.0499189\n",
      "[270]\tvalid_0's l2: 0.0499189\n",
      "[271]\tvalid_0's l2: 0.0499187\n",
      "[272]\tvalid_0's l2: 0.0499189\n",
      "[273]\tvalid_0's l2: 0.0499188\n",
      "[274]\tvalid_0's l2: 0.0499189\n",
      "[275]\tvalid_0's l2: 0.0499189\n",
      "[276]\tvalid_0's l2: 0.0499187\n",
      "[277]\tvalid_0's l2: 0.0499189\n",
      "[278]\tvalid_0's l2: 0.0499189\n",
      "[279]\tvalid_0's l2: 0.049919\n",
      "[280]\tvalid_0's l2: 0.049919\n",
      "[281]\tvalid_0's l2: 0.0499191\n",
      "[282]\tvalid_0's l2: 0.0499193\n",
      "[283]\tvalid_0's l2: 0.0499193\n",
      "[284]\tvalid_0's l2: 0.0499192\n",
      "[285]\tvalid_0's l2: 0.0499192\n",
      "[286]\tvalid_0's l2: 0.0499191\n",
      "[287]\tvalid_0's l2: 0.0499189\n",
      "[288]\tvalid_0's l2: 0.0499188\n",
      "[289]\tvalid_0's l2: 0.0499188\n",
      "[290]\tvalid_0's l2: 0.0499188\n",
      "[291]\tvalid_0's l2: 0.0499185\n",
      "[292]\tvalid_0's l2: 0.0499186\n",
      "[293]\tvalid_0's l2: 0.0499186\n",
      "[294]\tvalid_0's l2: 0.0499186\n",
      "[295]\tvalid_0's l2: 0.0499188\n",
      "[296]\tvalid_0's l2: 0.0499188\n",
      "[297]\tvalid_0's l2: 0.0499189\n",
      "[298]\tvalid_0's l2: 0.0499189\n",
      "[299]\tvalid_0's l2: 0.0499191\n",
      "[300]\tvalid_0's l2: 0.0499192\n",
      "[301]\tvalid_0's l2: 0.0499192\n",
      "[302]\tvalid_0's l2: 0.0499193\n",
      "[303]\tvalid_0's l2: 0.0499192\n",
      "[304]\tvalid_0's l2: 0.0499192\n",
      "[305]\tvalid_0's l2: 0.0499191\n",
      "[306]\tvalid_0's l2: 0.049919\n",
      "[307]\tvalid_0's l2: 0.049919\n",
      "[308]\tvalid_0's l2: 0.0499189\n",
      "[309]\tvalid_0's l2: 0.049919\n",
      "[310]\tvalid_0's l2: 0.0499191\n",
      "[311]\tvalid_0's l2: 0.0499192\n",
      "[312]\tvalid_0's l2: 0.049919\n",
      "[313]\tvalid_0's l2: 0.0499193\n",
      "[314]\tvalid_0's l2: 0.0499192\n",
      "[315]\tvalid_0's l2: 0.0499191\n",
      "[316]\tvalid_0's l2: 0.0499191\n",
      "[317]\tvalid_0's l2: 0.0499189\n",
      "[318]\tvalid_0's l2: 0.049919\n",
      "[319]\tvalid_0's l2: 0.0499188\n",
      "[320]\tvalid_0's l2: 0.0499187\n",
      "[321]\tvalid_0's l2: 0.0499187\n",
      "[322]\tvalid_0's l2: 0.0499187\n",
      "[323]\tvalid_0's l2: 0.0499189\n",
      "[324]\tvalid_0's l2: 0.049919\n",
      "[325]\tvalid_0's l2: 0.049919\n",
      "[326]\tvalid_0's l2: 0.049919\n",
      "[327]\tvalid_0's l2: 0.049919\n",
      "[328]\tvalid_0's l2: 0.0499189\n",
      "[329]\tvalid_0's l2: 0.049919\n",
      "[330]\tvalid_0's l2: 0.0499191\n",
      "[331]\tvalid_0's l2: 0.0499191\n",
      "[332]\tvalid_0's l2: 0.0499191\n",
      "[333]\tvalid_0's l2: 0.0499191\n",
      "[334]\tvalid_0's l2: 0.0499192\n",
      "[335]\tvalid_0's l2: 0.0499193\n",
      "[336]\tvalid_0's l2: 0.0499192\n",
      "[337]\tvalid_0's l2: 0.0499192\n",
      "[338]\tvalid_0's l2: 0.049919\n",
      "[339]\tvalid_0's l2: 0.0499191\n",
      "[340]\tvalid_0's l2: 0.0499191\n",
      "[341]\tvalid_0's l2: 0.0499189\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's l2: 0.0499185\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f386e919c10>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x_tr, x_val = data_dict['data'][t_idx], data_dict['data'][v_idx]\n",
    "y_tr, y_val = data_dict['target'][t_idx],data_dict['target'][v_idx]\n",
    "d_tr = lgb.Dataset(x_tr,label=y_tr)\n",
    "d_val = lgb.Dataset(x_val,label=y_val)\n",
    "clf = lgb.train(p_lgb,d_tr,1000,valid_sets=[d_val],early_stopping_rounds=50,verbose_eval=True)\n",
    "clf.save_model(f'./saved_models/trained/lgb.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'data': data, 'target': target,\n",
    "                 'features': features, 'era': era}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ae['input_size'] = len(features)\n",
    "p_ae['output_size'] = 1\n",
    "model = utils.load_model('./saved_models/trained/trained_ae.pth',\n",
    "                    p=p_ae, pl_lightning=False, model=SupAE)\n"
   ]
  },
  {
   "source": [
    "\n",
    "models_dict = {'lgb':[clf,p_lgb]}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a9fa17b290b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hidden_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_true'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "ae_output = create_hidden_rep(model, data_dict)\n",
    "data_dict['hidden'],ae_preds = ae_output['hidden'], ae_output['preds']\n",
    "data_dict['hidden_true'] = True\n",
    "p['hidden_len'] = data_dict['hidden'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(\n",
    "            data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
    "dataloaders = utils.create_dataloaders(\n",
    "            dataset, indexes={'train': t_idx}, batch_size=p['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_res = {'dim_1': 843, 'dim_2': 2637, 'dim_3': 1618, 'dim_4': 880, 'dim_5': 220, 'activation': nn.LeakyReLU, 'dropout': 0.453246718032545, 'lr': 0.04565788979670108, 'batch_size': 10836,'loss':nn.MSELoss,'embedding':True,'input_size':310,'output_size':1,'hidden_len':p_ae['hidden']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bdad4d20804c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'./saved_models/trained/ResNet_state_dict.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "model = resnet.ResNet(input_size = p['input_size'],output_size=p['output_size'],params=p)\n",
    "es = EarlyStopping(monitor='val_mse',patience=10,min_delta=0.0005,mode='min')\n",
    "trainer = pl.Trainer(max_epochs=100,gpus=1,callbacks=[es])\n",
    "trainer.fit(model,train_dataloader=dataloaders['train'],val_dataloaders=dataloaders['val'])\n",
    "torch.save(model.state_dict(),f'./saved_models/trained/ResNet_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#model = utils.load_model('./saved_models/trained/ResNet_state_dict.pth',\n",
    " #                   p=p, pl_lightning=False, model=resnet.ResNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = resnet.ResNet(input_size=p_res['input_size'],output_size=p_res['output_size'],params=p_res)\n",
    "model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\n",
    "model_ae = utils.load_model(path='./saved_models/trained/trained_ae.pth',p=p_ae,pl_lightning=False,model=SupAE)\n",
    "models_dict = {'ResNet':[model_res,p_res],'ae':[model_ae,p_ae]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/numerai_dataset_258/predictions/132.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ee752c9d09f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Numerai/data_loading/utils.py\u001b[0m in \u001b[0;36mcreate_predictions\u001b[0;34m(root_dir, models, hidden, ae)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prediction_lgb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mpred_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{get_data_path(root_dir)}/predictions/{era[0]}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{pred_path}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Kaggle/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/numerai_dataset_258/predictions/132.csv'"
     ]
    }
   ],
   "source": [
    "utils.create_predictions(models=models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd2951b82103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2, train_loss=1.090]\n",
      "Epoch 0:  88%|████████▊ | 248/283 [00:12<00:01, 19.77it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Validating:  79%|███████▉  | 130/165 [00:03<00:00, 37.07it/s]\u001b[A\n",
      "Epoch 0:  89%|████████▉ | 253/283 [00:12<00:01, 19.97it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  91%|█████████ | 258/283 [00:12<00:01, 20.16it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  93%|█████████▎| 263/283 [00:12<00:00, 20.33it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  95%|█████████▍| 268/283 [00:13<00:00, 20.50it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Validating:  91%|█████████ | 150/165 [00:04<00:00, 37.26it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 273/283 [00:13<00:00, 20.68it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0:  98%|█████████▊| 278/283 [00:13<00:00, 20.86it/s, loss=1.09, v_num=20, val_loss=1.080, val_sup_loss=0.222, train_loss=1.090]\n",
      "Epoch 0: 100%|██████████| 283/283 [00:13<00:00, 20.91it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.080, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  42%|████▏     | 118/283 [00:08<00:11, 14.24it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 120/283 [00:08<00:11, 13.95it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  44%|████▍     | 125/283 [00:08<00:11, 14.30it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  46%|████▌     | 130/283 [00:08<00:10, 14.63it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  48%|████▊     | 135/283 [00:09<00:09, 14.97it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "Epoch 1:  49%|████▉     | 139/283 [00:09<00:09, 15.09it/s, loss=1.09, v_num=20, val_loss=0.972, val_sup_loss=0.0511, train_loss=1.090, sup_loss=0.0748, recon_loss=1.190]\n",
      "/home/james/.virtualenvs/Kaggle/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | loss_recon       | MSELoss    | 0     \n",
      "1 | loss_sup_ae      | MSELoss    | 0     \n",
      "2 | embedding_layers | ModuleList | 4.7 K \n",
      "3 | encoder          | Sequential | 1.3 M \n",
      "4 | regressor        | Sequential | 232   \n",
      "5 | decoder          | Sequential | 1.3 M \n",
      "------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.174    Total estimated model params size (MB)\n",
      "Epoch 0:   0%|          | 0/452 [00:00<?, ?it/s] Exception ignored in: <function _releaseLock at 0x7f63c342a550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Epoch 0:   0%|          | 0/452 [00:00<?, ?it/s]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | loss_recon       | MSELoss    | 0     \n",
      "1 | loss_sup_ae      | MSELoss    | 0     \n",
      "2 | embedding_layers | ModuleList | 4.7 K \n",
      "3 | encoder          | Sequential | 1.3 M \n",
      "4 | regressor        | Sequential | 232   \n",
      "5 | decoder          | Sequential | 1.3 M \n",
      "------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.174    Total estimated model params size (MB)\n",
      "Epoch 0:  32%|███▏      | 203/629 [00:14<00:30, 13.97it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  73%|███████▎  | 457/629 [00:32<00:12, 14.26it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  73%|███████▎  | 459/629 [00:32<00:11, 14.18it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  74%|███████▎  | 463/629 [00:32<00:11, 14.25it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  74%|███████▍  | 468/629 [00:32<00:11, 14.35it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  75%|███████▌  | 473/629 [00:32<00:10, 14.45it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  76%|███████▌  | 478/629 [00:32<00:10, 14.54it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  77%|███████▋  | 483/629 [00:33<00:09, 14.63it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  15%|█▌        | 26/172 [00:00<00:04, 34.06it/s]\u001b[A\n",
      "Epoch 0:  78%|███████▊  | 488/629 [00:33<00:09, 14.72it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  78%|███████▊  | 493/629 [00:33<00:09, 14.81it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  79%|███████▉  | 498/629 [00:33<00:08, 14.90it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  80%|███████▉  | 503/629 [00:33<00:08, 14.99it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  27%|██▋       | 46/172 [00:01<00:03, 36.70it/s]\u001b[A\n",
      "Epoch 0:  81%|████████  | 508/629 [00:33<00:08, 15.08it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  82%|████████▏ | 513/629 [00:33<00:07, 15.17it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  82%|████████▏ | 518/629 [00:33<00:07, 15.26it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  83%|████████▎ | 523/629 [00:34<00:06, 15.34it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  38%|███▊      | 66/172 [00:02<00:02, 37.07it/s]\u001b[A\n",
      "Epoch 0:  84%|████████▍ | 528/629 [00:34<00:06, 15.43it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  85%|████████▍ | 533/629 [00:34<00:06, 15.51it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▌ | 538/629 [00:34<00:05, 15.60it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  86%|████████▋ | 543/629 [00:34<00:05, 15.68it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  50%|█████     | 86/172 [00:02<00:02, 36.73it/s]\u001b[A\n",
      "Epoch 0:  87%|████████▋ | 548/629 [00:34<00:05, 15.76it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  88%|████████▊ | 553/629 [00:34<00:04, 15.85it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  89%|████████▊ | 558/629 [00:35<00:04, 15.93it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  90%|████████▉ | 563/629 [00:35<00:04, 16.01it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  62%|██████▏   | 106/172 [00:03<00:01, 37.26it/s]\u001b[A\n",
      "Epoch 0:  90%|█████████ | 568/629 [00:35<00:03, 16.09it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  91%|█████████ | 573/629 [00:35<00:03, 16.18it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  92%|█████████▏| 578/629 [00:35<00:03, 16.26it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  93%|█████████▎| 583/629 [00:35<00:02, 16.34it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  93%|█████████▎| 588/629 [00:35<00:02, 16.42it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  94%|█████████▍| 593/629 [00:35<00:02, 16.50it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  95%|█████████▌| 598/629 [00:36<00:01, 16.58it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  96%|█████████▌| 603/629 [00:36<00:01, 16.66it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Validating:  85%|████████▍ | 146/172 [00:04<00:00, 39.22it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 608/629 [00:36<00:01, 16.73it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  97%|█████████▋| 613/629 [00:36<00:00, 16.81it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  98%|█████████▊| 618/629 [00:36<00:00, 16.89it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0:  99%|█████████▉| 623/629 [00:36<00:00, 16.98it/s, loss=1.11, v_num=22, val_loss=1.240, val_sup_loss=0.376, train_loss=1.110]\n",
      "Epoch 0: 100%|██████████| 629/629 [00:36<00:00, 17.03it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  73%|███████▎  | 457/629 [00:31<00:11, 14.38it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 460/629 [00:32<00:11, 14.31it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  74%|███████▍  | 465/629 [00:32<00:11, 14.41it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  75%|███████▍  | 470/629 [00:32<00:10, 14.51it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 28.14it/s]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 475/629 [00:32<00:10, 14.60it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  76%|███████▋  | 480/629 [00:32<00:10, 14.70it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  77%|███████▋  | 485/629 [00:32<00:09, 14.79it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  78%|███████▊  | 490/629 [00:32<00:09, 14.88it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:03, 36.47it/s]\u001b[A\n",
      "Epoch 1:  79%|███████▊  | 495/629 [00:33<00:08, 14.98it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  79%|███████▉  | 500/629 [00:33<00:08, 15.07it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  80%|████████  | 505/629 [00:33<00:08, 15.15it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  81%|████████  | 510/629 [00:33<00:07, 15.24it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 37.17it/s]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 515/629 [00:33<00:07, 15.33it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  83%|████████▎ | 520/629 [00:33<00:07, 15.42it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  83%|████████▎ | 525/629 [00:33<00:06, 15.51it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  84%|████████▍ | 530/629 [00:33<00:06, 15.59it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  42%|████▏     | 73/172 [00:02<00:02, 37.23it/s]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 535/629 [00:34<00:05, 15.67it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  86%|████████▌ | 540/629 [00:34<00:05, 15.75it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  87%|████████▋ | 545/629 [00:34<00:05, 15.84it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  87%|████████▋ | 550/629 [00:34<00:04, 15.92it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  54%|█████▍    | 93/172 [00:02<00:02, 36.17it/s]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 555/629 [00:34<00:04, 16.00it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  89%|████████▉ | 560/629 [00:34<00:04, 16.08it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  90%|████████▉ | 565/629 [00:34<00:03, 16.16it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  91%|█████████ | 570/629 [00:35<00:03, 16.25it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  66%|██████▌   | 113/172 [00:03<00:01, 37.19it/s]\u001b[A\n",
      "Epoch 1:  91%|█████████▏| 575/629 [00:35<00:03, 16.33it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  92%|█████████▏| 580/629 [00:35<00:02, 16.40it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  93%|█████████▎| 585/629 [00:35<00:02, 16.48it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  94%|█████████▍| 590/629 [00:35<00:02, 16.56it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  77%|███████▋  | 133/172 [00:03<00:01, 37.21it/s]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 595/629 [00:35<00:02, 16.64it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  95%|█████████▌| 600/629 [00:35<00:01, 16.71it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  96%|█████████▌| 605/629 [00:36<00:01, 16.79it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  97%|█████████▋| 610/629 [00:36<00:01, 16.87it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Validating:  89%|████████▉ | 153/172 [00:04<00:00, 37.76it/s]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 615/629 [00:36<00:00, 16.95it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  99%|█████████▊| 620/629 [00:36<00:00, 17.03it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1:  99%|█████████▉| 625/629 [00:36<00:00, 17.11it/s, loss=1.11, v_num=22, val_loss=0.983, val_sup_loss=0.0502, train_loss=1.110, sup_loss=0.0584, recon_loss=1.210]\n",
      "Epoch 1: 100%|██████████| 629/629 [00:36<00:00, 17.14it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.100, sup_loss=0.0505, recon_loss=1.210] \n",
      "Epoch 2:  22%|██▏       | 139/629 [00:09<00:33, 14.44it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  73%|███████▎  | 457/629 [00:31<00:11, 14.52it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 460/629 [00:31<00:11, 14.45it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  74%|███████▍  | 465/629 [00:31<00:11, 14.55it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  75%|███████▍  | 470/629 [00:32<00:10, 14.64it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 26.86it/s]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 475/629 [00:32<00:10, 14.73it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  76%|███████▋  | 480/629 [00:32<00:10, 14.82it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  77%|███████▋  | 485/629 [00:32<00:09, 14.91it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  78%|███████▊  | 490/629 [00:32<00:09, 15.00it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  79%|███████▊  | 495/629 [00:32<00:08, 15.09it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  22%|██▏       | 38/172 [00:01<00:03, 34.91it/s]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 500/629 [00:32<00:08, 15.17it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  80%|████████  | 505/629 [00:33<00:08, 15.26it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  81%|████████  | 510/629 [00:33<00:07, 15.35it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  82%|████████▏ | 515/629 [00:33<00:07, 15.43it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  83%|████████▎ | 520/629 [00:33<00:07, 15.52it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  37%|███▋      | 63/172 [00:02<00:02, 36.66it/s]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 525/629 [00:33<00:06, 15.61it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  84%|████████▍ | 530/629 [00:33<00:06, 15.70it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  85%|████████▌ | 535/629 [00:33<00:05, 15.78it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  86%|████████▌ | 540/629 [00:34<00:05, 15.87it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  87%|████████▋ | 545/629 [00:34<00:05, 15.96it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  51%|█████     | 88/172 [00:02<00:02, 38.46it/s]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 550/629 [00:34<00:04, 16.04it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  88%|████████▊ | 555/629 [00:34<00:04, 16.13it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  89%|████████▉ | 560/629 [00:34<00:04, 16.21it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  90%|████████▉ | 565/629 [00:34<00:03, 16.29it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  63%|██████▎   | 108/172 [00:03<00:01, 37.40it/s]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 570/629 [00:34<00:03, 16.37it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  91%|█████████▏| 575/629 [00:34<00:03, 16.45it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  92%|█████████▏| 580/629 [00:35<00:02, 16.53it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  93%|█████████▎| 585/629 [00:35<00:02, 16.61it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  74%|███████▍  | 128/172 [00:03<00:01, 37.43it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 590/629 [00:35<00:02, 16.69it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  95%|█████████▍| 595/629 [00:35<00:02, 16.77it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  95%|█████████▌| 600/629 [00:35<00:01, 16.85it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  96%|█████████▌| 605/629 [00:35<00:01, 16.92it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  97%|█████████▋| 610/629 [00:35<00:01, 17.01it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Validating:  89%|████████▉ | 153/172 [00:04<00:00, 39.09it/s]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 615/629 [00:35<00:00, 17.08it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  99%|█████████▊| 620/629 [00:36<00:00, 17.17it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2:  99%|█████████▉| 625/629 [00:36<00:00, 17.25it/s, loss=1.11, v_num=22, val_loss=0.988, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0505, recon_loss=1.210]\n",
      "Epoch 2: 100%|██████████| 629/629 [00:36<00:00, 17.27it/s, loss=1.11, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.110, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  73%|███████▎  | 457/629 [00:31<00:11, 14.60it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 460/629 [00:31<00:11, 14.52it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  74%|███████▍  | 465/629 [00:31<00:11, 14.62it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  75%|███████▍  | 470/629 [00:31<00:10, 14.71it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:06, 26.46it/s]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 475/629 [00:32<00:10, 14.80it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  76%|███████▋  | 480/629 [00:32<00:10, 14.90it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  77%|███████▋  | 485/629 [00:32<00:09, 14.99it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  78%|███████▊  | 490/629 [00:32<00:09, 15.08it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:03, 35.77it/s]\u001b[A\n",
      "Epoch 3:  79%|███████▊  | 495/629 [00:32<00:08, 15.18it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  79%|███████▉  | 500/629 [00:32<00:08, 15.27it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  80%|████████  | 505/629 [00:32<00:08, 15.36it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  81%|████████  | 510/629 [00:33<00:07, 15.45it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 36.94it/s]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 515/629 [00:33<00:07, 15.53it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  83%|████████▎ | 520/629 [00:33<00:06, 15.62it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  83%|████████▎ | 525/629 [00:33<00:06, 15.71it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  84%|████████▍ | 530/629 [00:33<00:06, 15.80it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  42%|████▏     | 73/172 [00:02<00:02, 37.35it/s]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 535/629 [00:33<00:05, 15.88it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  86%|████████▌ | 540/629 [00:33<00:05, 15.97it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  87%|████████▋ | 545/629 [00:33<00:05, 16.05it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  87%|████████▋ | 550/629 [00:34<00:04, 16.13it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  54%|█████▍    | 93/172 [00:02<00:02, 36.62it/s]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 555/629 [00:34<00:04, 16.22it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  89%|████████▉ | 560/629 [00:34<00:04, 16.30it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  90%|████████▉ | 565/629 [00:34<00:03, 16.38it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  91%|█████████ | 570/629 [00:34<00:03, 16.46it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  66%|██████▌   | 113/172 [00:03<00:01, 37.15it/s]\u001b[A\n",
      "Epoch 3:  91%|█████████▏| 575/629 [00:34<00:03, 16.54it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  92%|█████████▏| 580/629 [00:34<00:02, 16.62it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  93%|█████████▎| 585/629 [00:35<00:02, 16.70it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  94%|█████████▍| 590/629 [00:35<00:02, 16.78it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  77%|███████▋  | 133/172 [00:03<00:01, 37.47it/s]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 595/629 [00:35<00:02, 16.86it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  95%|█████████▌| 600/629 [00:35<00:01, 16.94it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  96%|█████████▌| 605/629 [00:35<00:01, 17.01it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  97%|█████████▋| 610/629 [00:35<00:01, 17.09it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  98%|█████████▊| 615/629 [00:35<00:00, 17.17it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Validating:  92%|█████████▏| 158/172 [00:04<00:00, 39.26it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 620/629 [00:35<00:00, 17.25it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3:  99%|█████████▉| 625/629 [00:36<00:00, 17.33it/s, loss=1.12, v_num=22, val_loss=0.992, val_sup_loss=0.0501, train_loss=1.120, sup_loss=0.050, recon_loss=1.220]\n",
      "Epoch 3: 100%|██████████| 629/629 [00:36<00:00, 17.36it/s, loss=1.12, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.110, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  73%|███████▎  | 457/629 [00:31<00:11, 14.49it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  73%|███████▎  | 460/629 [00:31<00:11, 14.42it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  74%|███████▍  | 465/629 [00:32<00:11, 14.52it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  75%|███████▍  | 470/629 [00:32<00:10, 14.61it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 26.54it/s]\u001b[A\n",
      "Epoch 4:  76%|███████▌  | 475/629 [00:32<00:10, 14.70it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  76%|███████▋  | 480/629 [00:32<00:10, 14.80it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  77%|███████▋  | 485/629 [00:32<00:09, 14.89it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  78%|███████▊  | 490/629 [00:32<00:09, 14.98it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:03, 35.10it/s]\u001b[A\n",
      "Epoch 4:  79%|███████▊  | 495/629 [00:32<00:08, 15.07it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  79%|███████▉  | 500/629 [00:32<00:08, 15.16it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  80%|████████  | 505/629 [00:33<00:08, 15.25it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  81%|████████  | 510/629 [00:33<00:07, 15.34it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 35.34it/s]\u001b[A\n",
      "Epoch 4:  82%|████████▏ | 515/629 [00:33<00:07, 15.42it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  83%|████████▎ | 520/629 [00:33<00:07, 15.51it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  83%|████████▎ | 525/629 [00:33<00:06, 15.61it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  84%|████████▍ | 530/629 [00:33<00:06, 15.69it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  85%|████████▌ | 535/629 [00:33<00:05, 15.78it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  45%|████▌     | 78/172 [00:02<00:02, 38.77it/s]\u001b[A\n",
      "Epoch 4:  86%|████████▌ | 540/629 [00:34<00:05, 15.87it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  87%|████████▋ | 545/629 [00:34<00:05, 15.95it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  87%|████████▋ | 550/629 [00:34<00:04, 16.03it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  88%|████████▊ | 555/629 [00:34<00:04, 16.12it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  57%|█████▋    | 98/172 [00:02<00:01, 37.44it/s]\u001b[A\n",
      "Epoch 4:  89%|████████▉ | 560/629 [00:34<00:04, 16.20it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  90%|████████▉ | 565/629 [00:34<00:03, 16.28it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  91%|█████████ | 570/629 [00:34<00:03, 16.36it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  91%|█████████▏| 575/629 [00:34<00:03, 16.44it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  69%|██████▊   | 118/172 [00:03<00:01, 37.02it/s]\u001b[A\n",
      "Epoch 4:  92%|█████████▏| 580/629 [00:35<00:02, 16.52it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  93%|█████████▎| 585/629 [00:35<00:02, 16.59it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  94%|█████████▍| 590/629 [00:35<00:02, 16.67it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  95%|█████████▍| 595/629 [00:35<00:02, 16.75it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  95%|█████████▌| 600/629 [00:35<00:01, 16.83it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Validating:  83%|████████▎ | 143/172 [00:04<00:00, 37.74it/s]\u001b[A\n",
      "Epoch 4:  96%|█████████▌| 605/629 [00:35<00:01, 16.91it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  97%|█████████▋| 610/629 [00:35<00:01, 16.98it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  98%|█████████▊| 615/629 [00:36<00:00, 17.06it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  99%|█████████▊| 620/629 [00:36<00:00, 17.14it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4:  99%|█████████▉| 625/629 [00:36<00:00, 17.22it/s, loss=1.14, v_num=22, val_loss=0.999, val_sup_loss=0.050, train_loss=1.140, sup_loss=0.0497, recon_loss=1.220]\n",
      "Epoch 4: 100%|██████████| 629/629 [00:36<00:00, 17.24it/s, loss=1.14, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.130, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  73%|███████▎  | 457/629 [00:32<00:12, 14.24it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  73%|███████▎  | 460/629 [00:32<00:11, 14.16it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  74%|███████▍  | 465/629 [00:32<00:11, 14.26it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  75%|███████▍  | 470/629 [00:32<00:11, 14.36it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:   8%|▊         | 13/172 [00:00<00:05, 26.90it/s]\u001b[A\n",
      "Epoch 5:  76%|███████▌  | 475/629 [00:32<00:10, 14.45it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  76%|███████▋  | 480/629 [00:33<00:10, 14.53it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  77%|███████▋  | 485/629 [00:33<00:09, 14.62it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  78%|███████▊  | 490/629 [00:33<00:09, 14.71it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  19%|█▉        | 33/172 [00:01<00:04, 34.60it/s]\u001b[A\n",
      "Epoch 5:  79%|███████▊  | 495/629 [00:33<00:09, 14.80it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  79%|███████▉  | 500/629 [00:33<00:08, 14.89it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  80%|████████  | 505/629 [00:33<00:08, 14.98it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  81%|████████  | 510/629 [00:33<00:07, 15.07it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  31%|███       | 53/172 [00:01<00:03, 36.29it/s]\u001b[A\n",
      "Epoch 5:  82%|████████▏ | 515/629 [00:33<00:07, 15.15it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  83%|████████▎ | 520/629 [00:34<00:07, 15.24it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  83%|████████▎ | 525/629 [00:34<00:06, 15.33it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  84%|████████▍ | 530/629 [00:34<00:06, 15.41it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  42%|████▏     | 73/172 [00:02<00:02, 36.99it/s]\u001b[A\n",
      "Epoch 5:  85%|████████▌ | 535/629 [00:34<00:06, 15.50it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  86%|████████▌ | 540/629 [00:34<00:05, 15.58it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  87%|████████▋ | 545/629 [00:34<00:05, 15.66it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  87%|████████▋ | 550/629 [00:34<00:05, 15.74it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  54%|█████▍    | 93/172 [00:02<00:02, 35.48it/s]\u001b[A\n",
      "Epoch 5:  88%|████████▊ | 555/629 [00:35<00:04, 15.82it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  89%|████████▉ | 560/629 [00:35<00:04, 15.90it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  90%|████████▉ | 565/629 [00:35<00:04, 15.98it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  91%|█████████ | 570/629 [00:35<00:03, 16.05it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  66%|██████▌   | 113/172 [00:03<00:01, 34.25it/s]\u001b[A\n",
      "Epoch 5:  91%|█████████▏| 575/629 [00:35<00:03, 16.13it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  92%|█████████▏| 580/629 [00:35<00:03, 16.20it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  93%|█████████▎| 585/629 [00:35<00:02, 16.27it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  94%|█████████▍| 590/629 [00:36<00:02, 16.35it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Validating:  77%|███████▋  | 133/172 [00:03<00:01, 35.08it/s]\u001b[A\n",
      "Epoch 5:  95%|█████████▍| 595/629 [00:36<00:02, 16.42it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  95%|█████████▌| 600/629 [00:36<00:01, 16.50it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  96%|█████████▌| 605/629 [00:36<00:01, 16.58it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  97%|█████████▋| 610/629 [00:36<00:01, 16.65it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  98%|█████████▊| 615/629 [00:36<00:00, 16.74it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  99%|█████████▊| 620/629 [00:36<00:00, 16.82it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5:  99%|█████████▉| 625/629 [00:36<00:00, 16.90it/s, loss=1.16, v_num=22, val_loss=1.020, val_sup_loss=0.050, train_loss=1.160, sup_loss=0.0495, recon_loss=1.230]\n",
      "Epoch 5: 100%|██████████| 629/629 [00:37<00:00, 16.92it/s, loss=1.16, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.160, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  73%|███████▎  | 457/629 [00:32<00:12, 13.93it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/172 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  73%|███████▎  | 460/629 [00:33<00:12, 13.86it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  74%|███████▍  | 465/629 [00:33<00:11, 13.95it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:   5%|▍         | 8/172 [00:00<00:08, 20.24it/s]\u001b[A\n",
      "Epoch 6:  75%|███████▍  | 470/629 [00:33<00:11, 14.05it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  76%|███████▌  | 475/629 [00:33<00:10, 14.15it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  76%|███████▋  | 480/629 [00:33<00:10, 14.24it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  77%|███████▋  | 485/629 [00:33<00:10, 14.32it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  16%|█▋        | 28/172 [00:01<00:04, 33.63it/s]\u001b[A\n",
      "Epoch 6:  78%|███████▊  | 490/629 [00:34<00:09, 14.41it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  79%|███████▊  | 495/629 [00:34<00:09, 14.50it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  79%|███████▉  | 500/629 [00:34<00:08, 14.59it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  80%|████████  | 505/629 [00:34<00:08, 14.68it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  28%|██▊       | 48/172 [00:01<00:03, 36.21it/s]\u001b[A\n",
      "Epoch 6:  81%|████████  | 510/629 [00:34<00:08, 14.77it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  82%|████████▏ | 515/629 [00:34<00:07, 14.85it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  83%|████████▎ | 520/629 [00:34<00:07, 14.94it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  83%|████████▎ | 525/629 [00:34<00:06, 15.02it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  40%|███▉      | 68/172 [00:02<00:02, 36.26it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▍ | 530/629 [00:35<00:06, 15.11it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  85%|████████▌ | 535/629 [00:35<00:06, 15.19it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  86%|████████▌ | 540/629 [00:35<00:05, 15.26it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  87%|████████▋ | 545/629 [00:35<00:05, 15.34it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  51%|█████     | 88/172 [00:02<00:02, 34.74it/s]\u001b[A\n",
      "Epoch 6:  87%|████████▋ | 550/629 [00:35<00:05, 15.42it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  88%|████████▊ | 555/629 [00:35<00:04, 15.51it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  89%|████████▉ | 560/629 [00:35<00:04, 15.59it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  90%|████████▉ | 565/629 [00:36<00:04, 15.68it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  63%|██████▎   | 108/172 [00:03<00:01, 37.79it/s]\u001b[A\n",
      "Epoch 6:  91%|█████████ | 570/629 [00:36<00:03, 15.75it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  91%|█████████▏| 575/629 [00:36<00:03, 15.83it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  92%|█████████▏| 580/629 [00:36<00:03, 15.91it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  93%|█████████▎| 585/629 [00:36<00:02, 15.98it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  74%|███████▍  | 128/172 [00:03<00:01, 35.56it/s]\u001b[A\n",
      "Epoch 6:  94%|█████████▍| 590/629 [00:36<00:02, 16.06it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  95%|█████████▍| 595/629 [00:36<00:02, 16.13it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  95%|█████████▌| 600/629 [00:37<00:01, 16.21it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  96%|█████████▌| 605/629 [00:37<00:01, 16.28it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Validating:  86%|████████▌ | 148/172 [00:04<00:00, 36.73it/s]\u001b[A\n",
      "Epoch 6:  97%|█████████▋| 610/629 [00:37<00:01, 16.35it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  98%|█████████▊| 615/629 [00:37<00:00, 16.43it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  99%|█████████▊| 620/629 [00:37<00:00, 16.51it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6:  99%|█████████▉| 625/629 [00:37<00:00, 16.58it/s, loss=1.21, v_num=22, val_loss=1.040, val_sup_loss=0.0501, train_loss=1.210, sup_loss=0.0492, recon_loss=1.260]\n",
      "Epoch 6: 100%|██████████| 629/629 [00:37<00:00, 16.60it/s, loss=1.21, v_num=22, val_loss=1.090, val_sup_loss=0.0502, train_loss=1.200, sup_loss=0.0488, recon_loss=1.300]\n",
      "Epoch 7:   4%|▍         | 25/629 [00:02<00:49, 12.09it/s, loss=1.2, v_num=22, val_loss=1.090, val_sup_loss=0.0502, train_loss=1.200, sup_loss=0.0488, recon_loss=1.300]"
     ]
    }
   ],
   "source": [
    "p['input_size'] = len(features)\n",
    "p['output_size'] = 1\n",
    "train = True\n",
    "if train:\n",
    "    gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "    models=[]\n",
    "    for i, (train_idx,val_idx) in enumerate(gts.split(data,groups=era)):\n",
    "        dataset = utils.FinData(data=data, target=target, era=era)\n",
    "        dataloaders = utils.create_dataloaders(\n",
    "        dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
    "        model = SupAE(p)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=10,\n",
    "                            min_delta=0.005, mode='min')\n",
    "        trainer = pl.Trainer(max_epochs=100,\n",
    "                                gpus=1,\n",
    "                                callbacks=[es])\n",
    "        trainer.fit(\n",
    "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
    "        torch.save(model.state_dict(), f'saved_models/ae_fold_{i}_state_dict.pth')\n",
    "        models.append(model)\n",
    "else:\n",
    "    models_nn = utils.load_model('./saved_models/AE',p=p,pl_lightning=False,model=SupAE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_,_,_,_ = utils.preprocess_data(df,nn=True)\n",
    "\n",
    "p = {'learning_rate': 0.030803514080008098,\n",
    "         'max_leaves': 50,\n",
    "         'bagging_fraction': 0.9011886437667906,\n",
    "         'bagging_freq': 5,\n",
    "         'feature_fraction': 0.3287921216266973,\n",
    "         'min_data_in_leaf': 396,\n",
    "         'lambda_l1': 0.02217696438630042,\n",
    "         'lambda_l2': 0.03985756503899372,\n",
    "         'boosting': 'gbdt',\n",
    "         'max_depth': 50,\n",
    "         'objective': 'regression',\n",
    "         'metric': 'mse',\n",
    "         'n_jobs':-1}\n",
    "gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "models_gbm = []\n",
    "scores = []\n",
    "for i, (tr_idx,val_idx) in enumerate(gts.split(data_,groups=era)):\n",
    "    x_tr, x_val = data_[tr_idx], data_[val_idx]\n",
    "    y_tr, y_val = target[tr_idx],target[val_idx]\n",
    "    d_tr = lgb.Dataset(x_tr,label=y_tr)\n",
    "    d_val = lgb.Dataset(x_val,label=y_val)\n",
    "    clf = lgb.train(p,d_tr,1000,valid_sets=[d_val],early_stopping_rounds=50,verbose_eval=True)\n",
    "    preds = clf.predict(x_val)\n",
    "    score = mean_squared_error(y_val,preds)\n",
    "    print(f'Fold {i} MSE score:\\t', score)\n",
    "    models_gbm.append(clf)\n",
    "    scores.append(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_,df,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = data\n",
    "output_size= 1\n",
    "p = {'dim_1': 1500,\n",
    "    'dim_2': 1000,\n",
    "    'dim_3': 500,\n",
    "    'dim_4': 250,\n",
    "    'dim_5': 150,\n",
    "    'activation': nn.SiLU,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 0.05,\n",
    "    'label_smoothing':0.1,\n",
    "    'amsgrad': False,\n",
    "    'batch_size': 5000,\n",
    "    'loss':nn.MSELoss(),\n",
    "    'embedding':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(data=data, target=target, era=era)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_nn[-1]\n",
    "model.eval().to('cuda')\n",
    "index = np.linspace(0,dataset.data.shape[0],dataset.data.shape[0],dtype='int')"
   ]
  },
  {
   "source": [
    "batch_size = 5000\n",
    "dataloaders = utils.create_dataloaders(dataset,{'train':index.tolist()},batch_size=batch_size)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens =[]\n",
    "for i, batch in enumerate(dataloaders['train']):\n",
    "    _,hidden,_,_ = model(batch['data'].view(batch['data'].size(1),-1).to('cuda'))\n",
    "    hiddens.append(hidden.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = np.array([hiddens[i][j] for i in range(len(hiddens)) for j in range(len(hiddens[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.concatenate([data,hiddens],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "dataset = utils.FinData(data=data, target=target, era=era)\n",
    "models=[]\n",
    "for i, (train_idx,val_idx) in enumerate(gts.split(data,groups=era)):\n",
    "    \n",
    "    dataloaders = utils.create_dataloaders(\n",
    "    dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
    "    model = resnet.ResNet(input_size=input_size,output_size=output_size,params=p)\n",
    "    #model.apply(lambda x: utils.init_weights(x,'relu'))\n",
    "    es = EarlyStopping(monitor='val_mse', patience=10,\n",
    "                        min_delta=0.005, mode='min')\n",
    "    trainer = pl.Trainer(max_epochs=100,\n",
    "                            gpus=1,\n",
    "                            callbacks=[es])\n",
    "    trainer.fit(\n",
    "        model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
    "    torch.save(model.state_dict(), f'saved_models/Resnet/fold_{i}_state_dict.pth')\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data,target,features,era = utils.preprocess_data(df.iloc[:10000,],test=True,ordinal=True)\n",
    "data_,_,_,_=utils.preprocess_data(df.iloc[:10000,],test=True,nn=True)\n",
    "\"\"\"\n",
    "data = data[0:1000]\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = torch.tensor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "\n",
    "# convenience method for scoring\n",
    "def scoring(df):\n",
    "    return correlation(df['preds'], df['target'])\n",
    "\n",
    "\n",
    "# Payout is just the score cliped at +/-25%\n",
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models_nn:\n",
    "    model.eval()\n",
    "    preds.append(model(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [preds[i][1].detach().numpy() for i in range(len(preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = np.mean(predictions,axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame.from_dict({'era':era,'preds':predictions,'target':target.T})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrs = df_preds.groupby('era').apply(scoring)\n",
    "corrs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "payout(corrs).mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = models_nn[0](data).reshape(-1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models_gbm:\n",
    "    preds.append(model.predict(data_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_gbm = np.mean(preds,axis=0)\n",
    "df_gbm = pd.DataFrame.from_dict({'era':era,'preds':predictions_gbm,'target':target.T})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_gbm = df_gbm.groupby('era').apply(scoring)\n",
    "corrs_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payout(corrs_gbm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['data_type']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wojhed-riDni0-hopnok"
   ]
  }
 ]
}